{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to the documentation for ZarrNii, a Python library for working with OME-Zarr and NIfTI formats. ZarrNii bridges the gap between these two popular formats, enabling seamless data transformation, metadata preservation, and efficient processing of large biomedical images.</p>"},{"location":"#what-is-zarrnii","title":"What is ZarrNii?","text":"<p>ZarrNii is designed for researchers and engineers working with:</p> <ul> <li>OME-Zarr: A format for storing multidimensional image data, commonly used in microscopy.</li> <li>NIfTI: A standard format for neuroimaging data.</li> </ul> <p>ZarrNii allows you to:</p> <ul> <li>Read and write OME-Zarr and NIfTI datasets.</li> <li>Work with 4D and 5D images, including time-series data (T,C,Z,Y,X).</li> <li>Perform transformations like cropping, downsampling, and interpolation.</li> <li>Select specific channels and timepoints from multidimensional datasets.</li> <li>Preserve and manipulate metadata from OME-Zarr (e.g., axes, coordinate transformations, OME annotations).</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Seamless Format Conversion: Easily convert between OME-Zarr and NIfTI while preserving spatial metadata.</li> <li>5D Image Support: Work with time-series data in (T,C,Z,Y,X) format with timepoint and channel selection.</li> <li>Transformations: Apply common operations like affine transformations, downsampling, and upsampling.</li> <li>Multiscale Support: Work with multiscale OME-Zarr pyramids.</li> <li>Metadata Handling: Access and modify OME-Zarr metadata like axes and transformations.</li> <li>Lazy Loading: Leverage Dask arrays for efficient processing of large datasets.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from zarrnii import ZarrNii\n\n# Load an OME-Zarr dataset\nznimg = ZarrNii.from_ome_zarr(\"path/to/zarr_dataset.ome.zarr\")\n\n# Load with specific timepoints and channels (5D support)\nznimg_subset = ZarrNii.from_ome_zarr(\"timeseries.zarr\", timepoints=[0, 2], channels=[1])\n\n# Perform a transformation (e.g., downsample)\ndownsampled_znimg = znimg.downsample(level=2)\n\n# Save as NIfTI\ndownsampled_znimg.to_nifti(\"output_dataset.nii\")\n</code></pre>"},{"location":"#learn-more","title":"Learn More","text":"<p>Explore the documentation to get started:</p> <ul> <li>Walkthrough: Overview: Understand the core concepts.</li> <li>API Reference: Dive into the technical details.</li> <li>Examples: Learn through practical examples.</li> <li>FAQ: Find answers to common questions.</li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to ZarrNii will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Comprehensive documentation with examples and API reference</li> <li>Multi-resolution OME-Zarr support with pyramid creation</li> <li>Enhanced transformation pipeline with composite operations</li> <li>Memory-efficient processing with Dask integration</li> <li>Support for multi-channel and time-series data</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Migration from Poetry to uv: Faster dependency management and builds with modern Python packaging</li> <li>Automated versioning: Version now derived from git tags using setuptools-scm</li> <li>Enhanced CI/CD: Updated workflows with trusted publishing to PyPI</li> <li>Improved performance for large dataset operations</li> <li>Enhanced metadata preservation across format conversions</li> <li>Optimized chunk sizing for better I/O performance</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>NumPy 2.0 Compatibility: Fixed deprecated np.product usage</li> <li>Documentation build issues with missing files</li> <li>Improved error handling for malformed input files</li> <li>Better memory management for large transformations</li> </ul>"},{"location":"changelog/#010-initial-development","title":"[0.1.0] - Initial Development","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Core ZarrNii class for unified OME-Zarr and NIfTI handling</li> <li>Affine and displacement transformation support</li> <li>Basic downsampling and upsampling operations</li> <li>Format conversion between OME-Zarr and NIfTI</li> <li>Spatial coordinate system management</li> <li>Integration with nibabel and zarr libraries</li> </ul>"},{"location":"changelog/#features","title":"Features","text":"<ul> <li>Lazy loading with Dask arrays</li> <li>Metadata preservation during transformations</li> <li>Multi-scale image pyramid support</li> <li>Flexible resampling and interpolation methods</li> <li>Comprehensive test suite</li> </ul>"},{"location":"changelog/#documentation","title":"Documentation","text":"<ul> <li>Getting started guide</li> <li>API reference</li> <li>Example workflows</li> <li>Installation instructions</li> </ul>"},{"location":"changelog/#development-notes","title":"Development Notes","text":"<p>This project is under active development. The API may change between versions as we refine the interface based on user feedback and use cases.</p>"},{"location":"changelog/#contributing","title":"Contributing","text":"<p>See Contributing for information about contributing to ZarrNii development.</p>"},{"location":"contributing/","title":"Contributing to ZarrNii","text":"<p>Thank you for your interest in contributing to ZarrNii! This document provides guidelines for contributing to the project.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11 or higher</li> <li>Git</li> <li>uv package manager (recommended) or pip</li> </ul>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Clone the repository:    <code>bash    git clone https://github.com/khanlab/zarrnii.git    cd zarrnii</code></p> </li> <li> <p>Install dependencies:    ```bash    # Using uv (recommended)    uv sync --dev</p> </li> </ol> <p># Or using pip    pip install -e \".[dev]\"    ```</p> <ol> <li>Set up pre-commit hooks:    <code>bash    uv run pre-commit install</code></li> </ol>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#code-style-and-quality","title":"Code Style and Quality","text":"<p>ZarrNii follows Python best practices and uses several tools to maintain code quality:</p> <ul> <li>Black: Code formatting</li> <li>isort: Import sorting  </li> <li>flake8: Linting and style checking</li> <li>pre-commit: Automated checks before commits</li> </ul> <p>Run quality checks:</p> <pre><code># Format code\nuv run black .\n\n# Sort imports\nuv run isort .\n\n# Lint code\nuv run flake8 .\n\n# Or use justfile for convenience (if just is installed)\njust format\njust lint\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":"<p>ZarrNii uses pytest for testing. Tests are located in the <code>tests/</code> directory.</p> <pre><code># Run all tests\nuv run pytest -v\n\n# Run with coverage\nuv run pytest --cov=zarrnii\n\n# Run with detailed coverage report\nuv run pytest --cov=zarrnii --cov-report=term-missing\n\n# Generate HTML coverage report\nuv run pytest --cov=zarrnii --cov-report=html\n\n# Run specific test file\nuv run pytest tests/test_io.py\n\n# Or use justfile\njust test\n</code></pre>"},{"location":"contributing/#coverage-requirements","title":"Coverage Requirements","text":"<p>ZarrNii maintains high code coverage standards: - Target coverage: 85%+ - Current coverage: 84%+ - Coverage reports exclude <code>_version.py</code> and test files - New features must include comprehensive tests - Pull requests should not decrease overall coverage</p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>Documentation is built with MkDocs and hosted on GitHub Pages.</p> <pre><code># Serve documentation locally\nuv run mkdocs serve\n\n# Build documentation\nuv run mkdocs build\n\n# Deploy to GitHub Pages (maintainers only)\nuv run mkdocs gh-deploy\n\n# Or use justfile\njust serve-docs\njust build-docs\n</code></pre>"},{"location":"contributing/#contribution-types","title":"Contribution Types","text":""},{"location":"contributing/#bug-reports","title":"Bug Reports","text":"<p>When reporting bugs, please include:</p> <ul> <li>Python version and platform</li> <li>ZarrNii version</li> <li>Minimal code example that reproduces the issue</li> <li>Expected vs. actual behavior</li> <li>Full error traceback if applicable</li> </ul>"},{"location":"contributing/#feature-requests","title":"Feature Requests","text":"<p>For feature requests, please provide:</p> <ul> <li>Clear description of the proposed feature</li> <li>Use case and motivation</li> <li>Potential implementation approach</li> <li>Any relevant literature or references</li> </ul>"},{"location":"contributing/#code-contributions","title":"Code Contributions","text":""},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li> <p>Fork the repository and create a feature branch:    <code>bash    git checkout -b feature/your-feature-name</code></p> </li> <li> <p>Make your changes:</p> </li> <li>Follow existing code style and conventions</li> <li>Add tests for new functionality</li> <li>Update documentation as needed</li> <li> <p>Ensure all tests pass</p> </li> <li> <p>Commit your changes:    <code>bash    git add .    git commit -m \"Add: brief description of your changes\"</code></p> </li> <li> <p>Push and create a Pull Request:    <code>bash    git push origin feature/your-feature-name</code></p> </li> </ol>"},{"location":"contributing/#code-review-guidelines","title":"Code Review Guidelines","text":"<ul> <li>All code must pass CI checks</li> <li>New features require tests and documentation</li> <li>Breaking changes need detailed justification</li> <li>Performance implications should be considered</li> </ul>"},{"location":"contributing/#documentation-contributions","title":"Documentation Contributions","text":"<p>Documentation improvements are always welcome! This includes:</p> <ul> <li>Fixing typos and grammatical errors</li> <li>Adding examples and tutorials</li> <li>Improving API documentation</li> <li>Translating content</li> </ul>"},{"location":"contributing/#development-guidelines","title":"Development Guidelines","text":""},{"location":"contributing/#code-organization","title":"Code Organization","text":"<ul> <li>Core functionality: <code>zarrnii/core.py</code></li> <li>Transformations: <code>zarrnii/transform.py</code> </li> <li>Utilities: <code>zarrnii/utils.py</code></li> <li>Enumerations: <code>zarrnii/enums.py</code></li> </ul>"},{"location":"contributing/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>Use descriptive variable and function names</li> <li>Follow PEP 8 naming conventions</li> <li>Use type hints where appropriate</li> <li>Document complex algorithms and edge cases</li> </ul>"},{"location":"contributing/#error-handling","title":"Error Handling","text":"<ul> <li>Raise informative exceptions with clear messages</li> <li>Use appropriate exception types</li> <li>Handle common error scenarios gracefully</li> <li>Log warnings for non-fatal issues</li> </ul>"},{"location":"contributing/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Use Dask for lazy evaluation when possible</li> <li>Minimize memory allocation in hot paths</li> <li>Profile performance-critical code</li> <li>Consider memory usage for large datasets</li> </ul>"},{"location":"contributing/#api-design-principles","title":"API Design Principles","text":""},{"location":"contributing/#consistency","title":"Consistency","text":"<ul> <li>Methods should have predictable interfaces</li> <li>Similar operations should use similar naming patterns</li> <li>Return types should be consistent across methods</li> </ul>"},{"location":"contributing/#flexibility","title":"Flexibility","text":"<ul> <li>Support multiple input formats where reasonable</li> <li>Provide sensible defaults for optional parameters</li> <li>Allow customization through optional arguments</li> </ul>"},{"location":"contributing/#usability","title":"Usability","text":"<ul> <li>Prioritize common use cases in the main API</li> <li>Provide clear error messages</li> <li>Include comprehensive docstrings</li> </ul>"},{"location":"contributing/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"contributing/#test-categories","title":"Test Categories","text":"<ul> <li>Unit tests: Test individual functions and methods</li> <li>Integration tests: Test component interactions</li> <li>End-to-end tests: Test complete workflows</li> <li>Performance tests: Benchmark critical operations</li> </ul>"},{"location":"contributing/#test-data","title":"Test Data","text":"<ul> <li>Use synthetic data when possible</li> <li>Keep test files small</li> <li>Document data requirements clearly</li> <li>Provide utilities for generating test data</li> </ul>"},{"location":"contributing/#test-structure","title":"Test Structure","text":"<pre><code>def test_feature_description():\n    \"\"\"Test that feature works correctly under normal conditions.\"\"\"\n    # Arrange\n    input_data = create_test_data()\n\n    # Act\n    result = function_under_test(input_data)\n\n    # Assert\n    assert result.shape == expected_shape\n    assert np.allclose(result.data, expected_data)\n</code></pre>"},{"location":"contributing/#release-process","title":"Release Process","text":""},{"location":"contributing/#version-numbering","title":"Version Numbering","text":"<p>ZarrNii uses Semantic Versioning: - MAJOR: Incompatible API changes - MINOR: New functionality, backwards compatible - PATCH: Bug fixes, backwards compatible</p>"},{"location":"contributing/#release-checklist","title":"Release Checklist","text":"<ol> <li>Ensure all tests pass and CI is green</li> <li>Update changelog with new features and fixes</li> <li>Build and test documentation</li> <li>Create and push a git tag:    <code>bash    git tag v1.0.0    git push origin v1.0.0</code></li> <li>GitHub Actions will automatically:</li> <li>Build the package using uv</li> <li>Deploy to PyPI using trusted publishing</li> <li>Update GitHub release notes</li> </ol>"},{"location":"contributing/#community-guidelines","title":"Community Guidelines","text":""},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>We are committed to providing a welcoming and inclusive environment. Please:</p> <ul> <li>Be respectful and considerate</li> <li>Use inclusive language</li> <li>Focus on constructive feedback</li> <li>Help create a positive community</li> </ul>"},{"location":"contributing/#communication","title":"Communication","text":"<ul> <li>GitHub Issues: Bug reports and feature requests</li> <li>GitHub Discussions: Questions and general discussion</li> <li>Pull Requests: Code contributions and reviews</li> </ul>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<p>If you need help with development:</p> <ol> <li>Check existing documentation and examples</li> <li>Search through GitHub issues</li> <li>Create a new issue with your question</li> <li>Join community discussions</li> </ol>"},{"location":"contributing/#recognition","title":"Recognition","text":"<p>Contributors will be recognized in: - Project README - Release notes - Documentation acknowledgments</p> <p>Thank you for helping make ZarrNii better!</p>"},{"location":"faq/","title":"FAQ: Frequently Asked Questions","text":"<p>This page addresses common questions and provides troubleshooting tips for using ZarrNii.</p>"},{"location":"faq/#general-questions","title":"General Questions","text":""},{"location":"faq/#1-what-is-zarrnii","title":"1. What is ZarrNii?","text":"<p>ZarrNii is a Python library that bridges the gap between OME-Zarr and NIfTI formats, enabling seamless conversion, transformations, and metadata handling for multidimensional biomedical images.</p>"},{"location":"faq/#2-what-formats-does-zarrnii-support","title":"2. What formats does ZarrNii support?","text":"<p>ZarrNii supports: - OME-Zarr: A format for storing chunked, multidimensional microscopy images. - NIfTI: A format commonly used for neuroimaging data.</p>"},{"location":"faq/#3-can-zarrnii-handle-large-datasets","title":"3. Can ZarrNii handle large datasets?","text":"<p>Yes! ZarrNii uses Dask arrays to handle datasets that don't fit into memory. Most transformations are lazy, meaning computations are only performed when explicitly triggered using <code>.compute()</code>.</p>"},{"location":"faq/#installation-issues","title":"Installation Issues","text":""},{"location":"faq/#1-i-installed-zarrnii-but-i-cant-import-it","title":"1. I installed ZarrNii, but I can't import it.","text":"<p>Ensure that ZarrNii is installed in the correct Python environment. Use <code>uv tree</code> or <code>pip show zarrnii</code> to verify the installation.</p> <p>If you're still encountering issues, try reinstalling the library:</p> <pre><code>uv sync --dev\n</code></pre>"},{"location":"faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"faq/#performance-tips","title":"Performance Tips","text":""},{"location":"faq/#1-how-can-i-speed-up-transformations-on-large-datasets","title":"1. How can I speed up transformations on large datasets?","text":"<ul> <li>Use appropriate chunk sizes with <code>.rechunk()</code> for operations like downsampling or interpolation.</li> <li>Trigger computations only when necessary using <code>.compute()</code>.</li> </ul>"},{"location":"faq/#2-how-do-i-optimize-multiscale-processing","title":"2. How do I optimize multiscale processing?","text":"<p>For OME-Zarr datasets with multiscale pyramids: 1. Use the appropriate <code>level</code> when loading the dataset.</p> <pre><code>znimg = ZarrNii.from_ome_zarr(\"path/to/dataset.zarr\", level=2)\n</code></pre>"},{"location":"faq/#metadata-questions","title":"Metadata Questions","text":""},{"location":"faq/#1-how-do-i-access-ome-zarr-metadata","title":"1. How do I access OME-Zarr metadata?","text":"<p>ZarrNii provides attributes for accessing metadata:</p> <pre><code>print(\"Axes:\", znimg.axes)\nprint(\"Coordinate transformations:\", znimg.coordinate_transformations)\nprint(\"Omero metadata:\", znimg.omero)\n</code></pre>"},{"location":"faq/#2-does-zarrnii-preserve-metadata-during-transformations","title":"2. Does ZarrNii preserve metadata during transformations?","text":"<p>Yes, ZarrNii updates the metadata to remain consistent with transformations like cropping, downsampling, or affine transformations.</p>"},{"location":"faq/#getting-help","title":"Getting Help","text":"<p>If you encounter issues not covered here: 1. Check the API Reference for detailed information about ZarrNii methods. 2. Open an issue on the GitHub repository.</p>"},{"location":"faq/#summary","title":"Summary","text":"<p>This FAQ covers common questions about ZarrNii, troubleshooting tips, and best practices for working with large datasets and metadata. For more in-depth information, explore: - Examples - API Reference</p>"},{"location":"reference/","title":"API Reference","text":"<p>This page documents the core classes, methods, and functions in ZarrNii. </p>"},{"location":"reference/#core-classes","title":"Core Classes","text":""},{"location":"reference/#zarrnii","title":"ZarrNii","text":"<p>The <code>ZarrNii</code> class provides tools for reading, writing, and transforming datasets in OME-Zarr and NIfTI formats.</p>"},{"location":"reference/#key-methods","title":"Key Methods","text":"<ul> <li><code>from_ome_zarr</code>: Load data from OME-Zarr.</li> <li><code>from_nifti</code>: Load data from a NIfTI file.</li> <li><code>to_ome_zarr</code>: Save data as OME-Zarr.</li> <li><code>to_nifti</code>: Save data as a NIfTI file.</li> <li><code>crop</code>: Extract a region from the dataset.</li> <li><code>downsample</code>: Reduce resolution of datasets.</li> <li><code>upsample</code>: Increase resolution of datasets.</li> <li><code>apply_transform</code>: Apply spatial transformations.</li> </ul> <p>Zarr-based image with NIfTI compatibility using NgffImage internally.</p> <p>This class provides chainable operations on OME-Zarr data while maintaining compatibility with NIfTI workflows. It uses NgffImage objects internally for better multiscale support and metadata preservation.</p> <p>Attributes:</p> Name Type Description <code>ngff_image</code> <code>NgffImage</code> <p>The internal NgffImage object containing data and metadata.</p> <code>axes_order</code> <code>str</code> <p>The order of the axes for NIfTI compatibility ('ZYX' or 'XYZ').</p> <code>orientation</code> <code>str</code> <p>The anatomical orientation string (e.g., 'RAS', 'LPI').</p> <p>Constructor with backward compatibility for old signature.</p> Source code in <code>zarrnii/core.py</code> <pre><code>def __init__(\n    self,\n    darr=None,\n    affine=None,\n    axes_order=\"ZYX\",\n    orientation=\"RAS\",\n    ngff_image=None,\n    _omero=None,\n    **kwargs,\n):\n    \"\"\"\n    Constructor with backward compatibility for old signature.\n    \"\"\"\n    if ngff_image is not None:\n        # New signature\n        object.__setattr__(self, \"ngff_image\", ngff_image)\n        object.__setattr__(self, \"axes_order\", axes_order)\n        object.__setattr__(self, \"orientation\", orientation)\n        object.__setattr__(self, \"_omero\", _omero)\n    elif darr is not None:\n        # Legacy signature - delegate to from_darr\n        instance = self.from_darr(\n            darr=darr,\n            affine=affine,\n            axes_order=axes_order,\n            orientation=orientation,\n            **kwargs,\n        )\n        object.__setattr__(self, \"ngff_image\", instance.ngff_image)\n        object.__setattr__(self, \"axes_order\", instance.axes_order)\n        object.__setattr__(self, \"orientation\", instance.orientation)\n        object.__setattr__(self, \"_omero\", instance._omero)\n    else:\n        raise ValueError(\"Must provide either ngff_image or darr\")\n</code></pre>"},{"location":"reference/#methods","title":"Methods","text":""},{"location":"reference/#from_ome_zarr","title":"<code>from_ome_zarr</code>","text":"<p>Load from OME-Zarr store.</p> <p>Parameters:</p> Name Type Description Default <code>store_or_path</code> <p>Store or path to OME-Zarr file</p> required <code>level</code> <code>int</code> <p>Pyramid level to load (if beyond available levels, lazy downsampling is applied)</p> <code>0</code> <code>channels</code> <code>Optional[List[int]]</code> <p>Channel indices to load</p> <code>None</code> <code>channel_labels</code> <code>Optional[List[str]]</code> <p>Channel labels to load</p> <code>None</code> <code>timepoints</code> <code>Optional[List[int]]</code> <p>Timepoint indices to load</p> <code>None</code> <code>storage_options</code> <code>Optional[Dict]</code> <p>Storage options for Zarr</p> <code>None</code> <code>axes_order</code> <code>str</code> <p>Spatial axes order for NIfTI compatibility</p> <code>'ZYX'</code> <code>orientation</code> <code>str</code> <p>Default input orientation if none is specified in metadata (default: 'RAS')</p> <code>'RAS'</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>ZarrNii instance</p> Source code in <code>zarrnii/core.py</code> <pre><code>@classmethod\ndef from_ome_zarr(\n    cls,\n    store_or_path,\n    level: int = 0,\n    channels: Optional[List[int]] = None,\n    channel_labels: Optional[List[str]] = None,\n    timepoints: Optional[List[int]] = None,\n    storage_options: Optional[Dict] = None,\n    axes_order: str = \"ZYX\",\n    orientation: str = \"RAS\",\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Load from OME-Zarr store.\n\n    Args:\n        store_or_path: Store or path to OME-Zarr file\n        level: Pyramid level to load (if beyond available levels, lazy downsampling is applied)\n        channels: Channel indices to load\n        channel_labels: Channel labels to load\n        timepoints: Timepoint indices to load\n        storage_options: Storage options for Zarr\n        axes_order: Spatial axes order for NIfTI compatibility\n        orientation: Default input orientation if none is specified in metadata (default: 'RAS')\n\n    Returns:\n        ZarrNii instance\n    \"\"\"\n    # Validate channel and timepoint selection arguments\n    if channels is not None and channel_labels is not None:\n        raise ValueError(\"Cannot specify both 'channels' and 'channel_labels'\")\n\n    # Load the multiscales object\n    try:\n        if isinstance(store_or_path, str):\n            multiscales = nz.from_ngff_zarr(\n                store_or_path, storage_options=storage_options or {}\n            )\n        else:\n            multiscales = nz.from_ngff_zarr(store_or_path)\n    except Exception as e:\n        # Fallback for older zarr/ngff_zarr versions\n        if isinstance(store_or_path, str):\n            store = fsspec.get_mapper(store_or_path, **storage_options or {})\n        else:\n            store = store_or_path\n        multiscales = nz.from_ngff_zarr(store)\n\n    # Extract omero metadata if available\n    omero_metadata = None\n    try:\n        import zarr\n\n        if isinstance(store_or_path, str):\n            group = zarr.open_group(store_or_path, mode=\"r\")\n        else:\n            group = zarr.open_group(store_or_path, mode=\"r\")\n\n        if \"omero\" in group.attrs:\n            omero_dict = group.attrs[\"omero\"]\n\n            # Create a simple object to hold omero metadata\n            class OmeroMetadata:\n                def __init__(self, omero_dict):\n                    self.channels = []\n                    if \"channels\" in omero_dict:\n                        for ch_dict in omero_dict[\"channels\"]:\n                            # Create channel objects\n                            class ChannelMetadata:\n                                def __init__(self, ch_dict):\n                                    self.label = ch_dict.get(\"label\", \"\")\n                                    self.color = ch_dict.get(\"color\", \"\")\n                                    if \"window\" in ch_dict:\n\n                                        class WindowMetadata:\n                                            def __init__(self, win_dict):\n                                                self.min = win_dict.get(\"min\", 0.0)\n                                                self.max = win_dict.get(\n                                                    \"max\", 65535.0\n                                                )\n                                                self.start = win_dict.get(\n                                                    \"start\", 0.0\n                                                )\n                                                self.end = win_dict.get(\n                                                    \"end\", 65535.0\n                                                )\n\n                                        self.window = WindowMetadata(\n                                            ch_dict[\"window\"]\n                                        )\n                                    else:\n                                        self.window = None\n\n                            self.channels.append(ChannelMetadata(ch_dict))\n\n            omero_metadata = OmeroMetadata(omero_dict)\n    except Exception:\n        # If we can't load omero metadata, that's okay\n        pass\n\n    # Read orientation metadata (default to the provided orientation if not present)\n    try:\n        import zarr\n\n        if isinstance(store_or_path, str):\n            group = zarr.open_group(store_or_path, mode=\"r\")\n        else:\n            group = zarr.open_group(store_or_path, mode=\"r\")\n\n        # Get orientation from zarr metadata, fallback to provided orientation\n        orientation = group.attrs.get(\"orientation\", orientation)\n    except Exception:\n        # If we can't read orientation metadata, use the provided default\n        pass\n\n    # Determine the available pyramid levels and handle lazy downsampling\n    max_level = len(multiscales.images) - 1\n    actual_level = min(level, max_level)\n    do_downsample = level &gt; max_level\n\n    # Get the highest available level\n    ngff_image = multiscales.images[actual_level]\n\n    # Handle channel and timepoint selection and filter omero metadata accordingly\n    filtered_omero = omero_metadata\n    if channels is not None or channel_labels is not None or timepoints is not None:\n        ngff_image, filtered_omero = _select_dimensions_from_image_with_omero(\n            ngff_image,\n            multiscales,\n            channels,\n            channel_labels,\n            timepoints,\n            omero_metadata,\n        )\n\n    # Create ZarrNii instance with orientation\n    znimg = cls(\n        ngff_image=ngff_image,\n        axes_order=axes_order,\n        orientation=orientation,\n        _omero=filtered_omero,\n    )\n\n    # Apply lazy downsampling if needed\n    if do_downsample:\n        level_ds = level - max_level\n        downsample_factor = 2**level_ds\n\n        # Get spatial dims based on axes order\n        spatial_dims = [\"z\", \"y\", \"x\"] if axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n\n        # Apply downsampling using the existing method\n        znimg = znimg.downsample(\n            factors=downsample_factor, spatial_dims=spatial_dims\n        )\n\n    return znimg\n</code></pre>"},{"location":"reference/#from_nifti","title":"<code>from_nifti</code>","text":"<p>Load from NIfTI file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>Path to NIfTI file</p> required <code>chunks</code> <p>Chunking strategy for dask array</p> <code>'auto'</code> <code>axes_order</code> <p>Spatial axes order</p> <code>'XYZ'</code> <code>name</code> <p>Name for the NgffImage</p> <code>None</code> <code>as_ref</code> <p>If True, creates an empty dask array with the correct shape instead of loading data</p> <code>False</code> <code>zooms</code> <p>Target voxel spacing in xyz (only valid if as_ref=True)</p> <code>None</code> <p>Returns:</p> Type Description <p>ZarrNii instance</p> Source code in <code>zarrnii/core.py</code> <pre><code>@classmethod\ndef from_nifti(\n    cls, path, chunks=\"auto\", axes_order=\"XYZ\", name=None, as_ref=False, zooms=None\n):\n    \"\"\"\n    Load from NIfTI file.\n\n    Args:\n        path: Path to NIfTI file\n        chunks: Chunking strategy for dask array\n        axes_order: Spatial axes order\n        name: Name for the NgffImage\n        as_ref: If True, creates an empty dask array with the correct shape instead of loading data\n        zooms: Target voxel spacing in xyz (only valid if as_ref=True)\n\n    Returns:\n        ZarrNii instance\n    \"\"\"\n    if not as_ref and zooms is not None:\n        raise ValueError(\"`zooms` can only be used when `as_ref=True`.\")\n\n    # Load NIfTI file\n    nifti_img = nib.load(path)\n    shape = nifti_img.header.get_data_shape()\n    affine_matrix = nifti_img.affine.copy()\n\n    # Adjust shape and affine if zooms are provided\n    if zooms is not None:\n        in_zooms = np.sqrt(\n            (affine_matrix[:3, :3] ** 2).sum(axis=0)\n        )  # Current voxel spacing\n        scaling_factor = in_zooms / zooms\n        new_shape = [\n            int(np.floor(shape[0] * scaling_factor[2])),  # Z\n            int(np.floor(shape[1] * scaling_factor[1])),  # Y\n            int(np.floor(shape[2] * scaling_factor[0])),  # X\n        ]\n        np.fill_diagonal(affine_matrix[:3, :3], zooms)\n    else:\n        new_shape = shape\n\n    if as_ref:\n        # Create an empty dask array with the adjusted shape\n        darr = da.empty((1, *new_shape), chunks=chunks, dtype=\"float32\")\n    else:\n        # Load the NIfTI data and convert to a dask array\n        array = nifti_img.get_fdata()\n        darr = da.from_array(array, chunks=chunks)\n\n    # Add channel and time dimensions if not present\n    original_ndim = len(darr.shape)\n\n    if original_ndim == 3:\n        # 3D data: add channel dimension -&gt; (c, z, y, x) or (c, x, y, z)\n        darr = darr[np.newaxis, ...]\n    elif original_ndim == 4:\n        # 4D data: could be (c, z, y, x) or (t, z, y, x) - assume channel by default\n        # User can specify if it's time by using appropriate axes_order\n        pass  # Keep as is - 4D is already handled\n    elif original_ndim == 5:\n        # 5D data: assume (t, z, y, x, c) and handle appropriately\n        pass  # Keep as is - 5D is already the target format\n    else:\n        # For 1D, 2D, or &gt;5D data, add channel dimension and let user handle\n        darr = darr[np.newaxis, ...]\n\n    # Create dimensions based on data shape after dimension adjustments\n    final_ndim = len(darr.shape)\n    if final_ndim == 4:\n        # 4D: (c, z, y, x) or (c, x, y, z) - standard case\n        dims = [\"c\"] + list(axes_order.lower())\n    elif final_ndim == 5:\n        # 5D: (t, c, z, y, x) or (t, c, x, y, z) - time dimension included\n        dims = [\"t\", \"c\"] + list(axes_order.lower())\n    else:\n        # Fallback for other cases\n        dims = [\"c\"] + list(axes_order.lower())\n\n    # Extract scale and translation from affine\n    scale = {}\n    translation = {}\n    spatial_dims = [\"z\", \"y\", \"x\"] if axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n\n    for i, dim in enumerate(spatial_dims):\n        scale[dim] = np.sqrt((affine_matrix[i, :3] ** 2).sum())\n        translation[dim] = affine_matrix[i, 3]\n\n    # Create NgffImage\n    if name is None:\n        name = f\"nifti_image_{path}\"\n\n    ngff_image = nz.NgffImage(\n        data=darr, dims=dims, scale=scale, translation=translation, name=name\n    )\n\n    return cls(ngff_image=ngff_image, axes_order=axes_order)\n</code></pre>"},{"location":"reference/#to_ome_zarr","title":"<code>to_ome_zarr</code>","text":"<p>Save to OME-Zarr store and return self for continued chaining.</p> <p>OME-Zarr files are always written in ZYX order. If the current axes_order is XYZ, the data will be reordered to ZYX before writing.</p> <p>Parameters:</p> Name Type Description Default <code>store_or_path</code> <p>Target store or path</p> required <code>max_layer</code> <code>int</code> <p>Maximum number of pyramid levels</p> <code>4</code> <code>scale_factors</code> <code>Optional[List[int]]</code> <p>Custom scale factors for pyramid levels</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments for to_ngff_zarr</p> <code>{}</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>Self for continued chaining</p> Source code in <code>zarrnii/core.py</code> <pre><code>def to_ome_zarr(\n    self,\n    store_or_path,\n    max_layer: int = 4,\n    scale_factors: Optional[List[int]] = None,\n    **kwargs,\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Save to OME-Zarr store and return self for continued chaining.\n\n    OME-Zarr files are always written in ZYX order. If the current axes_order is XYZ,\n    the data will be reordered to ZYX before writing.\n\n    Args:\n        store_or_path: Target store or path\n        max_layer: Maximum number of pyramid levels\n        scale_factors: Custom scale factors for pyramid levels\n        **kwargs: Additional arguments for to_ngff_zarr\n\n    Returns:\n        Self for continued chaining\n    \"\"\"\n    # Determine the image to save\n    if self.axes_order == \"XYZ\":\n        # Need to reorder data from XYZ to ZYX for OME-Zarr\n        ngff_image_to_save = self._create_zyx_ngff_image()\n    else:\n        # Already in ZYX order\n        ngff_image_to_save = self.ngff_image\n\n    save_ngff_image(\n        ngff_image_to_save, store_or_path, max_layer, scale_factors, **kwargs\n    )\n\n    # Add orientation metadata to the zarr store\n    try:\n        import zarr\n\n        if isinstance(store_or_path, str):\n            group = zarr.open_group(store_or_path, mode=\"r+\")\n        else:\n            group = zarr.open_group(store_or_path, mode=\"r+\")\n\n        # Add metadata for orientation\n        if hasattr(self, \"orientation\") and self.orientation:\n            group.attrs[\"orientation\"] = self.orientation\n    except Exception:\n        # If we can't write orientation metadata, that's not critical\n        pass\n\n    return self\n</code></pre>"},{"location":"reference/#to_nifti","title":"<code>to_nifti</code>","text":"<p>Convert to NIfTI format.</p> <p>NIfTI files are always written in XYZ order. If the current axes_order is ZYX, the data will be reordered to XYZ and the affine matrix adjusted accordingly.</p> <p>For 5D data (T,C,Z,Y,X), singleton dimensions are removed automatically. Non-singleton time and channel dimensions will raise an error as NIfTI doesn't  support more than 4D data.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <p>Output filename, if None return nibabel image</p> <code>None</code> <p>Returns:</p> Type Description <p>nibabel.Nifti1Image or path if filename provided</p> Source code in <code>zarrnii/core.py</code> <pre><code>def to_nifti(self, filename=None):\n    \"\"\"\n    Convert to NIfTI format.\n\n    NIfTI files are always written in XYZ order. If the current axes_order is ZYX,\n    the data will be reordered to XYZ and the affine matrix adjusted accordingly.\n\n    For 5D data (T,C,Z,Y,X), singleton dimensions are removed automatically.\n    Non-singleton time and channel dimensions will raise an error as NIfTI doesn't \n    support more than 4D data.\n\n    Args:\n        filename: Output filename, if None return nibabel image\n\n    Returns:\n        nibabel.Nifti1Image or path if filename provided\n    \"\"\"\n    # Get data and dimensions\n    data = self.data.compute()\n    dims = self.dims\n\n    # Handle dimensional reduction for NIfTI compatibility\n    # NIfTI supports up to 4D, so we need to remove singleton dimensions\n    squeeze_axes = []\n    remaining_dims = []\n\n    for i, dim in enumerate(dims):\n        if dim in ['t', 'c'] and data.shape[i] == 1:\n            # Remove singleton time or channel dimensions\n            squeeze_axes.append(i)\n        elif dim in ['t', 'c'] and data.shape[i] &gt; 1:\n            # Non-singleton time or channel dimensions - NIfTI can't handle this\n            raise ValueError(f\"NIfTI format doesn't support non-singleton {dim} dimension. \"\n                           f\"Dimension '{dim}' has size {data.shape[i]}. \"\n                           f\"Consider selecting specific timepoints/channels first.\")\n        else:\n            remaining_dims.append(dim)\n\n    # Squeeze out singleton dimensions\n    if squeeze_axes:\n        data = np.squeeze(data, axis=tuple(squeeze_axes))\n\n    # Check final dimensionality\n    if data.ndim &gt; 4:\n        raise ValueError(f\"Resulting data has {data.ndim} dimensions, but NIfTI supports maximum 4D\")\n\n    # Now handle spatial reordering based on axes_order\n    if self.axes_order == \"ZYX\":\n        # Data spatial dimensions are in ZYX order, need to transpose to XYZ\n        if data.ndim == 3:\n            # Pure spatial data: ZYX -&gt; XYZ\n            data = data.transpose(2, 1, 0)\n        elif data.ndim == 4:\n            # 4D data with one non-spatial dimension remaining\n            # Could be (T,Z,Y,X) or (C,Z,Y,X) - spatial part needs ZYX-&gt;XYZ\n            # The non-spatial dimension stays first\n            data = data.transpose(0, 3, 2, 1)\n\n        # Get affine matrix in XYZ order\n        affine_matrix = self.get_affine_matrix(axes_order=\"XYZ\")\n    else:\n        # Data is already in XYZ order  \n        affine_matrix = self.get_affine_matrix(axes_order=\"XYZ\")\n\n    # Create NIfTI image\n    nifti_img = nib.Nifti1Image(data, affine_matrix)\n\n    if filename is not None:\n        nib.save(nifti_img, filename)\n        return filename\n    else:\n        return nifti_img\n</code></pre>"},{"location":"reference/#crop","title":"<code>crop</code>","text":"<p>Crop the image and return a new ZarrNii instance.</p> <p>Parameters:</p> Name Type Description Default <code>bbox_min</code> <code>tuple</code> <p>Minimum corner of bounding box</p> required <code>bbox_max</code> <code>tuple</code> <p>Maximum corner of bounding box</p> required <code>spatial_dims</code> <code>List[str]</code> <p>Names of spatial dimensions (derived from axes_order if None)</p> <code>None</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>New ZarrNii with cropped data</p> Source code in <code>zarrnii/core.py</code> <pre><code>def crop(\n    self, bbox_min: tuple, bbox_max: tuple, spatial_dims: List[str] = None\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Crop the image and return a new ZarrNii instance.\n\n    Args:\n        bbox_min: Minimum corner of bounding box\n        bbox_max: Maximum corner of bounding box\n        spatial_dims: Names of spatial dimensions (derived from axes_order if None)\n\n    Returns:\n        New ZarrNii with cropped data\n    \"\"\"\n    if spatial_dims is None:\n        spatial_dims = (\n            [\"z\", \"y\", \"x\"] if self.axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n        )\n    cropped_image = crop_ngff_image(\n        self.ngff_image, bbox_min, bbox_max, spatial_dims\n    )\n    return ZarrNii(\n        ngff_image=cropped_image,\n        axes_order=self.axes_order,\n        orientation=self.orientation,\n        _omero=self._omero,\n    )\n</code></pre>"},{"location":"reference/#downsample","title":"<code>downsample</code>","text":"<p>Downsample the image and return a new ZarrNii instance.</p> <p>Parameters:</p> Name Type Description Default <code>factors</code> <code>Union[int, List[int]]</code> <p>Downsampling factors (int for isotropic, list for per-dimension)</p> <code>None</code> <code>along_x</code> <code>int</code> <p>Legacy parameter for X downsampling</p> <code>1</code> <code>along_y</code> <code>int</code> <p>Legacy parameter for Y downsampling</p> <code>1</code> <code>along_z</code> <code>int</code> <p>Legacy parameter for Z downsampling</p> <code>1</code> <code>level</code> <code>int</code> <p>Legacy parameter for level-based downsampling (2^level)</p> <code>None</code> <code>spatial_dims</code> <code>List[str]</code> <p>Names of spatial dimensions (derived from axes_order if None)</p> <code>None</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>New ZarrNii with downsampled data</p> Source code in <code>zarrnii/core.py</code> <pre><code>def downsample(\n    self,\n    factors: Union[int, List[int]] = None,\n    along_x: int = 1,\n    along_y: int = 1,\n    along_z: int = 1,\n    level: int = None,\n    spatial_dims: List[str] = None,\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Downsample the image and return a new ZarrNii instance.\n\n    Args:\n        factors: Downsampling factors (int for isotropic, list for per-dimension)\n        along_x: Legacy parameter for X downsampling\n        along_y: Legacy parameter for Y downsampling\n        along_z: Legacy parameter for Z downsampling\n        level: Legacy parameter for level-based downsampling (2^level)\n        spatial_dims: Names of spatial dimensions (derived from axes_order if None)\n\n    Returns:\n        New ZarrNii with downsampled data\n    \"\"\"\n    # Handle legacy parameters\n    if factors is None:\n        if level is not None:\n            factors = 2**level\n        else:\n            factors = (\n                [along_z, along_y, along_x]\n                if self.axes_order == \"ZYX\"\n                else [along_x, along_y, along_z]\n            )\n\n    if spatial_dims is None:\n        spatial_dims = (\n            [\"z\", \"y\", \"x\"] if self.axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n        )\n\n    downsampled_image = downsample_ngff_image(\n        self.ngff_image, factors, spatial_dims\n    )\n    return ZarrNii(\n        ngff_image=downsampled_image,\n        axes_order=self.axes_order,\n        orientation=self.orientation,\n        _omero=self._omero,\n    )\n</code></pre>"},{"location":"reference/#upsample","title":"<code>upsample</code>","text":"<p>Upsamples the ZarrNii instance using <code>scipy.ndimage.zoom</code>.</p> <p>Parameters:</p> Name Type Description Default <code>along_x</code> <code>int</code> <p>Upsampling factor along the X-axis (default: 1).</p> <code>1</code> <code>along_y</code> <code>int</code> <p>Upsampling factor along the Y-axis (default: 1).</p> <code>1</code> <code>along_z</code> <code>int</code> <p>Upsampling factor along the Z-axis (default: 1).</p> <code>1</code> <code>to_shape</code> <code>tuple</code> <p>Target shape for upsampling. Should include all dimensions                          (e.g., <code>(c, z, y, x)</code> for ZYX or <code>(c, x, y, z)</code> for XYZ).                          If provided, <code>along_x</code>, <code>along_y</code>, and <code>along_z</code> are ignored.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ZarrNii</code> <p>A new ZarrNii instance with the upsampled data and updated affine.</p> Notes <ul> <li>This method supports both direct scaling via <code>along_*</code> factors or target shape via <code>to_shape</code>.</li> <li>If <code>to_shape</code> is provided, chunk sizes and scaling factors are dynamically calculated.</li> <li>The affine matrix is updated to reflect the new voxel size after upsampling.</li> </ul> Example Source code in <code>zarrnii/core.py</code> <pre><code>def upsample(self, along_x=1, along_y=1, along_z=1, to_shape=None):\n    \"\"\"\n    Upsamples the ZarrNii instance using `scipy.ndimage.zoom`.\n\n    Parameters:\n        along_x (int, optional): Upsampling factor along the X-axis (default: 1).\n        along_y (int, optional): Upsampling factor along the Y-axis (default: 1).\n        along_z (int, optional): Upsampling factor along the Z-axis (default: 1).\n        to_shape (tuple, optional): Target shape for upsampling. Should include all dimensions\n                                     (e.g., `(c, z, y, x)` for ZYX or `(c, x, y, z)` for XYZ).\n                                     If provided, `along_x`, `along_y`, and `along_z` are ignored.\n\n    Returns:\n        ZarrNii: A new ZarrNii instance with the upsampled data and updated affine.\n\n    Notes:\n        - This method supports both direct scaling via `along_*` factors or target shape via `to_shape`.\n        - If `to_shape` is provided, chunk sizes and scaling factors are dynamically calculated.\n        - The affine matrix is updated to reflect the new voxel size after upsampling.\n\n    Example:\n        # Upsample with scaling factors\n        upsampled_znimg = znimg.upsample(along_x=2, along_y=2, along_z=2)\n\n        # Upsample to a specific shape\n        upsampled_znimg = znimg.upsample(to_shape=(1, 256, 256, 256))\n    \"\"\"\n    # Determine scaling and chunks based on input parameters\n    if to_shape is None:\n        if self.axes_order == \"XYZ\":\n            scaling = (1, along_x, along_y, along_z)\n        else:\n            scaling = (1, along_z, along_y, along_x)\n\n        chunks_out = tuple(\n            tuple(c * scale for c in chunks_i)\n            for chunks_i, scale in zip(self.data.chunks, scaling)\n        )\n    else:\n        chunks_out, scaling = self.__get_upsampled_chunks(to_shape)\n\n    # Define block-wise upsampling function\n    def zoom_blocks(x, block_info=None):\n        \"\"\"\n        Scales blocks to the desired size using `scipy.ndimage.zoom`.\n\n        Parameters:\n            x (np.ndarray): Input block data.\n            block_info (dict, optional): Metadata about the current block.\n\n        Returns:\n            np.ndarray: The upscaled block.\n        \"\"\"\n        # Calculate scaling factors based on input and output chunk shapes\n        scaling = tuple(\n            out_n / in_n\n            for out_n, in_n in zip(block_info[None][\"chunk-shape\"], x.shape)\n        )\n        return zoom(x, scaling, order=1, prefilter=False)\n\n    # Perform block-wise upsampling\n    darr_scaled = da.map_blocks(\n        zoom_blocks, self.data, dtype=self.data.dtype, chunks=chunks_out\n    )\n\n    # Update the affine matrix to reflect the new voxel size\n    if self.axes_order == \"XYZ\":\n        scaling_matrix = np.diag(\n            (1 / scaling[1], 1 / scaling[2], 1 / scaling[3], 1)\n        )\n    else:\n        scaling_matrix = np.diag(\n            (1 / scaling[-1], 1 / scaling[-2], 1 / scaling[-3], 1)\n        )\n    new_affine = AffineTransform.from_array(scaling_matrix @ self.affine.matrix)\n\n    # Create new NgffImage with upsampled data\n    dims = self.dims\n    if self.axes_order == \"XYZ\":\n        new_scale = {\n            dims[1]: self.scale[dims[1]] / scaling[1],\n            dims[2]: self.scale[dims[2]] / scaling[2],\n            dims[3]: self.scale[dims[3]] / scaling[3],\n        }\n    else:\n        new_scale = {\n            dims[1]: self.scale[dims[1]] / scaling[1],\n            dims[2]: self.scale[dims[2]] / scaling[2],\n            dims[3]: self.scale[dims[3]] / scaling[3],\n        }\n\n    upsampled_ngff = nz.to_ngff_image(\n        darr_scaled,\n        dims=dims,\n        scale=new_scale,\n        translation=self.translation.copy(),\n        name=self.name,\n    )\n\n    # Return a new ZarrNii instance with the upsampled data\n    return ZarrNii.from_ngff_image(\n        upsampled_ngff,\n        axes_order=self.axes_order,\n        orientation=self.orientation,\n        omero=self.omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.upsample--upsample-with-scaling-factors","title":"Upsample with scaling factors","text":"<p>upsampled_znimg = znimg.upsample(along_x=2, along_y=2, along_z=2)</p>"},{"location":"reference/#zarrnii.ZarrNii.upsample--upsample-to-a-specific-shape","title":"Upsample to a specific shape","text":"<p>upsampled_znimg = znimg.upsample(to_shape=(1, 256, 256, 256))</p>"},{"location":"reference/#apply_transform","title":"<code>apply_transform</code>","text":"<p>Apply spatial transformation and return a new ZarrNii instance.</p> <p>Parameters:</p> Name Type Description Default <code>transforms</code> <code>Transform</code> <p>Transformations to apply</p> <code>()</code> <code>ref_znimg</code> <code>'ZarrNii'</code> <p>Reference ZarrNii defining output space</p> required <code>spatial_dims</code> <code>List[str]</code> <p>Names of spatial dimensions (derived from axes_order if None)</p> <code>None</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>New ZarrNii with transformed data</p> Source code in <code>zarrnii/core.py</code> <pre><code>def apply_transform(\n    self,\n    *transforms: Transform,\n    ref_znimg: \"ZarrNii\",\n    spatial_dims: List[str] = None,\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Apply spatial transformation and return a new ZarrNii instance.\n\n    Args:\n        transforms: Transformations to apply\n        ref_znimg: Reference ZarrNii defining output space\n        spatial_dims: Names of spatial dimensions (derived from axes_order if None)\n\n    Returns:\n        New ZarrNii with transformed data\n    \"\"\"\n    if spatial_dims is None:\n        spatial_dims = (\n            [\"z\", \"y\", \"x\"] if self.axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n        )\n\n    # For now, just apply the first transform (placeholder)\n    if transforms:\n        transformed_image = apply_transform_to_ngff_image(\n            self.ngff_image, transforms[0], ref_znimg.ngff_image, spatial_dims\n        )\n    else:\n        transformed_image = self.ngff_image\n\n    return ZarrNii(\n        ngff_image=transformed_image,\n        axes_order=self.axes_order,\n        orientation=self.orientation,\n        _omero=self._omero,\n    )\n</code></pre>"},{"location":"reference/#remaining-methods-and-functions","title":"Remaining Methods and Functions","text":"<p>Zarr-based image with NIfTI compatibility using NgffImage internally.</p> <p>This class provides chainable operations on OME-Zarr data while maintaining compatibility with NIfTI workflows. It uses NgffImage objects internally for better multiscale support and metadata preservation.</p> <p>Attributes:</p> Name Type Description <code>ngff_image</code> <code>NgffImage</code> <p>The internal NgffImage object containing data and metadata.</p> <code>axes_order</code> <code>str</code> <p>The order of the axes for NIfTI compatibility ('ZYX' or 'XYZ').</p> <code>orientation</code> <code>str</code> <p>The anatomical orientation string (e.g., 'RAS', 'LPI').</p> <p>Constructor with backward compatibility for old signature.</p> Source code in <code>zarrnii/core.py</code> <pre><code>def __init__(\n    self,\n    darr=None,\n    affine=None,\n    axes_order=\"ZYX\",\n    orientation=\"RAS\",\n    ngff_image=None,\n    _omero=None,\n    **kwargs,\n):\n    \"\"\"\n    Constructor with backward compatibility for old signature.\n    \"\"\"\n    if ngff_image is not None:\n        # New signature\n        object.__setattr__(self, \"ngff_image\", ngff_image)\n        object.__setattr__(self, \"axes_order\", axes_order)\n        object.__setattr__(self, \"orientation\", orientation)\n        object.__setattr__(self, \"_omero\", _omero)\n    elif darr is not None:\n        # Legacy signature - delegate to from_darr\n        instance = self.from_darr(\n            darr=darr,\n            affine=affine,\n            axes_order=axes_order,\n            orientation=orientation,\n            **kwargs,\n        )\n        object.__setattr__(self, \"ngff_image\", instance.ngff_image)\n        object.__setattr__(self, \"axes_order\", instance.axes_order)\n        object.__setattr__(self, \"orientation\", instance.orientation)\n        object.__setattr__(self, \"_omero\", instance._omero)\n    else:\n        raise ValueError(\"Must provide either ngff_image or darr\")\n</code></pre> <p>               Bases: <code>Transform</code></p> <p>               Bases: <code>Transform</code></p>"},{"location":"reference/#zarrnii.ZarrNii._omero","title":"<code>_omero = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/#zarrnii.ZarrNii.affine","title":"<code>affine</code>  <code>property</code>","text":"<p>Affine transformation matrix derived from NgffImage scale and translation.</p> <p>Returns:</p> Name Type Description <code>AffineTransform</code> <code>AffineTransform</code> <p>4x4 affine transformation matrix</p>"},{"location":"reference/#zarrnii.ZarrNii.axes","title":"<code>axes</code>  <code>property</code>","text":"<p>Axes metadata - derived from NgffImage for compatibility.</p>"},{"location":"reference/#zarrnii.ZarrNii.axes_order","title":"<code>axes_order = 'ZYX'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/#zarrnii.ZarrNii.coordinate_transformations","title":"<code>coordinate_transformations</code>  <code>property</code>","text":"<p>Coordinate transformations - derived from NgffImage scale/translation.</p>"},{"location":"reference/#zarrnii.ZarrNii.darr","title":"<code>darr</code>  <code>property</code> <code>writable</code>","text":"<p>Legacy property name for image data.</p>"},{"location":"reference/#zarrnii.ZarrNii.data","title":"<code>data</code>  <code>property</code> <code>writable</code>","text":"<p>Access the image data (dask array).</p>"},{"location":"reference/#zarrnii.ZarrNii.dims","title":"<code>dims</code>  <code>property</code>","text":"<p>Dimension names.</p>"},{"location":"reference/#zarrnii.ZarrNii.name","title":"<code>name</code>  <code>property</code>","text":"<p>Image name from NgffImage.</p>"},{"location":"reference/#zarrnii.ZarrNii.ngff_image","title":"<code>ngff_image</code>  <code>instance-attribute</code>","text":""},{"location":"reference/#zarrnii.ZarrNii.omero","title":"<code>omero</code>  <code>property</code>","text":"<p>Omero metadata object.</p>"},{"location":"reference/#zarrnii.ZarrNii.orientation","title":"<code>orientation = 'RAS'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/#zarrnii.ZarrNii.scale","title":"<code>scale</code>  <code>property</code>","text":"<p>Scale information from NgffImage.</p>"},{"location":"reference/#zarrnii.ZarrNii.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>Shape of the image data.</p>"},{"location":"reference/#zarrnii.ZarrNii.translation","title":"<code>translation</code>  <code>property</code>","text":"<p>Translation information from NgffImage.</p>"},{"location":"reference/#zarrnii.ZarrNii.__get_upsampled_chunks","title":"<code>__get_upsampled_chunks(target_shape, return_scaling=True)</code>","text":"<p>Calculates new chunk sizes for a dask array to match a target shape, while ensuring the chunks sum precisely to the target shape. Optionally, returns the scaling factors for each dimension.</p> <p>This method is useful for upsampling data or ensuring 1:1 correspondence between downsampled and upsampled arrays.</p> <p>Parameters:</p> Name Type Description Default <code>target_shape</code> <code>tuple</code> <p>The desired shape of the array after upsampling.</p> required <code>return_scaling</code> <code>bool</code> <p>Whether to return the scaling factors                              for each dimension (default: True).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>new_chunks (tuple): A tuple of tuples specifying the new chunk sizes                     for each dimension. scaling (list): A list of scaling factors for each dimension                 (only if <code>return_scaling=True</code>).</p> <p>OR</p> <code>tuple</code> <p>new_chunks (tuple): A tuple of tuples specifying the new chunk sizes                     for each dimension (if <code>return_scaling=False</code>).</p> Notes <ul> <li>The scaling factor for each dimension is calculated as:   <code>scaling_factor = target_shape[dim] / original_shape[dim]</code></li> <li>The last chunk in each dimension is adjusted to account for rounding   errors, ensuring the sum of chunks matches the target shape.</li> </ul> Example Source code in <code>zarrnii/core.py</code> <pre><code>def __get_upsampled_chunks(self, target_shape, return_scaling=True):\n    \"\"\"\n    Calculates new chunk sizes for a dask array to match a target shape,\n    while ensuring the chunks sum precisely to the target shape. Optionally,\n    returns the scaling factors for each dimension.\n\n    This method is useful for upsampling data or ensuring 1:1 correspondence\n    between downsampled and upsampled arrays.\n\n    Parameters:\n        target_shape (tuple): The desired shape of the array after upsampling.\n        return_scaling (bool, optional): Whether to return the scaling factors\n                                         for each dimension (default: True).\n\n    Returns:\n        tuple:\n            new_chunks (tuple): A tuple of tuples specifying the new chunk sizes\n                                for each dimension.\n            scaling (list): A list of scaling factors for each dimension\n                            (only if `return_scaling=True`).\n\n        OR\n\n        tuple:\n            new_chunks (tuple): A tuple of tuples specifying the new chunk sizes\n                                for each dimension (if `return_scaling=False`).\n\n    Notes:\n        - The scaling factor for each dimension is calculated as:\n          `scaling_factor = target_shape[dim] / original_shape[dim]`\n        - The last chunk in each dimension is adjusted to account for rounding\n          errors, ensuring the sum of chunks matches the target shape.\n\n    Example:\n        # Calculate upsampled chunks and scaling factors\n        new_chunks, scaling = znimg.__get_upsampled_chunks((256, 256, 256))\n        print(\"New chunks:\", new_chunks)\n        print(\"Scaling factors:\", scaling)\n\n        # Calculate only the new chunks\n        new_chunks = znimg.__get_upsampled_chunks((256, 256, 256), return_scaling=False)\n    \"\"\"\n    new_chunks = []\n    scaling = []\n\n    for dim, (orig_shape, orig_chunks, new_shape) in enumerate(\n        zip(self.data.shape, self.data.chunks, target_shape)\n    ):\n        # Calculate the scaling factor for this dimension\n        scaling_factor = new_shape / orig_shape\n\n        # Scale each chunk size and round to get an initial estimate\n        scaled_chunks = [\n            int(round(chunk * scaling_factor)) for chunk in orig_chunks\n        ]\n        total = sum(scaled_chunks)\n\n        # Adjust the chunks to ensure they sum up to the target shape exactly\n        diff = new_shape - total\n        if diff != 0:\n            # Correct rounding errors by adjusting the last chunk size in the dimension\n            scaled_chunks[-1] += diff\n\n        new_chunks.append(tuple(scaled_chunks))\n        scaling.append(scaling_factor)\n\n    if return_scaling:\n        return tuple(new_chunks), scaling\n    else:\n        return tuple(new_chunks)\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.__get_upsampled_chunks--calculate-upsampled-chunks-and-scaling-factors","title":"Calculate upsampled chunks and scaling factors","text":"<p>new_chunks, scaling = znimg.__get_upsampled_chunks((256, 256, 256)) print(\"New chunks:\", new_chunks) print(\"Scaling factors:\", scaling)</p>"},{"location":"reference/#zarrnii.ZarrNii.__get_upsampled_chunks--calculate-only-the-new-chunks","title":"Calculate only the new chunks","text":"<p>new_chunks = znimg.__get_upsampled_chunks((256, 256, 256), return_scaling=False)</p>"},{"location":"reference/#zarrnii.ZarrNii.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation.</p> Source code in <code>zarrnii/core.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation.\"\"\"\n    return (\n        f\"ZarrNii(name='{self.name}', \"\n        f\"shape={self.shape}, \"\n        f\"dims={self.dims}, \"\n        f\"scale={self.scale})\"\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii._create_zyx_ngff_image","title":"<code>_create_zyx_ngff_image()</code>","text":"<p>Create a new NgffImage with data reordered from XYZ to ZYX.</p> <p>This is used when saving to OME-Zarr format which expects ZYX ordering. The data array is transposed and scale/translation are reordered accordingly.</p> <p>Returns:</p> Name Type Description <code>NgffImage</code> <code>NgffImage</code> <p>New image with ZYX-ordered data and metadata</p> Source code in <code>zarrnii/core.py</code> <pre><code>def _create_zyx_ngff_image(self) -&gt; nz.NgffImage:\n    \"\"\"\n    Create a new NgffImage with data reordered from XYZ to ZYX.\n\n    This is used when saving to OME-Zarr format which expects ZYX ordering.\n    The data array is transposed and scale/translation are reordered accordingly.\n\n    Returns:\n        NgffImage: New image with ZYX-ordered data and metadata\n    \"\"\"\n    if self.axes_order != \"XYZ\":\n        raise ValueError(\"This method should only be called when axes_order is XYZ\")\n\n    # Transpose data from XYZ to ZYX (reverse the spatial dimensions)\n    # Assuming data shape is [C, X, Y, Z] -&gt; [C, Z, Y, X]\n    data = self.ngff_image.data\n\n    # Find spatial dimension indices\n    spatial_axes = []\n    channel_axes = []\n    for i, dim_name in enumerate(self.ngff_image.dims):\n        if dim_name.lower() in [\"x\", \"y\", \"z\"]:\n            spatial_axes.append(i)\n        else:\n            channel_axes.append(i)\n\n    # Create transpose indices: reverse the spatial axes order\n    transpose_indices = channel_axes + spatial_axes[::-1]\n    transposed_data = data.transpose(transpose_indices)\n\n    # Create new dims list with ZYX ordering\n    new_dims = []\n    for i, dim_name in enumerate(self.ngff_image.dims):\n        if dim_name.lower() not in [\"x\", \"y\", \"z\"]:\n            new_dims.append(dim_name)\n    # Add spatial dims in ZYX order\n    spatial_dim_names = [self.ngff_image.dims[i] for i in spatial_axes]\n    new_dims.extend(spatial_dim_names[::-1])\n\n    # Reorder scale and translation from XYZ to ZYX\n    current_scale = self.ngff_image.scale\n    current_translation = self.ngff_image.translation\n\n    new_scale = {}\n    new_translation = {}\n\n    # Copy non-spatial dimensions\n    for key, value in current_scale.items():\n        if key.lower() not in [\"x\", \"y\", \"z\"]:\n            new_scale[key] = value\n\n    for key, value in current_translation.items():\n        if key.lower() not in [\"x\", \"y\", \"z\"]:\n            new_translation[key] = value\n\n    # Reorder spatial dimensions from XYZ to ZYX\n    if \"x\" in current_scale and \"y\" in current_scale and \"z\" in current_scale:\n        new_scale[\"z\"] = current_scale[\"z\"]\n        new_scale[\"y\"] = current_scale[\"y\"]\n        new_scale[\"x\"] = current_scale[\"x\"]\n\n    if (\n        \"x\" in current_translation\n        and \"y\" in current_translation\n        and \"z\" in current_translation\n    ):\n        new_translation[\"z\"] = current_translation[\"z\"]\n        new_translation[\"y\"] = current_translation[\"y\"]\n        new_translation[\"x\"] = current_translation[\"x\"]\n\n    # Create new NgffImage with ZYX ordering\n    zyx_image = nz.NgffImage(\n        data=transposed_data,\n        dims=new_dims,\n        scale=new_scale,\n        translation=new_translation,\n        name=self.ngff_image.name,\n    )\n\n    return zyx_image\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.apply_transform","title":"<code>apply_transform(*transforms, ref_znimg, spatial_dims=None)</code>","text":"<p>Apply spatial transformation and return a new ZarrNii instance.</p> <p>Parameters:</p> Name Type Description Default <code>transforms</code> <code>Transform</code> <p>Transformations to apply</p> <code>()</code> <code>ref_znimg</code> <code>'ZarrNii'</code> <p>Reference ZarrNii defining output space</p> required <code>spatial_dims</code> <code>List[str]</code> <p>Names of spatial dimensions (derived from axes_order if None)</p> <code>None</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>New ZarrNii with transformed data</p> Source code in <code>zarrnii/core.py</code> <pre><code>def apply_transform(\n    self,\n    *transforms: Transform,\n    ref_znimg: \"ZarrNii\",\n    spatial_dims: List[str] = None,\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Apply spatial transformation and return a new ZarrNii instance.\n\n    Args:\n        transforms: Transformations to apply\n        ref_znimg: Reference ZarrNii defining output space\n        spatial_dims: Names of spatial dimensions (derived from axes_order if None)\n\n    Returns:\n        New ZarrNii with transformed data\n    \"\"\"\n    if spatial_dims is None:\n        spatial_dims = (\n            [\"z\", \"y\", \"x\"] if self.axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n        )\n\n    # For now, just apply the first transform (placeholder)\n    if transforms:\n        transformed_image = apply_transform_to_ngff_image(\n            self.ngff_image, transforms[0], ref_znimg.ngff_image, spatial_dims\n        )\n    else:\n        transformed_image = self.ngff_image\n\n    return ZarrNii(\n        ngff_image=transformed_image,\n        axes_order=self.axes_order,\n        orientation=self.orientation,\n        _omero=self._omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.apply_transform_flo_to_ref_indices","title":"<code>apply_transform_flo_to_ref_indices(*transforms, ref_znimg, indices)</code>","text":"<p>Transform indices from floating to reference space.</p> Source code in <code>zarrnii/core.py</code> <pre><code>def apply_transform_flo_to_ref_indices(self, *transforms, ref_znimg, indices):\n    \"\"\"Transform indices from floating to reference space.\"\"\"\n    # Placeholder implementation - would need full transform logic\n    return indices\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.apply_transform_ref_to_flo_indices","title":"<code>apply_transform_ref_to_flo_indices(*transforms, ref_znimg, indices)</code>","text":"<p>Transform indices from reference to floating space.</p> Source code in <code>zarrnii/core.py</code> <pre><code>def apply_transform_ref_to_flo_indices(self, *transforms, ref_znimg, indices):\n    \"\"\"Transform indices from reference to floating space.\"\"\"\n    # Placeholder implementation - would need full transform logic\n    return indices\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.compute","title":"<code>compute()</code>","text":"<p>Compute the dask array and return the underlying NgffImage.</p> <p>This triggers computation of any lazy operations and returns the NgffImage with computed data.</p> <p>Returns:</p> Type Description <code>NgffImage</code> <p>NgffImage with computed data</p> Source code in <code>zarrnii/core.py</code> <pre><code>def compute(self) -&gt; nz.NgffImage:\n    \"\"\"\n    Compute the dask array and return the underlying NgffImage.\n\n    This triggers computation of any lazy operations and returns\n    the NgffImage with computed data.\n\n    Returns:\n        NgffImage with computed data\n    \"\"\"\n    computed_data = self.ngff_image.data.compute()\n\n    # Create new NgffImage with computed data\n    computed_image = nz.NgffImage(\n        data=computed_data,\n        dims=self.ngff_image.dims,\n        scale=self.ngff_image.scale,\n        translation=self.ngff_image.translation,\n        name=self.ngff_image.name,\n    )\n    return computed_image\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.copy","title":"<code>copy()</code>","text":"<p>Create a copy of this ZarrNii.</p> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>New ZarrNii with copied data</p> Source code in <code>zarrnii/core.py</code> <pre><code>def copy(self) -&gt; \"ZarrNii\":\n    \"\"\"\n    Create a copy of this ZarrNii.\n\n    Returns:\n        New ZarrNii with copied data\n    \"\"\"\n    # Create a new NgffImage with the same properties\n    copied_image = nz.NgffImage(\n        data=self.ngff_image.data,  # Dask arrays are lazy so this is efficient\n        dims=self.ngff_image.dims.copy(),\n        scale=self.ngff_image.scale.copy(),\n        translation=self.ngff_image.translation.copy(),\n        name=self.ngff_image.name,\n    )\n    return ZarrNii(\n        ngff_image=copied_image,\n        axes_order=self.axes_order,\n        orientation=self.orientation,\n        _omero=self._omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.crop","title":"<code>crop(bbox_min, bbox_max, spatial_dims=None)</code>","text":"<p>Crop the image and return a new ZarrNii instance.</p> <p>Parameters:</p> Name Type Description Default <code>bbox_min</code> <code>tuple</code> <p>Minimum corner of bounding box</p> required <code>bbox_max</code> <code>tuple</code> <p>Maximum corner of bounding box</p> required <code>spatial_dims</code> <code>List[str]</code> <p>Names of spatial dimensions (derived from axes_order if None)</p> <code>None</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>New ZarrNii with cropped data</p> Source code in <code>zarrnii/core.py</code> <pre><code>def crop(\n    self, bbox_min: tuple, bbox_max: tuple, spatial_dims: List[str] = None\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Crop the image and return a new ZarrNii instance.\n\n    Args:\n        bbox_min: Minimum corner of bounding box\n        bbox_max: Maximum corner of bounding box\n        spatial_dims: Names of spatial dimensions (derived from axes_order if None)\n\n    Returns:\n        New ZarrNii with cropped data\n    \"\"\"\n    if spatial_dims is None:\n        spatial_dims = (\n            [\"z\", \"y\", \"x\"] if self.axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n        )\n    cropped_image = crop_ngff_image(\n        self.ngff_image, bbox_min, bbox_max, spatial_dims\n    )\n    return ZarrNii(\n        ngff_image=cropped_image,\n        axes_order=self.axes_order,\n        orientation=self.orientation,\n        _omero=self._omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.crop_with_bounding_box","title":"<code>crop_with_bounding_box(bbox_min, bbox_max, ras_coords=False)</code>","text":"<p>Legacy method name for crop.</p> Source code in <code>zarrnii/core.py</code> <pre><code>def crop_with_bounding_box(self, bbox_min, bbox_max, ras_coords=False):\n    \"\"\"Legacy method name for crop.\"\"\"\n    return self.crop(bbox_min, bbox_max)\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.downsample","title":"<code>downsample(factors=None, along_x=1, along_y=1, along_z=1, level=None, spatial_dims=None)</code>","text":"<p>Downsample the image and return a new ZarrNii instance.</p> <p>Parameters:</p> Name Type Description Default <code>factors</code> <code>Union[int, List[int]]</code> <p>Downsampling factors (int for isotropic, list for per-dimension)</p> <code>None</code> <code>along_x</code> <code>int</code> <p>Legacy parameter for X downsampling</p> <code>1</code> <code>along_y</code> <code>int</code> <p>Legacy parameter for Y downsampling</p> <code>1</code> <code>along_z</code> <code>int</code> <p>Legacy parameter for Z downsampling</p> <code>1</code> <code>level</code> <code>int</code> <p>Legacy parameter for level-based downsampling (2^level)</p> <code>None</code> <code>spatial_dims</code> <code>List[str]</code> <p>Names of spatial dimensions (derived from axes_order if None)</p> <code>None</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>New ZarrNii with downsampled data</p> Source code in <code>zarrnii/core.py</code> <pre><code>def downsample(\n    self,\n    factors: Union[int, List[int]] = None,\n    along_x: int = 1,\n    along_y: int = 1,\n    along_z: int = 1,\n    level: int = None,\n    spatial_dims: List[str] = None,\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Downsample the image and return a new ZarrNii instance.\n\n    Args:\n        factors: Downsampling factors (int for isotropic, list for per-dimension)\n        along_x: Legacy parameter for X downsampling\n        along_y: Legacy parameter for Y downsampling\n        along_z: Legacy parameter for Z downsampling\n        level: Legacy parameter for level-based downsampling (2^level)\n        spatial_dims: Names of spatial dimensions (derived from axes_order if None)\n\n    Returns:\n        New ZarrNii with downsampled data\n    \"\"\"\n    # Handle legacy parameters\n    if factors is None:\n        if level is not None:\n            factors = 2**level\n        else:\n            factors = (\n                [along_z, along_y, along_x]\n                if self.axes_order == \"ZYX\"\n                else [along_x, along_y, along_z]\n            )\n\n    if spatial_dims is None:\n        spatial_dims = (\n            [\"z\", \"y\", \"x\"] if self.axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n        )\n\n    downsampled_image = downsample_ngff_image(\n        self.ngff_image, factors, spatial_dims\n    )\n    return ZarrNii(\n        ngff_image=downsampled_image,\n        axes_order=self.axes_order,\n        orientation=self.orientation,\n        _omero=self._omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.from_darr","title":"<code>from_darr(darr, affine=None, axes_order='ZYX', orientation='RAS', spacing=(1.0, 1.0, 1.0), origin=(0.0, 0.0, 0.0), name='image', omero=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create ZarrNii from dask array (legacy compatibility constructor).</p> <p>Parameters:</p> Name Type Description Default <code>darr</code> <code>Array</code> <p>Dask array containing image data</p> required <code>affine</code> <code>Optional[AffineTransform]</code> <p>Optional affine transformation</p> <code>None</code> <code>axes_order</code> <code>str</code> <p>Spatial axes order</p> <code>'ZYX'</code> <code>orientation</code> <code>str</code> <p>Anatomical orientation string</p> <code>'RAS'</code> <code>spacing</code> <code>Tuple[float, float, float]</code> <p>Voxel spacing (used if no affine provided)</p> <code>(1.0, 1.0, 1.0)</code> <code>origin</code> <code>Tuple[float, float, float]</code> <p>Origin offset (used if no affine provided)</p> <code>(0.0, 0.0, 0.0)</code> <code>name</code> <code>str</code> <p>Image name</p> <code>'image'</code> <code>omero</code> <code>Optional[object]</code> <p>Optional omero metadata</p> <code>None</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>ZarrNii instance</p> Source code in <code>zarrnii/core.py</code> <pre><code>@classmethod\ndef from_darr(\n    cls,\n    darr: da.Array,\n    affine: Optional[AffineTransform] = None,\n    axes_order: str = \"ZYX\",\n    orientation: str = \"RAS\",\n    spacing: Tuple[float, float, float] = (1.0, 1.0, 1.0),\n    origin: Tuple[float, float, float] = (0.0, 0.0, 0.0),\n    name: str = \"image\",\n    omero: Optional[object] = None,\n    **kwargs,\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Create ZarrNii from dask array (legacy compatibility constructor).\n\n    Args:\n        darr: Dask array containing image data\n        affine: Optional affine transformation\n        axes_order: Spatial axes order\n        orientation: Anatomical orientation string\n        spacing: Voxel spacing (used if no affine provided)\n        origin: Origin offset (used if no affine provided)\n        name: Image name\n        omero: Optional omero metadata\n\n    Returns:\n        ZarrNii instance\n    \"\"\"\n    # Create scale and translation from affine if provided\n    if affine is not None:\n        # Extract scale and translation from affine matrix\n        affine_matrix = affine.matrix\n        if axes_order == \"ZYX\":\n            scale = {\n                \"z\": affine_matrix[0, 0],\n                \"y\": affine_matrix[1, 1],\n                \"x\": affine_matrix[2, 2],\n            }\n            translation = {\n                \"z\": affine_matrix[0, 3],\n                \"y\": affine_matrix[1, 3],\n                \"x\": affine_matrix[2, 3],\n            }\n        else:  # XYZ\n            scale = {\n                \"x\": affine_matrix[0, 0],\n                \"y\": affine_matrix[1, 1],\n                \"z\": affine_matrix[2, 2],\n            }\n            translation = {\n                \"x\": affine_matrix[0, 3],\n                \"y\": affine_matrix[1, 3],\n                \"z\": affine_matrix[2, 3],\n            }\n    else:\n        # Use spacing and origin\n        if axes_order == \"ZYX\":\n            scale = {\"z\": spacing[0], \"y\": spacing[1], \"x\": spacing[2]}\n            translation = {\"z\": origin[0], \"y\": origin[1], \"x\": origin[2]}\n        else:  # XYZ\n            scale = {\"x\": spacing[0], \"y\": spacing[1], \"z\": spacing[2]}\n            translation = {\"x\": origin[0], \"y\": origin[1], \"z\": origin[2]}\n\n    # Create NgffImage\n    dims = [\"c\", \"z\", \"y\", \"x\"] if axes_order == \"ZYX\" else [\"c\", \"x\", \"y\", \"z\"]\n    ngff_image = nz.NgffImage(\n        data=darr, dims=dims, scale=scale, translation=translation, name=name\n    )\n\n    return cls(\n        ngff_image=ngff_image,\n        axes_order=axes_order,\n        orientation=orientation,\n        _omero=omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.from_ngff_image","title":"<code>from_ngff_image(ngff_image, axes_order='ZYX', orientation='RAS', omero=None)</code>  <code>classmethod</code>","text":"<p>Create ZarrNii from an existing NgffImage.</p> <p>Parameters:</p> Name Type Description Default <code>ngff_image</code> <code>NgffImage</code> <p>NgffImage to wrap</p> required <code>axes_order</code> <code>str</code> <p>Spatial axes order for NIfTI compatibility</p> <code>'ZYX'</code> <code>orientation</code> <code>str</code> <p>Anatomical orientation string</p> <code>'RAS'</code> <code>omero</code> <code>Optional[object]</code> <p>Optional omero metadata object</p> <code>None</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>ZarrNii instance</p> Source code in <code>zarrnii/core.py</code> <pre><code>@classmethod\ndef from_ngff_image(\n    cls,\n    ngff_image: nz.NgffImage,\n    axes_order: str = \"ZYX\",\n    orientation: str = \"RAS\",\n    omero: Optional[object] = None,\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Create ZarrNii from an existing NgffImage.\n\n    Args:\n        ngff_image: NgffImage to wrap\n        axes_order: Spatial axes order for NIfTI compatibility\n        orientation: Anatomical orientation string\n        omero: Optional omero metadata object\n\n    Returns:\n        ZarrNii instance\n    \"\"\"\n    return cls(\n        ngff_image=ngff_image,\n        axes_order=axes_order,\n        orientation=orientation,\n        _omero=omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.from_nifti","title":"<code>from_nifti(path, chunks='auto', axes_order='XYZ', name=None, as_ref=False, zooms=None)</code>  <code>classmethod</code>","text":"<p>Load from NIfTI file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>Path to NIfTI file</p> required <code>chunks</code> <p>Chunking strategy for dask array</p> <code>'auto'</code> <code>axes_order</code> <p>Spatial axes order</p> <code>'XYZ'</code> <code>name</code> <p>Name for the NgffImage</p> <code>None</code> <code>as_ref</code> <p>If True, creates an empty dask array with the correct shape instead of loading data</p> <code>False</code> <code>zooms</code> <p>Target voxel spacing in xyz (only valid if as_ref=True)</p> <code>None</code> <p>Returns:</p> Type Description <p>ZarrNii instance</p> Source code in <code>zarrnii/core.py</code> <pre><code>@classmethod\ndef from_nifti(\n    cls, path, chunks=\"auto\", axes_order=\"XYZ\", name=None, as_ref=False, zooms=None\n):\n    \"\"\"\n    Load from NIfTI file.\n\n    Args:\n        path: Path to NIfTI file\n        chunks: Chunking strategy for dask array\n        axes_order: Spatial axes order\n        name: Name for the NgffImage\n        as_ref: If True, creates an empty dask array with the correct shape instead of loading data\n        zooms: Target voxel spacing in xyz (only valid if as_ref=True)\n\n    Returns:\n        ZarrNii instance\n    \"\"\"\n    if not as_ref and zooms is not None:\n        raise ValueError(\"`zooms` can only be used when `as_ref=True`.\")\n\n    # Load NIfTI file\n    nifti_img = nib.load(path)\n    shape = nifti_img.header.get_data_shape()\n    affine_matrix = nifti_img.affine.copy()\n\n    # Adjust shape and affine if zooms are provided\n    if zooms is not None:\n        in_zooms = np.sqrt(\n            (affine_matrix[:3, :3] ** 2).sum(axis=0)\n        )  # Current voxel spacing\n        scaling_factor = in_zooms / zooms\n        new_shape = [\n            int(np.floor(shape[0] * scaling_factor[2])),  # Z\n            int(np.floor(shape[1] * scaling_factor[1])),  # Y\n            int(np.floor(shape[2] * scaling_factor[0])),  # X\n        ]\n        np.fill_diagonal(affine_matrix[:3, :3], zooms)\n    else:\n        new_shape = shape\n\n    if as_ref:\n        # Create an empty dask array with the adjusted shape\n        darr = da.empty((1, *new_shape), chunks=chunks, dtype=\"float32\")\n    else:\n        # Load the NIfTI data and convert to a dask array\n        array = nifti_img.get_fdata()\n        darr = da.from_array(array, chunks=chunks)\n\n    # Add channel and time dimensions if not present\n    original_ndim = len(darr.shape)\n\n    if original_ndim == 3:\n        # 3D data: add channel dimension -&gt; (c, z, y, x) or (c, x, y, z)\n        darr = darr[np.newaxis, ...]\n    elif original_ndim == 4:\n        # 4D data: could be (c, z, y, x) or (t, z, y, x) - assume channel by default\n        # User can specify if it's time by using appropriate axes_order\n        pass  # Keep as is - 4D is already handled\n    elif original_ndim == 5:\n        # 5D data: assume (t, z, y, x, c) and handle appropriately\n        pass  # Keep as is - 5D is already the target format\n    else:\n        # For 1D, 2D, or &gt;5D data, add channel dimension and let user handle\n        darr = darr[np.newaxis, ...]\n\n    # Create dimensions based on data shape after dimension adjustments\n    final_ndim = len(darr.shape)\n    if final_ndim == 4:\n        # 4D: (c, z, y, x) or (c, x, y, z) - standard case\n        dims = [\"c\"] + list(axes_order.lower())\n    elif final_ndim == 5:\n        # 5D: (t, c, z, y, x) or (t, c, x, y, z) - time dimension included\n        dims = [\"t\", \"c\"] + list(axes_order.lower())\n    else:\n        # Fallback for other cases\n        dims = [\"c\"] + list(axes_order.lower())\n\n    # Extract scale and translation from affine\n    scale = {}\n    translation = {}\n    spatial_dims = [\"z\", \"y\", \"x\"] if axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n\n    for i, dim in enumerate(spatial_dims):\n        scale[dim] = np.sqrt((affine_matrix[i, :3] ** 2).sum())\n        translation[dim] = affine_matrix[i, 3]\n\n    # Create NgffImage\n    if name is None:\n        name = f\"nifti_image_{path}\"\n\n    ngff_image = nz.NgffImage(\n        data=darr, dims=dims, scale=scale, translation=translation, name=name\n    )\n\n    return cls(ngff_image=ngff_image, axes_order=axes_order)\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.from_ome_zarr","title":"<code>from_ome_zarr(store_or_path, level=0, channels=None, channel_labels=None, timepoints=None, storage_options=None, axes_order='ZYX', orientation='RAS')</code>  <code>classmethod</code>","text":"<p>Load from OME-Zarr store.</p> <p>Parameters:</p> Name Type Description Default <code>store_or_path</code> <p>Store or path to OME-Zarr file</p> required <code>level</code> <code>int</code> <p>Pyramid level to load (if beyond available levels, lazy downsampling is applied)</p> <code>0</code> <code>channels</code> <code>Optional[List[int]]</code> <p>Channel indices to load</p> <code>None</code> <code>channel_labels</code> <code>Optional[List[str]]</code> <p>Channel labels to load</p> <code>None</code> <code>timepoints</code> <code>Optional[List[int]]</code> <p>Timepoint indices to load</p> <code>None</code> <code>storage_options</code> <code>Optional[Dict]</code> <p>Storage options for Zarr</p> <code>None</code> <code>axes_order</code> <code>str</code> <p>Spatial axes order for NIfTI compatibility</p> <code>'ZYX'</code> <code>orientation</code> <code>str</code> <p>Default input orientation if none is specified in metadata (default: 'RAS')</p> <code>'RAS'</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>ZarrNii instance</p> Source code in <code>zarrnii/core.py</code> <pre><code>@classmethod\ndef from_ome_zarr(\n    cls,\n    store_or_path,\n    level: int = 0,\n    channels: Optional[List[int]] = None,\n    channel_labels: Optional[List[str]] = None,\n    timepoints: Optional[List[int]] = None,\n    storage_options: Optional[Dict] = None,\n    axes_order: str = \"ZYX\",\n    orientation: str = \"RAS\",\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Load from OME-Zarr store.\n\n    Args:\n        store_or_path: Store or path to OME-Zarr file\n        level: Pyramid level to load (if beyond available levels, lazy downsampling is applied)\n        channels: Channel indices to load\n        channel_labels: Channel labels to load\n        timepoints: Timepoint indices to load\n        storage_options: Storage options for Zarr\n        axes_order: Spatial axes order for NIfTI compatibility\n        orientation: Default input orientation if none is specified in metadata (default: 'RAS')\n\n    Returns:\n        ZarrNii instance\n    \"\"\"\n    # Validate channel and timepoint selection arguments\n    if channels is not None and channel_labels is not None:\n        raise ValueError(\"Cannot specify both 'channels' and 'channel_labels'\")\n\n    # Load the multiscales object\n    try:\n        if isinstance(store_or_path, str):\n            multiscales = nz.from_ngff_zarr(\n                store_or_path, storage_options=storage_options or {}\n            )\n        else:\n            multiscales = nz.from_ngff_zarr(store_or_path)\n    except Exception as e:\n        # Fallback for older zarr/ngff_zarr versions\n        if isinstance(store_or_path, str):\n            store = fsspec.get_mapper(store_or_path, **storage_options or {})\n        else:\n            store = store_or_path\n        multiscales = nz.from_ngff_zarr(store)\n\n    # Extract omero metadata if available\n    omero_metadata = None\n    try:\n        import zarr\n\n        if isinstance(store_or_path, str):\n            group = zarr.open_group(store_or_path, mode=\"r\")\n        else:\n            group = zarr.open_group(store_or_path, mode=\"r\")\n\n        if \"omero\" in group.attrs:\n            omero_dict = group.attrs[\"omero\"]\n\n            # Create a simple object to hold omero metadata\n            class OmeroMetadata:\n                def __init__(self, omero_dict):\n                    self.channels = []\n                    if \"channels\" in omero_dict:\n                        for ch_dict in omero_dict[\"channels\"]:\n                            # Create channel objects\n                            class ChannelMetadata:\n                                def __init__(self, ch_dict):\n                                    self.label = ch_dict.get(\"label\", \"\")\n                                    self.color = ch_dict.get(\"color\", \"\")\n                                    if \"window\" in ch_dict:\n\n                                        class WindowMetadata:\n                                            def __init__(self, win_dict):\n                                                self.min = win_dict.get(\"min\", 0.0)\n                                                self.max = win_dict.get(\n                                                    \"max\", 65535.0\n                                                )\n                                                self.start = win_dict.get(\n                                                    \"start\", 0.0\n                                                )\n                                                self.end = win_dict.get(\n                                                    \"end\", 65535.0\n                                                )\n\n                                        self.window = WindowMetadata(\n                                            ch_dict[\"window\"]\n                                        )\n                                    else:\n                                        self.window = None\n\n                            self.channels.append(ChannelMetadata(ch_dict))\n\n            omero_metadata = OmeroMetadata(omero_dict)\n    except Exception:\n        # If we can't load omero metadata, that's okay\n        pass\n\n    # Read orientation metadata (default to the provided orientation if not present)\n    try:\n        import zarr\n\n        if isinstance(store_or_path, str):\n            group = zarr.open_group(store_or_path, mode=\"r\")\n        else:\n            group = zarr.open_group(store_or_path, mode=\"r\")\n\n        # Get orientation from zarr metadata, fallback to provided orientation\n        orientation = group.attrs.get(\"orientation\", orientation)\n    except Exception:\n        # If we can't read orientation metadata, use the provided default\n        pass\n\n    # Determine the available pyramid levels and handle lazy downsampling\n    max_level = len(multiscales.images) - 1\n    actual_level = min(level, max_level)\n    do_downsample = level &gt; max_level\n\n    # Get the highest available level\n    ngff_image = multiscales.images[actual_level]\n\n    # Handle channel and timepoint selection and filter omero metadata accordingly\n    filtered_omero = omero_metadata\n    if channels is not None or channel_labels is not None or timepoints is not None:\n        ngff_image, filtered_omero = _select_dimensions_from_image_with_omero(\n            ngff_image,\n            multiscales,\n            channels,\n            channel_labels,\n            timepoints,\n            omero_metadata,\n        )\n\n    # Create ZarrNii instance with orientation\n    znimg = cls(\n        ngff_image=ngff_image,\n        axes_order=axes_order,\n        orientation=orientation,\n        _omero=filtered_omero,\n    )\n\n    # Apply lazy downsampling if needed\n    if do_downsample:\n        level_ds = level - max_level\n        downsample_factor = 2**level_ds\n\n        # Get spatial dims based on axes order\n        spatial_dims = [\"z\", \"y\", \"x\"] if axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n\n        # Apply downsampling using the existing method\n        znimg = znimg.downsample(\n            factors=downsample_factor, spatial_dims=spatial_dims\n        )\n\n    return znimg\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.get_affine_matrix","title":"<code>get_affine_matrix(axes_order=None)</code>","text":"<p>Get 4x4 affine transformation matrix from NgffImage metadata.</p> <p>Parameters:</p> Name Type Description Default <code>axes_order</code> <code>str</code> <p>Spatial axes order, defaults to self.axes_order</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>4x4 affine transformation matrix</p> Source code in <code>zarrnii/core.py</code> <pre><code>def get_affine_matrix(self, axes_order: str = None) -&gt; np.ndarray:\n    \"\"\"\n    Get 4x4 affine transformation matrix from NgffImage metadata.\n\n    Args:\n        axes_order: Spatial axes order, defaults to self.axes_order\n\n    Returns:\n        4x4 affine transformation matrix\n    \"\"\"\n    if axes_order is None:\n        axes_order = self.axes_order\n\n    # Create identity 4x4 matrix\n    affine = np.eye(4)\n\n    # Map axes order to matrix indices\n    spatial_dims = [\"z\", \"y\", \"x\"] if axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n\n    # Set scale values\n    for i, dim in enumerate(spatial_dims):\n        if dim in self.ngff_image.scale:\n            affine[i, i] = self.ngff_image.scale[dim]\n\n    # Set translation values\n    for i, dim in enumerate(spatial_dims):\n        if dim in self.ngff_image.translation:\n            affine[i, 3] = self.ngff_image.translation[dim]\n\n    # Apply orientation alignment if orientation is available\n    if hasattr(self, \"orientation\") and self.orientation:\n        affine = align_affine_to_input_orientation(affine, self.orientation)\n\n    return affine\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.get_affine_transform","title":"<code>get_affine_transform(axes_order=None)</code>","text":"<p>Get AffineTransform object from NgffImage metadata.</p> <p>Parameters:</p> Name Type Description Default <code>axes_order</code> <code>str</code> <p>Spatial axes order, defaults to self.axes_order</p> <code>None</code> <p>Returns:</p> Type Description <code>AffineTransform</code> <p>AffineTransform object</p> Source code in <code>zarrnii/core.py</code> <pre><code>def get_affine_transform(self, axes_order: str = None) -&gt; AffineTransform:\n    \"\"\"\n    Get AffineTransform object from NgffImage metadata.\n\n    Args:\n        axes_order: Spatial axes order, defaults to self.axes_order\n\n    Returns:\n        AffineTransform object\n    \"\"\"\n    matrix = self.get_affine_matrix(axes_order)\n    return AffineTransform.from_array(matrix)\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.get_orientation","title":"<code>get_orientation()</code>","text":"<p>Get orientation string from affine matrix.</p> Source code in <code>zarrnii/core.py</code> <pre><code>def get_orientation(self):\n    \"\"\"Get orientation string from affine matrix.\"\"\"\n    return affine_to_orientation(self.get_affine_matrix())\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.get_origin","title":"<code>get_origin(axes_order=None)</code>","text":"<p>Get origin (translation) from NgffImage.</p> <p>Parameters:</p> Name Type Description Default <code>axes_order</code> <code>str</code> <p>Spatial axes order, defaults to self.axes_order</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of origin coordinates</p> Source code in <code>zarrnii/core.py</code> <pre><code>def get_origin(self, axes_order: str = None) -&gt; np.ndarray:\n    \"\"\"\n    Get origin (translation) from NgffImage.\n\n    Args:\n        axes_order: Spatial axes order, defaults to self.axes_order\n\n    Returns:\n        Array of origin coordinates\n    \"\"\"\n    if axes_order is None:\n        axes_order = self.axes_order\n\n    spatial_dims = [\"z\", \"y\", \"x\"] if axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n    origin = []\n\n    for dim in spatial_dims:\n        if dim in self.ngff_image.translation:\n            origin.append(self.ngff_image.translation[dim])\n        else:\n            origin.append(0.0)\n\n    return np.array(origin)\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.get_zooms","title":"<code>get_zooms(axes_order=None)</code>","text":"<p>Get voxel spacing (zooms) from NgffImage scale.</p> <p>Parameters:</p> Name Type Description Default <code>axes_order</code> <code>str</code> <p>Spatial axes order, defaults to self.axes_order</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of voxel spacings</p> Source code in <code>zarrnii/core.py</code> <pre><code>def get_zooms(self, axes_order: str = None) -&gt; np.ndarray:\n    \"\"\"\n    Get voxel spacing (zooms) from NgffImage scale.\n\n    Args:\n        axes_order: Spatial axes order, defaults to self.axes_order\n\n    Returns:\n        Array of voxel spacings\n    \"\"\"\n    if axes_order is None:\n        axes_order = self.axes_order\n\n    spatial_dims = [\"z\", \"y\", \"x\"] if axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n    zooms = []\n\n    for dim in spatial_dims:\n        if dim in self.ngff_image.scale:\n            zooms.append(self.ngff_image.scale[dim])\n        else:\n            zooms.append(1.0)\n\n    return np.array(zooms)\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.list_channels","title":"<code>list_channels()</code>","text":"<p>List available channel labels from omero metadata.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of channel labels, or empty list if no omero metadata</p> Source code in <code>zarrnii/core.py</code> <pre><code>def list_channels(self) -&gt; List[str]:\n    \"\"\"\n    List available channel labels from omero metadata.\n\n    Returns:\n        List of channel labels, or empty list if no omero metadata\n    \"\"\"\n    if self.omero is None or not hasattr(self.omero, \"channels\"):\n        return []\n\n    return [\n        ch.label if hasattr(ch, \"label\") else ch.get(\"label\", \"\")\n        for ch in self.omero.channels\n    ]\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.select_channels","title":"<code>select_channels(channels=None, channel_labels=None)</code>","text":"<p>Select channels from the image data and return a new ZarrNii instance.</p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>Optional[List[int]]</code> <p>Channel indices to select</p> <code>None</code> <code>channel_labels</code> <code>Optional[List[str]]</code> <p>Channel labels to select</p> <code>None</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>New ZarrNii instance with selected channels</p> Source code in <code>zarrnii/core.py</code> <pre><code>def select_channels(\n    self,\n    channels: Optional[List[int]] = None,\n    channel_labels: Optional[List[str]] = None,\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Select channels from the image data and return a new ZarrNii instance.\n\n    Args:\n        channels: Channel indices to select\n        channel_labels: Channel labels to select\n\n    Returns:\n        New ZarrNii instance with selected channels\n    \"\"\"\n    if channels is not None and channel_labels is not None:\n        raise ValueError(\"Cannot specify both 'channels' and 'channel_labels'\")\n\n    if channel_labels is not None:\n        if self.omero is None:\n            raise ValueError(\n                \"Channel labels were specified but no omero metadata found\"\n            )\n\n        available_labels = self.list_channels()\n        channel_indices = []\n        for label in channel_labels:\n            if label not in available_labels:\n                raise ValueError(f\"Channel label '{label}' not found\")\n            channel_indices.append(available_labels.index(label))\n        channels = channel_indices\n\n    if channels is None:\n        # Return a copy with all channels\n        return self.copy()\n\n    # Select channels from data (assumes channel is last dimension)\n    selected_data = self.data[..., channels]\n\n    # Create new NgffImage with selected data\n    new_ngff_image = nz.NgffImage(\n        data=selected_data,\n        dims=self.dims,\n        scale=self.scale,\n        translation=self.translation,\n        name=self.name,\n    )\n\n    # Filter omero metadata to match selected channels\n    filtered_omero = None\n    if self.omero is not None and hasattr(self.omero, \"channels\"):\n\n        class FilteredOmero:\n            def __init__(self, channels):\n                self.channels = channels\n\n        filtered_channels = [self.omero.channels[i] for i in channels]\n        filtered_omero = FilteredOmero(filtered_channels)\n\n    return ZarrNii(\n        ngff_image=new_ngff_image,\n        axes_order=self.axes_order,\n        orientation=self.orientation,\n        _omero=filtered_omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.select_timepoints","title":"<code>select_timepoints(timepoints=None)</code>","text":"<p>Select timepoints from the image data and return a new ZarrNii instance.</p> <p>Parameters:</p> Name Type Description Default <code>timepoints</code> <code>Optional[List[int]]</code> <p>Timepoint indices to select</p> <code>None</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>New ZarrNii instance with selected timepoints</p> Source code in <code>zarrnii/core.py</code> <pre><code>def select_timepoints(self, timepoints: Optional[List[int]] = None) -&gt; \"ZarrNii\":\n    \"\"\"\n    Select timepoints from the image data and return a new ZarrNii instance.\n\n    Args:\n        timepoints: Timepoint indices to select\n\n    Returns:\n        New ZarrNii instance with selected timepoints\n    \"\"\"\n    if timepoints is None:\n        # Return a copy with all timepoints\n        return self.copy()\n\n    # Check if time dimension exists\n    if \"t\" not in self.dims:\n        raise ValueError(\"No time dimension found in the data\")\n\n    # Get time dimension index\n    t_idx = self.dims.index(\"t\")\n\n    # Create slice objects\n    slices = [slice(None)] * len(self.data.shape)\n    slices[t_idx] = timepoints\n\n    # Select timepoints from data\n    selected_data = self.data[tuple(slices)]\n\n    # Create new NgffImage with selected data\n    new_ngff_image = nz.NgffImage(\n        data=selected_data,\n        dims=self.dims,\n        scale=self.scale,\n        translation=self.translation,\n        name=self.name,\n    )\n\n    return ZarrNii(\n        ngff_image=new_ngff_image,\n        axes_order=self.axes_order,\n        orientation=self.orientation,\n        _omero=self._omero,  # Timepoint selection doesn't affect omero metadata\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.to_ngff_image","title":"<code>to_ngff_image(name=None)</code>","text":"<p>Convert to NgffImage object.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Optional name for the image</p> <code>None</code> <p>Returns:</p> Type Description <code>NgffImage</code> <p>NgffImage representation</p> Source code in <code>zarrnii/core.py</code> <pre><code>def to_ngff_image(self, name: str = None) -&gt; nz.NgffImage:\n    \"\"\"\n    Convert to NgffImage object.\n\n    Args:\n        name: Optional name for the image\n\n    Returns:\n        NgffImage representation\n    \"\"\"\n    if name is None:\n        name = self.name\n\n    return nz.NgffImage(\n        data=self.data,\n        dims=self.dims,\n        scale=self.scale,\n        translation=self.translation,\n        name=name,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.to_nifti","title":"<code>to_nifti(filename=None)</code>","text":"<p>Convert to NIfTI format.</p> <p>NIfTI files are always written in XYZ order. If the current axes_order is ZYX, the data will be reordered to XYZ and the affine matrix adjusted accordingly.</p> <p>For 5D data (T,C,Z,Y,X), singleton dimensions are removed automatically. Non-singleton time and channel dimensions will raise an error as NIfTI doesn't  support more than 4D data.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <p>Output filename, if None return nibabel image</p> <code>None</code> <p>Returns:</p> Type Description <p>nibabel.Nifti1Image or path if filename provided</p> Source code in <code>zarrnii/core.py</code> <pre><code>def to_nifti(self, filename=None):\n    \"\"\"\n    Convert to NIfTI format.\n\n    NIfTI files are always written in XYZ order. If the current axes_order is ZYX,\n    the data will be reordered to XYZ and the affine matrix adjusted accordingly.\n\n    For 5D data (T,C,Z,Y,X), singleton dimensions are removed automatically.\n    Non-singleton time and channel dimensions will raise an error as NIfTI doesn't \n    support more than 4D data.\n\n    Args:\n        filename: Output filename, if None return nibabel image\n\n    Returns:\n        nibabel.Nifti1Image or path if filename provided\n    \"\"\"\n    # Get data and dimensions\n    data = self.data.compute()\n    dims = self.dims\n\n    # Handle dimensional reduction for NIfTI compatibility\n    # NIfTI supports up to 4D, so we need to remove singleton dimensions\n    squeeze_axes = []\n    remaining_dims = []\n\n    for i, dim in enumerate(dims):\n        if dim in ['t', 'c'] and data.shape[i] == 1:\n            # Remove singleton time or channel dimensions\n            squeeze_axes.append(i)\n        elif dim in ['t', 'c'] and data.shape[i] &gt; 1:\n            # Non-singleton time or channel dimensions - NIfTI can't handle this\n            raise ValueError(f\"NIfTI format doesn't support non-singleton {dim} dimension. \"\n                           f\"Dimension '{dim}' has size {data.shape[i]}. \"\n                           f\"Consider selecting specific timepoints/channels first.\")\n        else:\n            remaining_dims.append(dim)\n\n    # Squeeze out singleton dimensions\n    if squeeze_axes:\n        data = np.squeeze(data, axis=tuple(squeeze_axes))\n\n    # Check final dimensionality\n    if data.ndim &gt; 4:\n        raise ValueError(f\"Resulting data has {data.ndim} dimensions, but NIfTI supports maximum 4D\")\n\n    # Now handle spatial reordering based on axes_order\n    if self.axes_order == \"ZYX\":\n        # Data spatial dimensions are in ZYX order, need to transpose to XYZ\n        if data.ndim == 3:\n            # Pure spatial data: ZYX -&gt; XYZ\n            data = data.transpose(2, 1, 0)\n        elif data.ndim == 4:\n            # 4D data with one non-spatial dimension remaining\n            # Could be (T,Z,Y,X) or (C,Z,Y,X) - spatial part needs ZYX-&gt;XYZ\n            # The non-spatial dimension stays first\n            data = data.transpose(0, 3, 2, 1)\n\n        # Get affine matrix in XYZ order\n        affine_matrix = self.get_affine_matrix(axes_order=\"XYZ\")\n    else:\n        # Data is already in XYZ order  \n        affine_matrix = self.get_affine_matrix(axes_order=\"XYZ\")\n\n    # Create NIfTI image\n    nifti_img = nib.Nifti1Image(data, affine_matrix)\n\n    if filename is not None:\n        nib.save(nifti_img, filename)\n        return filename\n    else:\n        return nifti_img\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.to_ome_zarr","title":"<code>to_ome_zarr(store_or_path, max_layer=4, scale_factors=None, **kwargs)</code>","text":"<p>Save to OME-Zarr store and return self for continued chaining.</p> <p>OME-Zarr files are always written in ZYX order. If the current axes_order is XYZ, the data will be reordered to ZYX before writing.</p> <p>Parameters:</p> Name Type Description Default <code>store_or_path</code> <p>Target store or path</p> required <code>max_layer</code> <code>int</code> <p>Maximum number of pyramid levels</p> <code>4</code> <code>scale_factors</code> <code>Optional[List[int]]</code> <p>Custom scale factors for pyramid levels</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments for to_ngff_zarr</p> <code>{}</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>Self for continued chaining</p> Source code in <code>zarrnii/core.py</code> <pre><code>def to_ome_zarr(\n    self,\n    store_or_path,\n    max_layer: int = 4,\n    scale_factors: Optional[List[int]] = None,\n    **kwargs,\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Save to OME-Zarr store and return self for continued chaining.\n\n    OME-Zarr files are always written in ZYX order. If the current axes_order is XYZ,\n    the data will be reordered to ZYX before writing.\n\n    Args:\n        store_or_path: Target store or path\n        max_layer: Maximum number of pyramid levels\n        scale_factors: Custom scale factors for pyramid levels\n        **kwargs: Additional arguments for to_ngff_zarr\n\n    Returns:\n        Self for continued chaining\n    \"\"\"\n    # Determine the image to save\n    if self.axes_order == \"XYZ\":\n        # Need to reorder data from XYZ to ZYX for OME-Zarr\n        ngff_image_to_save = self._create_zyx_ngff_image()\n    else:\n        # Already in ZYX order\n        ngff_image_to_save = self.ngff_image\n\n    save_ngff_image(\n        ngff_image_to_save, store_or_path, max_layer, scale_factors, **kwargs\n    )\n\n    # Add orientation metadata to the zarr store\n    try:\n        import zarr\n\n        if isinstance(store_or_path, str):\n            group = zarr.open_group(store_or_path, mode=\"r+\")\n        else:\n            group = zarr.open_group(store_or_path, mode=\"r+\")\n\n        # Add metadata for orientation\n        if hasattr(self, \"orientation\") and self.orientation:\n            group.attrs[\"orientation\"] = self.orientation\n    except Exception:\n        # If we can't write orientation metadata, that's not critical\n        pass\n\n    return self\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.upsample","title":"<code>upsample(along_x=1, along_y=1, along_z=1, to_shape=None)</code>","text":"<p>Upsamples the ZarrNii instance using <code>scipy.ndimage.zoom</code>.</p> <p>Parameters:</p> Name Type Description Default <code>along_x</code> <code>int</code> <p>Upsampling factor along the X-axis (default: 1).</p> <code>1</code> <code>along_y</code> <code>int</code> <p>Upsampling factor along the Y-axis (default: 1).</p> <code>1</code> <code>along_z</code> <code>int</code> <p>Upsampling factor along the Z-axis (default: 1).</p> <code>1</code> <code>to_shape</code> <code>tuple</code> <p>Target shape for upsampling. Should include all dimensions                          (e.g., <code>(c, z, y, x)</code> for ZYX or <code>(c, x, y, z)</code> for XYZ).                          If provided, <code>along_x</code>, <code>along_y</code>, and <code>along_z</code> are ignored.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ZarrNii</code> <p>A new ZarrNii instance with the upsampled data and updated affine.</p> Notes <ul> <li>This method supports both direct scaling via <code>along_*</code> factors or target shape via <code>to_shape</code>.</li> <li>If <code>to_shape</code> is provided, chunk sizes and scaling factors are dynamically calculated.</li> <li>The affine matrix is updated to reflect the new voxel size after upsampling.</li> </ul> Example Source code in <code>zarrnii/core.py</code> <pre><code>def upsample(self, along_x=1, along_y=1, along_z=1, to_shape=None):\n    \"\"\"\n    Upsamples the ZarrNii instance using `scipy.ndimage.zoom`.\n\n    Parameters:\n        along_x (int, optional): Upsampling factor along the X-axis (default: 1).\n        along_y (int, optional): Upsampling factor along the Y-axis (default: 1).\n        along_z (int, optional): Upsampling factor along the Z-axis (default: 1).\n        to_shape (tuple, optional): Target shape for upsampling. Should include all dimensions\n                                     (e.g., `(c, z, y, x)` for ZYX or `(c, x, y, z)` for XYZ).\n                                     If provided, `along_x`, `along_y`, and `along_z` are ignored.\n\n    Returns:\n        ZarrNii: A new ZarrNii instance with the upsampled data and updated affine.\n\n    Notes:\n        - This method supports both direct scaling via `along_*` factors or target shape via `to_shape`.\n        - If `to_shape` is provided, chunk sizes and scaling factors are dynamically calculated.\n        - The affine matrix is updated to reflect the new voxel size after upsampling.\n\n    Example:\n        # Upsample with scaling factors\n        upsampled_znimg = znimg.upsample(along_x=2, along_y=2, along_z=2)\n\n        # Upsample to a specific shape\n        upsampled_znimg = znimg.upsample(to_shape=(1, 256, 256, 256))\n    \"\"\"\n    # Determine scaling and chunks based on input parameters\n    if to_shape is None:\n        if self.axes_order == \"XYZ\":\n            scaling = (1, along_x, along_y, along_z)\n        else:\n            scaling = (1, along_z, along_y, along_x)\n\n        chunks_out = tuple(\n            tuple(c * scale for c in chunks_i)\n            for chunks_i, scale in zip(self.data.chunks, scaling)\n        )\n    else:\n        chunks_out, scaling = self.__get_upsampled_chunks(to_shape)\n\n    # Define block-wise upsampling function\n    def zoom_blocks(x, block_info=None):\n        \"\"\"\n        Scales blocks to the desired size using `scipy.ndimage.zoom`.\n\n        Parameters:\n            x (np.ndarray): Input block data.\n            block_info (dict, optional): Metadata about the current block.\n\n        Returns:\n            np.ndarray: The upscaled block.\n        \"\"\"\n        # Calculate scaling factors based on input and output chunk shapes\n        scaling = tuple(\n            out_n / in_n\n            for out_n, in_n in zip(block_info[None][\"chunk-shape\"], x.shape)\n        )\n        return zoom(x, scaling, order=1, prefilter=False)\n\n    # Perform block-wise upsampling\n    darr_scaled = da.map_blocks(\n        zoom_blocks, self.data, dtype=self.data.dtype, chunks=chunks_out\n    )\n\n    # Update the affine matrix to reflect the new voxel size\n    if self.axes_order == \"XYZ\":\n        scaling_matrix = np.diag(\n            (1 / scaling[1], 1 / scaling[2], 1 / scaling[3], 1)\n        )\n    else:\n        scaling_matrix = np.diag(\n            (1 / scaling[-1], 1 / scaling[-2], 1 / scaling[-3], 1)\n        )\n    new_affine = AffineTransform.from_array(scaling_matrix @ self.affine.matrix)\n\n    # Create new NgffImage with upsampled data\n    dims = self.dims\n    if self.axes_order == \"XYZ\":\n        new_scale = {\n            dims[1]: self.scale[dims[1]] / scaling[1],\n            dims[2]: self.scale[dims[2]] / scaling[2],\n            dims[3]: self.scale[dims[3]] / scaling[3],\n        }\n    else:\n        new_scale = {\n            dims[1]: self.scale[dims[1]] / scaling[1],\n            dims[2]: self.scale[dims[2]] / scaling[2],\n            dims[3]: self.scale[dims[3]] / scaling[3],\n        }\n\n    upsampled_ngff = nz.to_ngff_image(\n        darr_scaled,\n        dims=dims,\n        scale=new_scale,\n        translation=self.translation.copy(),\n        name=self.name,\n    )\n\n    # Return a new ZarrNii instance with the upsampled data\n    return ZarrNii.from_ngff_image(\n        upsampled_ngff,\n        axes_order=self.axes_order,\n        orientation=self.orientation,\n        omero=self.omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.upsample--upsample-with-scaling-factors","title":"Upsample with scaling factors","text":"<p>upsampled_znimg = znimg.upsample(along_x=2, along_y=2, along_z=2)</p>"},{"location":"reference/#zarrnii.ZarrNii.upsample--upsample-to-a-specific-shape","title":"Upsample to a specific shape","text":"<p>upsampled_znimg = znimg.upsample(to_shape=(1, 256, 256, 256))</p>"},{"location":"reference/#zarrnii.transform.AffineTransform.matrix","title":"<code>matrix = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/#zarrnii.transform.AffineTransform.__array__","title":"<code>__array__()</code>","text":"<p>Define how the object behaves when converted to a numpy array. Returns the matrix of the affine transform.</p> Source code in <code>zarrnii/transform.py</code> <pre><code>def __array__(self):\n    \"\"\"\n    Define how the object behaves when converted to a numpy array.\n    Returns the matrix of the affine transform.\n    \"\"\"\n    return self.matrix\n</code></pre>"},{"location":"reference/#zarrnii.transform.AffineTransform.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Enable array-like indexing on the matrix.</p> Source code in <code>zarrnii/transform.py</code> <pre><code>def __getitem__(self, key):\n    \"\"\"\n    Enable array-like indexing on the matrix.\n    \"\"\"\n    return self.matrix[key]\n</code></pre>"},{"location":"reference/#zarrnii.transform.AffineTransform.__matmul__","title":"<code>__matmul__(other)</code>","text":"<p>Perform matrix multiplication with another object.</p> <ul> <li>other (np.ndarray or AffineTransform): The object to multiply with:<ul> <li>(3,) or (3, 1): A 3D point or vector (voxel coordinates).</li> <li>(3, N): A batch of N 3D points or vectors (voxel coordinates).</li> <li>(4,) or (4, 1): A 4D point/vector in homogeneous coordinates.</li> <li>(4, N): A batch of N 4D points in homogeneous coordinates.</li> <li>(4, 4): Another affine transformation matrix.</li> </ul> </li> </ul> <ul> <li>np.ndarray or AffineTransform:<ul> <li>Transformed 3D point(s) or vector(s) as a numpy array.</li> <li>A new AffineTransform object if multiplying two affine matrices.</li> </ul> </li> </ul> <p>Raises: - ValueError: If the shape of <code>other</code> is unsupported. - TypeError: If <code>other</code> is not an np.ndarray or AffineTransform.</p> Source code in <code>zarrnii/transform.py</code> <pre><code>def __matmul__(self, other):\n    \"\"\"\n    Perform matrix multiplication with another object.\n\n    Parameters:\n    - other (np.ndarray or AffineTransform): The object to multiply with:\n        - (3,) or (3, 1): A 3D point or vector (voxel coordinates).\n        - (3, N): A batch of N 3D points or vectors (voxel coordinates).\n        - (4,) or (4, 1): A 4D point/vector in homogeneous coordinates.\n        - (4, N): A batch of N 4D points in homogeneous coordinates.\n        - (4, 4): Another affine transformation matrix.\n\n    Returns:\n    - np.ndarray or AffineTransform:\n        - Transformed 3D point(s) or vector(s) as a numpy array.\n        - A new AffineTransform object if multiplying two affine matrices.\n\n    Raises:\n    - ValueError: If the shape of `other` is unsupported.\n    - TypeError: If `other` is not an np.ndarray or AffineTransform.\n    \"\"\"\n    if isinstance(other, np.ndarray):\n        if other.shape == (3,):\n            # Single 3D point/vector\n            homog_point = np.append(other, 1)  # Convert to homogeneous coordinates\n            result = self.matrix @ homog_point\n            return result[:3] / result[3]  # Convert back to 3D\n        elif len(other.shape) == 2 and other.shape[0] == 3:\n            # Batch of 3D points/vectors (3 x N)\n            homog_points = np.vstack(\n                [other, np.ones((1, other.shape[1]))]\n            )  # Add homogeneous row\n            transformed_points = (\n                self.matrix @ homog_points\n            )  # Apply affine transform\n            return (\n                transformed_points[:3] / transformed_points[3]\n            )  # Convert back to 3D\n        elif other.shape == (4,):\n            # Single 4D point/vector\n            result = self.matrix @ other\n            return result[:3] / result[3]\n        elif len(other.shape) == 2 and other.shape[0] == 4:\n            # Batch of 4D points in homogeneous coordinates (4 x N)\n            transformed_points = self.matrix @ other  # Apply affine transform\n            return transformed_points  # No conversion needed, stays in 4D space\n        elif other.shape == (4, 4):\n            # Matrix multiplication with another affine matrix\n            return AffineTransform.from_array(self.matrix @ other)\n        else:\n            raise ValueError(f\"Unsupported shape for multiplication: {other.shape}\")\n    elif isinstance(other, AffineTransform):\n        # Matrix multiplication with another AffineTransform object\n        return AffineTransform.from_array(self.matrix @ other.matrix)\n    else:\n        raise TypeError(f\"Unsupported type for multiplication: {type(other)}\")\n</code></pre>"},{"location":"reference/#zarrnii.transform.AffineTransform.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Enable array-like assignment to the matrix.</p> Source code in <code>zarrnii/transform.py</code> <pre><code>def __setitem__(self, key, value):\n    \"\"\"\n    Enable array-like assignment to the matrix.\n    \"\"\"\n    self.matrix[key] = value\n</code></pre>"},{"location":"reference/#zarrnii.transform.AffineTransform.apply_transform","title":"<code>apply_transform(vecs)</code>","text":"Source code in <code>zarrnii/transform.py</code> <pre><code>def apply_transform(self, vecs: np.array) -&gt; np.array:\n    return self @ vecs\n</code></pre>"},{"location":"reference/#zarrnii.transform.AffineTransform.from_array","title":"<code>from_array(matrix, invert=False)</code>  <code>classmethod</code>","text":"Source code in <code>zarrnii/transform.py</code> <pre><code>@classmethod\ndef from_array(cls, matrix, invert=False):\n    if invert:\n        matrix = np.linalg.inv(matrix)\n\n    return cls(matrix=matrix)\n</code></pre>"},{"location":"reference/#zarrnii.transform.AffineTransform.from_txt","title":"<code>from_txt(path, invert=False)</code>  <code>classmethod</code>","text":"Source code in <code>zarrnii/transform.py</code> <pre><code>@classmethod\ndef from_txt(cls, path, invert=False):\n    matrix = np.loadtxt(path)\n    if invert:\n        matrix = np.linalg.inv(matrix)\n\n    return cls(matrix=matrix)\n</code></pre>"},{"location":"reference/#zarrnii.transform.AffineTransform.identity","title":"<code>identity()</code>  <code>classmethod</code>","text":"Source code in <code>zarrnii/transform.py</code> <pre><code>@classmethod\ndef identity(cls):\n    return cls(matrix=np.eye(4, 4))\n</code></pre>"},{"location":"reference/#zarrnii.transform.AffineTransform.invert","title":"<code>invert()</code>","text":"<p>Return the inverse of the matrix transformation.</p> Source code in <code>zarrnii/transform.py</code> <pre><code>def invert(self):\n    \"\"\"Return the inverse of the matrix transformation.\"\"\"\n    return AffineTransform.from_array(np.linalg.inv(self.matrix))\n</code></pre>"},{"location":"reference/#zarrnii.transform.AffineTransform.update_for_orientation","title":"<code>update_for_orientation(input_orientation, output_orientation)</code>","text":"<p>Update the matrix to map from the input orientation to the output orientation.</p> <p>Parameters:</p> Name Type Description Default <code>input_orientation</code> <code>str</code> <p>Current anatomical orientation (e.g., 'RPI').</p> required <code>output_orientation</code> <code>str</code> <p>Target anatomical orientation (e.g., 'RAS').</p> required Source code in <code>zarrnii/transform.py</code> <pre><code>def update_for_orientation(self, input_orientation, output_orientation):\n    \"\"\"\n    Update the matrix to map from the input orientation to the output orientation.\n\n    Parameters:\n        input_orientation (str): Current anatomical orientation (e.g., 'RPI').\n        output_orientation (str): Target anatomical orientation (e.g., 'RAS').\n    \"\"\"\n\n    # Define a mapping of anatomical directions to axis indices and flips\n    axis_map = {\n        \"R\": (0, 1),\n        \"L\": (0, -1),\n        \"A\": (1, 1),\n        \"P\": (1, -1),\n        \"S\": (2, 1),\n        \"I\": (2, -1),\n    }\n\n    # Parse the input and output orientations\n    input_axes = [axis_map[ax] for ax in input_orientation]\n    output_axes = [axis_map[ax] for ax in output_orientation]\n\n    # Create a mapping from input to output\n    reorder_indices = [None] * 3\n    flip_signs = [1] * 3\n\n    for out_idx, (out_axis, out_sign) in enumerate(output_axes):\n        for in_idx, (in_axis, in_sign) in enumerate(input_axes):\n            if out_axis == in_axis:  # Match axis\n                reorder_indices[out_idx] = in_idx\n                flip_signs[out_idx] = out_sign * in_sign\n                break\n\n    # Reorder and flip the affine matrix\n    reordered_matrix = np.zeros_like(self.matrix)\n    for i, (reorder_idx, flip_sign) in enumerate(zip(reorder_indices, flip_signs)):\n        if reorder_idx is None:\n            raise ValueError(\n                f\"Cannot match all axes from {input_orientation} to {output_orientation}.\"\n            )\n        reordered_matrix[i, :3] = flip_sign * self.matrix[reorder_idx, :3]\n        reordered_matrix[i, 3] = flip_sign * self.matrix[reorder_idx, 3]\n    reordered_matrix[3, :] = self.matrix[3, :]  # Preserve the homogeneous row\n\n    return AffineTransform.from_array(reordered_matrix)\n</code></pre>"},{"location":"reference/#zarrnii.transform.DisplacementTransform.disp_affine","title":"<code>disp_affine = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/#zarrnii.transform.DisplacementTransform.disp_grid","title":"<code>disp_grid = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/#zarrnii.transform.DisplacementTransform.disp_xyz","title":"<code>disp_xyz = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/#zarrnii.transform.DisplacementTransform.apply_transform","title":"<code>apply_transform(vecs)</code>","text":"Source code in <code>zarrnii/transform.py</code> <pre><code>def apply_transform(self, vecs: np.array) -&gt; np.array:\n    # we have the grid points, the volumes to interpolate displacements\n\n    # first we need to transform points to vox space of the warp\n    vox_vecs = self.disp_affine.invert() @ vecs\n\n    # then interpolate the displacement in x, y, z:\n    disp_vecs = np.zeros(vox_vecs.shape)\n\n    for ax in range(3):\n        disp_vecs[ax, :] = interpn(\n            self.disp_grid,\n            self.disp_xyz[:, :, :, ax].squeeze(),\n            vox_vecs[:3, :].T,\n            method=\"linear\",\n            bounds_error=False,\n            fill_value=0,\n        )\n\n    return vecs + disp_vecs\n</code></pre>"},{"location":"reference/#zarrnii.transform.DisplacementTransform.from_nifti","title":"<code>from_nifti(path)</code>  <code>classmethod</code>","text":"Source code in <code>zarrnii/transform.py</code> <pre><code>@classmethod\ndef from_nifti(cls, path):\n    disp_nib = nib.load(path)\n    disp_xyz = disp_nib.get_fdata().squeeze()\n    disp_affine = AffineTransform.from_array(disp_nib.affine)\n\n    # convert from itk transform\n    disp_xyz[:, :, :, 0] = -disp_xyz[:, :, :, 0]\n    disp_xyz[:, :, :, 1] = -disp_xyz[:, :, :, 1]\n\n    disp_grid = (\n        np.arange(disp_xyz.shape[0]),\n        np.arange(disp_xyz.shape[1]),\n        np.arange(disp_xyz.shape[2]),\n    )\n\n    return cls(\n        disp_xyz=disp_xyz,\n        disp_grid=disp_grid,\n        disp_affine=disp_affine,\n    )\n</code></pre>"},{"location":"examples/downsampling/","title":"Downsampling and Upsampling","text":"<p>This section covers resolution changes in ZarrNii, including downsampling for efficient processing and upsampling for analysis.</p>"},{"location":"examples/downsampling/#overview","title":"Overview","text":"<p>ZarrNii provides methods for changing image resolution through downsampling and upsampling operations. These are essential for creating multi-resolution datasets and efficient processing workflows.</p>"},{"location":"examples/downsampling/#downsampling","title":"Downsampling","text":""},{"location":"examples/downsampling/#basic-downsampling","title":"Basic Downsampling","text":"<pre><code>from zarrnii import ZarrNii\n\n# Load a high-resolution dataset\nznimg = ZarrNii.from_nifti(\"path/to/highres.nii\")\nprint(\"Original shape:\", znimg.darr.shape)\n\n# Downsample by level (2^level reduction)\ndownsampled = znimg.downsample(level=2)\nprint(\"Downsampled shape:\", downsampled.darr.shape)\n</code></pre>"},{"location":"examples/downsampling/#custom-downsampling-factors","title":"Custom Downsampling Factors","text":"<pre><code># Downsample with specific factors for each axis\ndownsampled_custom = znimg.downsample(\n    along_x=2, \n    along_y=2, \n    along_z=1  # 2x in X,Y; no change in Z\n)\n</code></pre>"},{"location":"examples/downsampling/#upsampling","title":"Upsampling","text":""},{"location":"examples/downsampling/#basic-upsampling","title":"Basic Upsampling","text":"<pre><code># Upsample by factors\nupsampled = znimg.upsample(\n    along_x=2, \n    along_y=2, \n    along_z=1  # 2x in X,Y; no change in Z\n)\n\n# Upsample to specific target shape\ntarget_shape = (100, 200, 300)\nupsampled_to_shape = znimg.upsample(to_shape=target_shape)\n</code></pre>"},{"location":"examples/downsampling/#multi-resolution-workflows","title":"Multi-Resolution Workflows","text":""},{"location":"examples/downsampling/#creating-image-pyramids","title":"Creating Image Pyramids","text":"<pre><code># Create multiple resolution levels\npyramid_levels = []\ncurrent = znimg\n\nfor level in range(4):\n    pyramid_levels.append(current)\n    current = current.downsample(level=1)\n    print(f\"Level {level}: {current.darr.shape}\")\n</code></pre>"},{"location":"examples/downsampling/#working-with-ome-zarr-multi-resolution","title":"Working with OME-Zarr Multi-Resolution","text":"<pre><code># Load multi-resolution OME-Zarr at specific level\nznimg_level0 = ZarrNii.from_ome_zarr(\"path/to/multires.zarr\", level=0)\nznimg_level2 = ZarrNii.from_ome_zarr(\"path/to/multires.zarr\", level=2)\n\n# Create new multi-resolution OME-Zarr\nznimg.to_ome_zarr(\n    \"output_multires.zarr\",\n    max_layer=4\n)\n</code></pre>"},{"location":"examples/downsampling/#memory-efficient-processing","title":"Memory-Efficient Processing","text":"<pre><code># Work with large images efficiently using Dask\n# Downsampling is lazy and computed only when needed\ndownsampled = znimg.downsample(level=2)\n\n# Compute result when needed\nresult = downsampled.darr.compute()\n</code></pre>"},{"location":"examples/downsampling/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use appropriate chunk sizes: For Dask arrays, ensure chunks are well-sized for your operations</li> <li>Lazy evaluation: Downsampling operations are lazy and computed only when <code>.compute()</code> is called</li> <li>Memory management: Use <code>.rechunk()</code> if needed to optimize chunk sizes for your workflow</li> <li>Level-based downsampling: Use <code>level</code> parameter for consistent 2^level reductions</li> </ol>"},{"location":"examples/downsampling/#see-also","title":"See Also","text":"<ul> <li>Multiscale Processing for advanced multi-resolution workflows  </li> <li>Working with Zarr and NIfTI for basic format operations</li> <li>API Reference for detailed method documentation</li> </ul>"},{"location":"examples/multiscale/","title":"Multiscale OME-Zarr","text":"<p>This section covers working with multi-resolution OME-Zarr datasets, including creation, manipulation, and optimization strategies.</p>"},{"location":"examples/multiscale/#overview","title":"Overview","text":"<p>OME-Zarr supports multi-resolution image pyramids that enable efficient visualization and analysis at different scales. ZarrNii provides support for creating and reading multiscale datasets.</p>"},{"location":"examples/multiscale/#understanding-ome-zarr-multiscale","title":"Understanding OME-Zarr Multiscale","text":""},{"location":"examples/multiscale/#loading-multiscale-data","title":"Loading Multiscale Data","text":"<pre><code>from zarrnii import ZarrNii\nimport zarr\n\n# Load different resolution levels of a multiscale OME-Zarr dataset\nznimg_level0 = ZarrNii.from_ome_zarr(\"path/to/multiscale.zarr\", level=0)  # Full resolution\nznimg_level1 = ZarrNii.from_ome_zarr(\"path/to/multiscale.zarr\", level=1)  # Half resolution\nznimg_level2 = ZarrNii.from_ome_zarr(\"path/to/multiscale.zarr\", level=2)  # Quarter resolution\n\nprint(\"Level 0 shape:\", znimg_level0.darr.shape)\nprint(\"Level 1 shape:\", znimg_level1.darr.shape)\nprint(\"Level 2 shape:\", znimg_level2.darr.shape)\n</code></pre>"},{"location":"examples/multiscale/#inspecting-available-levels","title":"Inspecting Available Levels","text":"<pre><code># Use zarr directly to inspect the structure\nstore = zarr.open_group(\"path/to/multiscale.zarr\", mode='r')\nprint(\"Available arrays:\", list(store.keys()))\n\n# Check shapes at each level\nfor key in sorted(store.keys()):\n    if key.isdigit():\n        array = store[key]\n        print(f\"Level {key}: shape {array.shape}, chunks {array.chunks}\")\n</code></pre>"},{"location":"examples/multiscale/#creating-multiscale-ome-zarr","title":"Creating Multiscale OME-Zarr","text":""},{"location":"examples/multiscale/#basic-multiscale-creation","title":"Basic Multiscale Creation","text":"<pre><code># Load a single-resolution image\nznimg = ZarrNii.from_nifti(\"path/to/highres.nii\")\n\n# Create multiscale OME-Zarr with default parameters\nznimg.to_ome_zarr(\n    \"output_multiscale.zarr\",\n    max_layer=4  # Creates 4 downsampling levels\n)\n</code></pre>"},{"location":"examples/multiscale/#custom-multiscale-parameters","title":"Custom Multiscale Parameters","text":"<pre><code># Create multiscale with custom settings\nznimg.to_ome_zarr(\n    \"custom_multiscale.zarr\",\n    max_layer=6,  # More downsampling levels\n    scaling_method=None  # Use default downsampling\n)\n</code></pre>"},{"location":"examples/multiscale/#working-with-different-resolution-levels","title":"Working with Different Resolution Levels","text":""},{"location":"examples/multiscale/#processing-at-different-scales","title":"Processing at Different Scales","text":"<pre><code># Load and process at different resolutions for different tasks\n\n# Use low resolution for quick overview\nthumbnail = ZarrNii.from_ome_zarr(\"data.zarr\", level=3)\noverview_stats = compute_statistics(thumbnail)\n\n# Use medium resolution for analysis  \nanalysis_res = ZarrNii.from_ome_zarr(\"data.zarr\", level=1)\nfeature_map = extract_features(analysis_res)\n\n# Use full resolution for final processing\nfull_res = ZarrNii.from_ome_zarr(\"data.zarr\", level=0)\nfinal_result = apply_detailed_processing(full_res)\n</code></pre>"},{"location":"examples/multiscale/#multi-resolution-workflow","title":"Multi-Resolution Workflow","text":"<pre><code># Progressive processing workflow\ndef progressive_analysis(zarr_path):\n    # Start with thumbnail for parameter estimation\n    low_res = ZarrNii.from_ome_zarr(zarr_path, level=3)\n    parameters = estimate_parameters(low_res)\n\n    # Refine on medium resolution\n    med_res = ZarrNii.from_ome_zarr(zarr_path, level=1) \n    refined_params = refine_parameters(med_res, parameters)\n\n    # Apply to full resolution\n    full_res = ZarrNii.from_ome_zarr(zarr_path, level=0)\n    final_result = process_with_params(full_res, refined_params)\n\n    return final_result\n\nresult = progressive_analysis(\"multiscale_data.zarr\")\n</code></pre>"},{"location":"examples/multiscale/#channel-and-time-series-support","title":"Channel and Time Series Support","text":""},{"location":"examples/multiscale/#multi-channel-multiscale","title":"Multi-Channel Multiscale","text":"<pre><code># Load specific channels from multiscale data\n# Channel selection works with any resolution level\ndapi_full = ZarrNii.from_ome_zarr(\"multi_channel.zarr\", level=0, channels=[0])\ngfp_thumbnail = ZarrNii.from_ome_zarr(\"multi_channel.zarr\", level=3, channels=[1])\n\nprint(\"DAPI full resolution:\", dapi_full.darr.shape)\nprint(\"GFP thumbnail:\", gfp_thumbnail.darr.shape)\n</code></pre>"},{"location":"examples/multiscale/#channel-labels","title":"Channel labels","text":"<pre><code># If channel labels are present from OME metadata (e.g. for data from SPIMprep)\n# You can select channels based on label\nabeta_full = ZarrNii.from_ome_zarr(\"multi_channel.zarr\", level=3, channel_labels=[\"Abeta\"])\n</code></pre>"},{"location":"examples/multiscale/#memory-management","title":"Memory Management","text":""},{"location":"examples/multiscale/#efficient-loading","title":"Efficient Loading","text":"<pre><code># Load only what you need\n# Zarr and Dask handle lazy loading automatically\n\n# Load with appropriate chunking\nznimg = ZarrNii.from_ome_zarr(\"large_dataset.zarr\", level=1)\n\n# Process in blocks to manage memory\ndef process_large_dataset(znimg):\n    # Process data block by block\n    result = znimg.darr.map_blocks(\n        process_block,\n        dtype=np.float32,\n        drop_axis=None\n    )\n    return result\n\nprocessed = process_large_dataset(znimg)\n</code></pre>"},{"location":"examples/multiscale/#best-practices","title":"Best Practices","text":""},{"location":"examples/multiscale/#choosing-resolution-levels","title":"Choosing Resolution Levels","text":"<pre><code># Guidelines for choosing appropriate resolution levels\n\ndef choose_resolution_level(task_type, data_size):\n    \"\"\"Choose optimal resolution level based on task and data size\"\"\"\n    if task_type == \"thumbnail\":\n        return 4  # Very low resolution for quick preview\n    elif task_type == \"segmentation\":\n        return 1  # Medium resolution for segmentation\n    elif task_type == \"measurement\":\n        return 0  # Full resolution for accurate measurements\n    else:\n        # Default to medium resolution\n        return 2\n\n# Use the function\nlevel = choose_resolution_level(\"segmentation\", data.shape)\nznimg = ZarrNii.from_ome_zarr(\"data.zarr\", level=level)\n</code></pre>"},{"location":"examples/multiscale/#performance-tips","title":"Performance Tips","text":"<ol> <li>Choose appropriate levels: Use lower resolution levels for exploratory analysis</li> <li>Minimize data loading: Only load the resolution level you actually need</li> <li>Leverage lazy evaluation: Let Dask handle memory management automatically</li> </ol>"},{"location":"examples/multiscale/#see-also","title":"See Also","text":"<ul> <li>Downsampling and Upsampling for resolution change operations</li> <li>Working with Zarr and NIfTI for basic format operations  </li> <li>API Reference for detailed method documentation</li> </ul>"},{"location":"examples/transformations/","title":"Transformations","text":"<p>This section covers spatial transformations in ZarrNii, including affine transforms and displacement fields.</p>"},{"location":"examples/transformations/#overview","title":"Overview","text":"<p>ZarrNii provides support for spatial transformations through the <code>AffineTransform</code> and <code>DisplacementTransform</code> classes. These can be used to apply transformations derived from performing ANTS registration on downsampled (e.g. level &gt; 3)  images, then applied to higher resolution (e.g. level &lt; 3) images. ZarrNii performs these computations in a block-wise manner using Dask, upsampling the displacement field for a block before applying it. You can also provide a sequence of transforms to perform composition of transformations to resample in a single step.</p>"},{"location":"examples/transformations/#affine-transformations","title":"Affine Transformations","text":""},{"location":"examples/transformations/#creating-affine-transformations","title":"Creating Affine Transformations","text":"<pre><code>from zarrnii import ZarrNii\nfrom zarrnii.transform import AffineTransform\nimport numpy as np\n\n# Load a dataset\nznimg = ZarrNii.from_nifti(\"path/to/image.nii\")\n\n# Create identity transformation\nidentity_transform = AffineTransform.identity()\n\n# Create transformation from a matrix\nmatrix = np.array([\n    [2.0, 0.0, 0.0, 10.0],  # Scale X by 2, translate by 10\n    [0.0, 2.0, 0.0, -5.0],  # Scale Y by 2, translate by -5\n    [0.0, 0.0, 1.0, 0.0],   # No change in Z\n    [0.0, 0.0, 0.0, 1.0]    # Homogeneous coordinates\n])\ntransform = AffineTransform.from_array(matrix)\n\n# Load transformation from text file\ntransform_from_file = AffineTransform.from_txt(\"transform.txt\")\n</code></pre>"},{"location":"examples/transformations/#applying-transformations","title":"Applying Transformations","text":"<pre><code># Apply transformation (requires reference image)\nref_znimg = ZarrNii.from_nifti(\"path/to/reference.nii\")\ntransformed = znimg.apply_transform(transform, ref_znimg=ref_znimg)\n</code></pre>"},{"location":"examples/transformations/#working-with-displacement-transformations","title":"Working with Displacement Transformations","text":"<pre><code>from zarrnii.transform import DisplacementTransform\n\n# Load displacement field from NIfTI file\ndisp_transform = DisplacementTransform.from_nifti(\"displacement_field.nii\")\n\n# Apply displacement transformation\ndeformed = znimg.apply_transform(disp_transform, ref_znimg=ref_znimg)\n</code></pre>"},{"location":"examples/transformations/#multiple-transformations","title":"Multiple Transformations","text":"<pre><code># Apply multiple transformations in sequence\n# Each transformation is applied sequentially\nresult = znimg.apply_transform(transform1, transform2, ref_znimg=ref_znimg)\n</code></pre>"},{"location":"examples/transformations/#coordinate-transformations","title":"Coordinate Transformations","text":"<pre><code># Transform coordinates using the matrix multiplication operator\nvoxel_coords = np.array([50, 60, 30])\nras_coords = transform @ voxel_coords\n\n# Transform multiple points\npoints = np.array([[50, 60, 30], [100, 120, 60]]).T  # 3xN array\ntransformed_points = transform @ points\n</code></pre>"},{"location":"examples/transformations/#inverting-transformations","title":"Inverting Transformations","text":"<pre><code># Get the inverse of a transformation\ninverse_transform = transform.invert()\n\n# Apply inverse transformation\nrestored = transformed_znimg.apply_transform(inverse_transform, ref_znimg=znimg)\n</code></pre>"},{"location":"examples/transformations/#coordinate-system-handling","title":"Coordinate System Handling","text":"<pre><code># Update transformation for different orientations\nupdated_transform = transform.update_for_orientation(\"RPI\", \"RAS\")\n</code></pre>"},{"location":"examples/transformations/#best-practices","title":"Best Practices","text":"<ol> <li>Use ref_znimg parameter: Always provide a reference image when applying transformations</li> <li>Consider coordinate systems: Be aware of voxel vs RAS coordinate conventions  </li> <li>Memory efficiency: Use lazy evaluation with Dask arrays for large datasets</li> <li>Transformation order: Remember that multiple transforms are applied sequentially</li> </ol>"},{"location":"examples/transformations/#see-also","title":"See Also","text":"<ul> <li>API Reference for detailed method documentation</li> <li>Downsampling and Upsampling for resolution change operations</li> <li>Working with Zarr and NIfTI for basic format operations</li> </ul>"},{"location":"examples/zarr_nifti/","title":"Examples: Working with Zarr and NIfTI","text":"<p>This section provides practical workflows for using ZarrNii with OME-Zarr and NIfTI datasets.</p>"},{"location":"examples/zarr_nifti/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Loading Datasets<ul> <li>From OME-Zarr</li> <li>From NIfTI</li> </ul> </li> <li>Performing Transformations<ul> <li>Downsampling</li> <li>Cropping</li> <li>Combining Affine Transformations</li> </ul> </li> <li>Saving Results<ul> <li>To OME-Zarr</li> <li>To NIfTI</li> </ul> </li> <li>Advanced Example: Full Workflow</li> </ol>"},{"location":"examples/zarr_nifti/#loading-datasets","title":"Loading Datasets","text":""},{"location":"examples/zarr_nifti/#from-ome-zarr","title":"From OME-Zarr","text":"<p>Load a dataset from an OME-Zarr file and inspect its metadata:</p> <pre><code>from zarrnii import ZarrNii\n\n# Load OME-Zarr dataset\nznimg = ZarrNii.from_ome_zarr(\"path/to/dataset.zarr\")\n\n# Inspect data\nprint(\"Shape:\", znimg.darr.shape)\nprint(\"Affine matrix:\\n\", znimg.affine.matrix)\n</code></pre>"},{"location":"examples/zarr_nifti/#from-nifti","title":"From NIfTI","text":"<p>Load a NIfTI dataset and inspect its attributes:</p> <pre><code># Load NIfTI dataset\nznimg = ZarrNii.from_nifti(\"path/to/dataset.nii\")\n\n# Inspect data\nprint(\"Shape:\", znimg.darr.shape)\nprint(\"Affine matrix:\\n\", znimg.affine.matrix)\n</code></pre>"},{"location":"examples/zarr_nifti/#performing-transformations","title":"Performing Transformations","text":""},{"location":"examples/zarr_nifti/#downsampling","title":"Downsampling","text":"<p>Reduce the resolution of the dataset using the <code>downsample</code> method:</p> <pre><code># Downsample by level\ndownsampled = znimg.downsample(level=2)\nprint(\"Downsampled shape:\", downsampled.darr.shape)\n</code></pre>"},{"location":"examples/zarr_nifti/#cropping","title":"Cropping","text":"<p>Extract a specific region from the dataset using bounding boxes:</p>"},{"location":"examples/zarr_nifti/#voxel-space","title":"Voxel Space:","text":"<pre><code>cropped = znimg.crop((10, 10, 10), (50, 50, 50))\nprint(\"Cropped shape:\", cropped.darr.shape)\n</code></pre>"},{"location":"examples/zarr_nifti/#with-ras-coordinates","title":"With RAS Coordinates:","text":"<pre><code># Note: crop_with_bounding_box is a legacy method that still supports RAS coords\ncropped_ras = znimg.crop_with_bounding_box(\n    (-20, -20, -20), (20, 20, 20), ras_coords=True\n)\nprint(\"Cropped shape:\", cropped_ras.darr.shape)\n</code></pre>"},{"location":"examples/zarr_nifti/#combining-affine-transformations","title":"Combining Affine Transformations","text":"<p>Apply multiple transformations to the dataset in sequence:</p> <pre><code>from zarrnii.transform import AffineTransform\nimport numpy as np\n\n# Define transformations using matrices\nscale_matrix = np.array([\n    [2.0, 0.0, 0.0, 0.0],\n    [0.0, 2.0, 0.0, 0.0], \n    [0.0, 0.0, 1.0, 0.0],\n    [0.0, 0.0, 0.0, 1.0]\n])\nscale = AffineTransform.from_array(scale_matrix)\n\ntranslate_matrix = np.array([\n    [1.0, 0.0, 0.0, 10.0],\n    [0.0, 1.0, 0.0, -5.0],\n    [0.0, 0.0, 1.0, 0.0],\n    [0.0, 0.0, 0.0, 1.0]\n])\ntranslate = AffineTransform.from_array(translate_matrix)\n\n# Apply transformations\ntransformed = znimg.apply_transform(scale, translate, ref_znimg=znimg)\nprint(\"Transformed affine matrix:\\n\", transformed.affine.matrix)\n</code></pre>"},{"location":"examples/zarr_nifti/#saving-results","title":"Saving Results","text":""},{"location":"examples/zarr_nifti/#to-ome-zarr","title":"To OME-Zarr","text":"<p>Save the dataset to OME-Zarr format:</p> <pre><code>znimg.to_ome_zarr(\"output.zarr\", max_layer=3, scaling_method=\"local_mean\")\n</code></pre>"},{"location":"examples/zarr_nifti/#to-nifti","title":"To NIfTI","text":"<p>Save the dataset to NIfTI format:</p> <pre><code>znimg.to_nifti(\"output.nii\")\n</code></pre>"},{"location":"examples/zarr_nifti/#advanced-example-full-workflow","title":"Advanced Example: Full Workflow","text":"<p>Combine multiple operations in a single workflow:</p> <pre><code>from zarrnii import ZarrNii\nfrom zarrnii.transform import AffineTransform\nimport numpy as np\n\n# Load an OME-Zarr dataset\nznimg = ZarrNii.from_ome_zarr(\"path/to/dataset.zarr\")\n\n# Crop the dataset\ncropped = znimg.crop((10, 10, 10), (100, 100, 100))\n\n# Downsample the dataset\ndownsampled = cropped.downsample(level=2)\n\n# Apply an affine transformation\nscale_matrix = np.array([\n    [1.5, 0.0, 0.0, 0.0],\n    [0.0, 1.5, 0.0, 0.0],\n    [0.0, 0.0, 1.0, 0.0],\n    [0.0, 0.0, 0.0, 1.0]\n])\nscale = AffineTransform.from_array(scale_matrix)\ntransformed = downsampled.apply_transform(scale, ref_znimg=downsampled)\n\n# Save the result as a NIfTI file\ntransformed.to_nifti(\"final_output.nii\")\n</code></pre>"},{"location":"examples/zarr_nifti/#summary","title":"Summary","text":"<p>In this section, you learned how to: - Load datasets from OME-Zarr and NIfTI formats. - Perform transformations like downsampling, cropping, and affine transformations. - Save results back to OME-Zarr or NIfTI.</p> <p>Next: - Explore the API Reference for in-depth details about ZarrNii's classes and methods. - Check the FAQ for answers to common questions.</p>"},{"location":"walkthrough/advanced_use_cases/","title":"Walkthrough: Advanced Use Cases","text":"<p>This guide explores advanced workflows with ZarrNii, including metadata preservation, handling multiscale OME-Zarr pyramids, and combining multiple transformations.</p>"},{"location":"walkthrough/advanced_use_cases/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Preserving Metadata</li> <li>Working with Multiscale Pyramids</li> <li>Combining Transformations</li> <li>Handling Large Datasets</li> </ol>"},{"location":"walkthrough/advanced_use_cases/#preserving-metadata","title":"Preserving Metadata","text":"<p>ZarrNii is designed to handle and preserve metadata when converting between formats or applying transformations.</p>"},{"location":"walkthrough/advanced_use_cases/#accessing-metadata","title":"Accessing Metadata","text":"<p>OME-Zarr metadata is automatically extracted and stored in the <code>axes</code>, <code>coordinate_transformations</code>, and <code>omero</code> attributes of a <code>ZarrNii</code> instance.</p> <pre><code>znimg = ZarrNii.from_ome_zarr(\"path/to/dataset.zarr\")\n\n# Access axes metadata\nprint(\"Axes metadata:\", znimg.axes)\n\n# Access coordinate transformations\nprint(\"Coordinate transformations:\", znimg.coordinate_transformations)\n\n# Access Omero metadata\nprint(\"Omero metadata:\", znimg.omero)\n</code></pre>"},{"location":"walkthrough/advanced_use_cases/#preserving-metadata-during-transformations","title":"Preserving Metadata During Transformations","text":"<p>When you perform transformations like cropping or downsampling, ZarrNii ensures metadata remains consistent.</p> <pre><code>cropped = znimg.crop((10, 10, 10), (50, 50, 50))\nprint(\"Updated metadata:\", cropped.coordinate_transformations)\n</code></pre>"},{"location":"walkthrough/advanced_use_cases/#working-with-multiscale-pyramids","title":"Working with Multiscale Pyramids","text":"<p>OME-Zarr datasets often include multiscale pyramids, where each level represents a progressively downsampled version of the data.</p>"},{"location":"walkthrough/advanced_use_cases/#loading-a-specific-level","title":"Loading a Specific Level","text":"<p>You can load a specific pyramid level using the <code>level</code> argument in <code>from_ome_zarr</code>:</p> <pre><code>znimg = ZarrNii.from_ome_zarr(\"path/to/dataset.zarr\", level=2)\nprint(\"Loaded shape:\", znimg.darr.shape)\n</code></pre>"},{"location":"walkthrough/advanced_use_cases/#handling-custom-downsampling","title":"Handling Custom Downsampling","text":"<p>If the desired level isn't available in the pyramid, ZarrNii computes additional downsampling lazily:</p> <pre><code>level, do_downsample, ds_kwargs = ZarrNii.get_level_and_downsampling_kwargs(\n    \"path/to/dataset.zarr\", level=5\n)\nif do_downsample:\n    znimg = znimg.downsample(**ds_kwargs)\n</code></pre>"},{"location":"walkthrough/advanced_use_cases/#combining-transformations","title":"Combining Transformations","text":"<p>ZarrNii allows you to chain multiple transformations into a single workflow. This is useful when applying affine transformations, interpolations, or warping.</p>"},{"location":"walkthrough/advanced_use_cases/#chaining-affine-transformations","title":"Chaining Affine Transformations","text":"<pre><code>from zarrnii.transform import AffineTransform\nimport numpy as np\n\n# Create transformations using matrices\nscaling_matrix = np.array([\n    [2.0, 0.0, 0.0, 0.0],\n    [0.0, 2.0, 0.0, 0.0], \n    [0.0, 0.0, 1.0, 0.0],\n    [0.0, 0.0, 0.0, 1.0]\n])\nscaling = AffineTransform.from_array(scaling_matrix)\n\ntranslation_matrix = np.array([\n    [1.0, 0.0, 0.0, 10.0],\n    [0.0, 1.0, 0.0, -5.0],\n    [0.0, 0.0, 1.0, 0.0],\n    [0.0, 0.0, 0.0, 1.0]\n])\ntranslation = AffineTransform.from_array(translation_matrix)\n\n# Apply multiple transformations sequentially\ncombined = znimg.apply_transform(scaling, translation, ref_znimg=znimg)\nprint(\"New affine matrix:\\n\", combined.affine.matrix)\n</code></pre>"},{"location":"walkthrough/advanced_use_cases/#handling-large-datasets","title":"Handling Large Datasets","text":"<p>ZarrNii leverages Dask to handle datasets that don't fit into memory.</p>"},{"location":"walkthrough/advanced_use_cases/#optimizing-chunking","title":"Optimizing Chunking","text":"<p>Ensure the dataset is chunked appropriately for operations like downsampling or interpolation:</p> <pre><code># Rechunk for efficient processing\nrechunked = znimg.darr.rechunk((1, 64, 64, 64))\nprint(\"Rechunked shape:\", rechunked.shape)\n</code></pre>"},{"location":"walkthrough/advanced_use_cases/#lazy-evaluation","title":"Lazy Evaluation","text":"<p>Most transformations in ZarrNii are lazy, meaning computations are only triggered when necessary. Use <code>.compute()</code> to materialize results.</p> <pre><code># Trigger computation\ncropped = znimg.crop((10, 10, 10), (50, 50, 50))\ncropped.darr.compute()\n</code></pre>"},{"location":"walkthrough/advanced_use_cases/#summary","title":"Summary","text":"<p>This guide covered: - Preserving metadata across transformations and format conversions. - Working with multiscale pyramids in OME-Zarr. - Combining transformations for complex workflows. - Handling large datasets efficiently with Dask.</p> <p>Next, explore: - Examples: Detailed workflows and practical use cases. - API Reference: Technical details for ZarrNii classes and methods.</p>"},{"location":"walkthrough/basic_tasks/","title":"Walkthrough: Basic Tasks","text":"<p>This guide covers the most common tasks you'll perform with ZarrNii, including reading data, performing transformations, and saving results.</p>"},{"location":"walkthrough/basic_tasks/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Reading Data<ul> <li>From OME-Zarr</li> <li>From NIfTI</li> <li>Working with 5D Data</li> </ul> </li> <li>Transforming Data<ul> <li>Cropping</li> <li>Downsampling</li> <li>Upsampling</li> <li>Applying Affine Transformations</li> </ul> </li> <li>Saving Data<ul> <li>To NIfTI</li> <li>To OME-Zarr</li> </ul> </li> </ol>"},{"location":"walkthrough/basic_tasks/#reading-data","title":"Reading Data","text":""},{"location":"walkthrough/basic_tasks/#from-ome-zarr","title":"From OME-Zarr","text":"<p>Load a dataset from an OME-Zarr file using <code>from_ome_zarr</code>:</p> <pre><code>from zarrnii import ZarrNii\n\n# Load the dataset\nznimg = ZarrNii.from_ome_zarr(\"path/to/dataset.ome.zarr\")\n\n# Inspect the data\nprint(\"Data shape:\", znimg.darr.shape)\nprint(\"Affine matrix:\\n\", znimg.affine.matrix)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#from-nifti","title":"From NIfTI","text":"<p>Load a dataset from a NIfTI file using <code>from_nifti</code>:</p> <pre><code># Load the dataset\nznimg = ZarrNii.from_nifti(\"path/to/dataset.nii\")\n\n# Inspect the data\nprint(\"Data shape:\", znimg.darr.shape)\nprint(\"Affine matrix:\\n\", znimg.affine.matrix)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#working-with-5d-data","title":"Working with 5D Data","text":"<p>ZarrNii supports 5D images with time and channel dimensions (T,C,Z,Y,X). You can select specific timepoints and channels during loading or after loading.</p>"},{"location":"walkthrough/basic_tasks/#loading-with-timepoint-selection","title":"Loading with Timepoint Selection:","text":"<pre><code># Load specific timepoints\nznimg_time = ZarrNii.from_ome_zarr(\"timeseries.zarr\", timepoints=[0, 2, 4])\nprint(\"Timepoint subset shape:\", znimg_time.darr.shape)\n\n# Load specific channels by index\nznimg_channels = ZarrNii.from_ome_zarr(\"multichannel.zarr\", channels=[0, 2])\n\n# Load specific channels by label\nznimg_labels = ZarrNii.from_ome_zarr(\"labeled.zarr\", channel_labels=[\"DAPI\", \"GFP\"])\n\n# Combine timepoint and channel selection\nznimg_subset = ZarrNii.from_ome_zarr(\"data.zarr\", timepoints=[1, 3], channels=[0])\n</code></pre>"},{"location":"walkthrough/basic_tasks/#post-loading-selection","title":"Post-loading Selection:","text":"<pre><code># Load full dataset first\nznimg = ZarrNii.from_ome_zarr(\"timeseries.zarr\")\n\n# Select timepoints after loading\nselected_time = znimg.select_timepoints([0, 2])\n\n# Select channels after loading\nselected_channels = znimg.select_channels([1, 2])\n\n# Chain selections\nsubset = znimg.select_timepoints([0, 1]).select_channels([0])\n</code></pre>"},{"location":"walkthrough/basic_tasks/#transforming-data","title":"Transforming Data","text":""},{"location":"walkthrough/basic_tasks/#cropping","title":"Cropping","text":"<p>Crop the dataset to a specific bounding box. You can define the bounding box in either voxel space or RAS (real-world) coordinates. For 5D data, cropping operates only on spatial dimensions, preserving time and channel dimensions.</p>"},{"location":"walkthrough/basic_tasks/#voxel-space-cropping","title":"Voxel Space Cropping:","text":"<pre><code># Preferred method using crop()\ncropped = znimg.crop((10, 10, 10), (50, 50, 50))\nprint(\"Cropped shape:\", cropped.darr.shape)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#ras-space-cropping","title":"RAS Space Cropping:","text":"<pre><code># Use legacy method for RAS coordinates\ncropped_ras = znimg.crop_with_bounding_box(\n    (-20, -20, -20), (20, 20, 20), ras_coords=True\n)\nprint(\"Cropped shape:\", cropped_ras.darr.shape)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#downsampling","title":"Downsampling","text":"<p>Downsample the dataset to reduce its resolution. You can specify either a downsampling level or individual scaling factors for each axis. For 5D data, downsampling operates only on spatial dimensions, preserving time and channel dimensions.</p>"},{"location":"walkthrough/basic_tasks/#by-level","title":"By Level:","text":"<pre><code>downsampled = znimg.downsample(level=2)\nprint(\"Downsampled shape:\", downsampled.darr.shape)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#by-scaling-factors","title":"By Scaling Factors:","text":"<pre><code>downsampled_manual = znimg.downsample(along_x=2, along_y=2, along_z=1)\nprint(\"Downsampled shape:\", downsampled_manual.darr.shape)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#upsampling","title":"Upsampling","text":"<p>Increase the resolution of the dataset by upsampling.</p>"},{"location":"walkthrough/basic_tasks/#by-scaling-factors_1","title":"By Scaling Factors:","text":"<pre><code>upsampled = znimg.upsample(along_x=2, along_y=2, along_z=2)\nprint(\"Upsampled shape:\", upsampled.darr.shape)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#to-target-shape","title":"To Target Shape:","text":"<pre><code>upsampled_target = znimg.upsample(to_shape=(1, 256, 256, 256))\nprint(\"Upsampled shape:\", upsampled_target.darr.shape)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#applying-affine-transformations","title":"Applying Affine Transformations","text":"<p>Apply a custom affine transformation to the dataset.</p> <pre><code>from zarrnii.transform import AffineTransform\nimport numpy as np\n\n# Define a scaling transformation using a matrix\nscaling_matrix = np.array([\n    [2.0, 0.0, 0.0, 0.0],\n    [0.0, 2.0, 0.0, 0.0], \n    [0.0, 0.0, 1.0, 0.0],\n    [0.0, 0.0, 0.0, 1.0]\n])\nscaling_transform = AffineTransform.from_array(scaling_matrix)\n\n# Apply the transformation  \ntransformed = znimg.apply_transform(scaling_transform, ref_znimg=znimg)\nprint(\"Transformed affine matrix:\\n\", transformed.affine.matrix)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#saving-data","title":"Saving Data","text":""},{"location":"walkthrough/basic_tasks/#to-nifti","title":"To NIfTI","text":"<p>Save the dataset as a NIfTI file using <code>to_nifti</code>:</p> <pre><code>znimg.to_nifti(\"output_dataset.nii\")\n</code></pre>"},{"location":"walkthrough/basic_tasks/#to-ome-zarr","title":"To OME-Zarr","text":"<p>Save the dataset as an OME-Zarr file using <code>to_ome_zarr</code>:</p> <pre><code>znimg.to_ome_zarr(\"output_dataset.ome.zarr\")\n</code></pre> <p>You can also save additional metadata during the process:</p> <pre><code>znimg.to_ome_zarr(\n    \"output_dataset.ome.zarr\",\n    max_layer=3,\n    scaling_method=\"local_mean\"\n)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#summary","title":"Summary","text":"<p>This guide covered the essential operations you can perform with ZarrNii: - Reading datasets from OME-Zarr and NIfTI formats. - Transforming datasets through cropping, downsampling, upsampling, and affine transformations. - Saving datasets back to either format.</p> <p>Next, explore Advanced Use Cases or dive into the API Reference for detailed technical documentation.</p>"},{"location":"walkthrough/getting_started/","title":"Getting Started","text":"<p>This guide helps you set up ZarrNii and get started with its basic functionality. By the end of this guide, you'll be able to read OME-Zarr and NIfTI datasets, perform basic transformations, and save your results.</p>"},{"location":"walkthrough/getting_started/#installation","title":"Installation","text":"<p>ZarrNii requires Python 3.11 or later. Install it using uv, a modern, fast Python package installer and project manager.</p>"},{"location":"walkthrough/getting_started/#1-clone-the-repository","title":"1. Clone the Repository","text":"<p>If you're using the source code, clone the ZarrNii repository:</p> <pre><code>git clone https://github.com/khanlab/zarrnii.git\ncd zarrnii\n</code></pre>"},{"location":"walkthrough/getting_started/#2-install-with-uv","title":"2. Install with uv","text":"<p>Run the following command to install the library and its dependencies:</p> <pre><code>uv sync --dev\n</code></pre> <p>If you don't use uv, install ZarrNii and its dependencies using <code>pip</code>:</p> <pre><code>pip install zarrnii\n</code></pre>"},{"location":"walkthrough/getting_started/#prerequisites","title":"Prerequisites","text":"<p>Before using ZarrNii, ensure you have: - OME-Zarr datasets: Multidimensional images in Zarr format. - NIfTI datasets: Neuroimaging data in <code>.nii</code> or <code>.nii.gz</code> format.</p>"},{"location":"walkthrough/getting_started/#basic-usage","title":"Basic Usage","text":""},{"location":"walkthrough/getting_started/#1-reading-data","title":"1. Reading Data","text":"<p>You can load an OME-Zarr or NIfTI dataset into a <code>ZarrNii</code> object.</p>"},{"location":"walkthrough/getting_started/#from-ome-zarr","title":"From OME-Zarr:","text":"<pre><code>from zarrnii import ZarrNii\n\n# Load OME-Zarr\nznimg = ZarrNii.from_ome_zarr(\"path/to/dataset.ome.zarr\")\n\nprint(\"Data shape:\", znimg.darr.shape)\nprint(\"Affine matrix:\\n\", znimg.affine.matrix)\n\n# Load specific timepoints and channels from 5D data\nznimg_5d = ZarrNii.from_ome_zarr(\"timeseries.zarr\", timepoints=[0, 2], channels=[1])\nprint(\"5D subset shape:\", znimg_5d.darr.shape)\n</code></pre>"},{"location":"walkthrough/getting_started/#from-nifti","title":"From NIfTI:","text":"<pre><code># Load NIfTI (supports 3D, 4D, and 5D data)\nznimg = ZarrNii.from_nifti(\"path/to/dataset.nii\")\n\nprint(\"Data shape:\", znimg.darr.shape)\nprint(\"Affine matrix:\\n\", znimg.affine.matrix)\n</code></pre>"},{"location":"walkthrough/getting_started/#2-working-with-5d-data","title":"2. Working with 5D Data","text":"<p>ZarrNii supports 5D images with time and channel dimensions (T,C,Z,Y,X format). You can select specific timepoints and channels either during loading or after loading.</p>"},{"location":"walkthrough/getting_started/#loading-with-selection","title":"Loading with Selection:","text":"<pre><code># Load specific timepoints\nznimg_time = ZarrNii.from_ome_zarr(\"timeseries.zarr\", timepoints=[0, 2, 4])\n\n# Load specific channels  \nznimg_channels = ZarrNii.from_ome_zarr(\"multichannel.zarr\", channels=[0, 2])\n\n# Load specific channels by label\nznimg_labels = ZarrNii.from_ome_zarr(\"labeled.zarr\", channel_labels=[\"DAPI\", \"GFP\"])\n\n# Combine timepoint and channel selection\nznimg_subset = ZarrNii.from_ome_zarr(\"data.zarr\", timepoints=[1, 3], channels=[0])\n</code></pre>"},{"location":"walkthrough/getting_started/#post-loading-selection","title":"Post-loading Selection:","text":"<pre><code># Load full dataset first\nznimg = ZarrNii.from_ome_zarr(\"timeseries.zarr\")\n\n# Select timepoints after loading\nselected_time = znimg.select_timepoints([0, 2])\n\n# Select channels after loading\nselected_channels = znimg.select_channels([1, 2])\n\n# Chain selections\nsubset = znimg.select_timepoints([0, 1]).select_channels([0])\n</code></pre>"},{"location":"walkthrough/getting_started/#3-performing-transformations","title":"3. Performing Transformations","text":"<p>ZarrNii supports various transformations, such as cropping, downsampling, and upsampling. When working with 5D data, spatial transformations preserve the time and channel dimensions.</p>"},{"location":"walkthrough/getting_started/#cropping","title":"Cropping:","text":"<p>Crop a region from the dataset using voxel coordinates:</p> <pre><code>cropped = znimg.crop((10, 10, 10), (50, 50, 50))\nprint(\"Cropped shape:\", cropped.darr.shape)\n</code></pre>"},{"location":"walkthrough/getting_started/#downsampling","title":"Downsampling:","text":"<p>Reduce the resolution of your dataset:</p> <pre><code>downsampled = znimg.downsample(level=2)\nprint(\"Downsampled shape:\", downsampled.darr.shape)\n\n# For 5D data: (3, 2, 16, 32, 32) -&gt; (3, 2, 8, 16, 16)\n# Time and channel dimensions are preserved\n</code></pre>"},{"location":"walkthrough/getting_started/#upsampling","title":"Upsampling:","text":"<p>Increase the resolution of your dataset:</p> <pre><code>upsampled = znimg.upsample(along_x=2, along_y=2, along_z=2)\nprint(\"Upsampled shape:\", upsampled.darr.shape)\n</code></pre>"},{"location":"walkthrough/getting_started/#4-saving-data","title":"4. Saving Data","text":"<p>ZarrNii makes it easy to save your datasets in both OME-Zarr and NIfTI formats.</p>"},{"location":"walkthrough/getting_started/#to-nifti","title":"To NIfTI:","text":"<p>Save the dataset as a <code>.nii</code> file:</p> <pre><code>znimg.to_nifti(\"output_dataset.nii\")\n</code></pre>"},{"location":"walkthrough/getting_started/#to-ome-zarr","title":"To OME-Zarr:","text":"<p>Save the dataset back to OME-Zarr format:</p> <pre><code>znimg.to_ome_zarr(\"output_dataset.ome.zarr\")\n</code></pre>"},{"location":"walkthrough/getting_started/#example-workflow","title":"Example Workflow","text":"<p>Here\u2019s a full workflow from loading an OME-Zarr dataset to saving a downsampled version as NIfTI:</p> <pre><code>from zarrnii import ZarrNii\n\n# Load a 5D OME-Zarr dataset with specific timepoints and channels\nznimg = ZarrNii.from_ome_zarr(\"path/to/timeseries.zarr\", \n                              timepoints=[0, 2, 4], \n                              channels=[0, 1])\n\n# Perform transformations\ncropped = znimg.crop((10, 10, 10), (100, 100, 100))\ndownsampled = cropped.downsample(level=2)\n\n# Save the result as a NIfTI file\ndownsampled.to_nifti(\"processed_timeseries.nii\")\n\n# Or save as OME-Zarr with metadata preservation\ndownsampled.to_ome_zarr(\"processed_timeseries.ome.zarr\")\n</code></pre>"},{"location":"walkthrough/getting_started/#whats-next","title":"What\u2019s Next?","text":"<ul> <li>Walkthrough: Basic Tasks: Learn more about common workflows like cropping, interpolation, and combining transformations.</li> <li>API Reference: Explore the detailed API for ZarrNii.</li> </ul>"},{"location":"walkthrough/overview/","title":"Walkthrough: Overview","text":"<p>This page provides an overview of the core concepts behind ZarrNii. It\u2019s the starting point for understanding how to work with OME-Zarr, NIfTI, and ZarrNii\u2019s transformation tools.</p>"},{"location":"walkthrough/overview/#core-concepts","title":"Core Concepts","text":""},{"location":"walkthrough/overview/#1-zarr-and-ome-zarr","title":"1. Zarr and OME-Zarr","text":"<ul> <li>Zarr is a format for chunked, compressed N-dimensional arrays.</li> <li>OME-Zarr extends Zarr with metadata for multidimensional microscopy images, supporting axes definitions and multiscale pyramids.</li> </ul>"},{"location":"walkthrough/overview/#key-features-of-ome-zarr","title":"Key Features of OME-Zarr:","text":"<ul> <li>Axes Metadata: Defines spatial dimensions (e.g., <code>x</code>, <code>y</code>, <code>z</code>).</li> <li>Multiscale Pyramids: Stores image resolutions at multiple scales.</li> <li>Annotations: Includes OME metadata for visualization and analysis.</li> </ul>"},{"location":"walkthrough/overview/#2-nifti","title":"2. NIfTI","text":"<ul> <li>NIfTI is a neuroimaging file format, commonly used for MRI and fMRI data.</li> <li>It supports spatial metadata, such as voxel sizes and affine transformations, for anatomical alignment.</li> </ul>"},{"location":"walkthrough/overview/#3-zarrnii","title":"3. ZarrNii","text":"<ul> <li>ZarrNii provides tools to bridge these formats while preserving spatial metadata and enabling transformations.</li> </ul>"},{"location":"walkthrough/overview/#main-features","title":"Main Features:","text":"<ul> <li>Read and write OME-Zarr and NIfTI formats.</li> <li>Apply transformations like cropping, downsampling, and interpolation.</li> <li>Convert between ZYX (OME-Zarr) and XYZ (NIfTI) axes orders.</li> </ul>"},{"location":"walkthrough/overview/#data-model","title":"Data Model","text":"<p>ZarrNii wraps datasets using the <code>ZarrNii</code> class, which has the following attributes:</p> <ul> <li><code>darr</code>: The dask array containing image data.</li> <li><code>affine</code>: An affine transformation matrix for spatial alignment.</li> <li><code>axes_order</code>: Specifies the data layout (<code>ZYX</code> or <code>XYZ</code>).</li> <li>OME-Zarr Metadata:</li> <li><code>axes</code>: Defines the dimensions and units.</li> <li><code>coordinate_transformations</code>: Lists scaling and translation transformations.</li> <li><code>omero</code>: Contains channel and visualization metadata.</li> </ul>"},{"location":"walkthrough/overview/#example-workflow","title":"Example Workflow","text":"<p>Here\u2019s a high-level example workflow using ZarrNii:</p> <ol> <li> <p>Read Data:    <code>python    from zarrnii import ZarrNii    znimg = ZarrNii.from_ome_zarr(\"path/to/dataset.ome.zarr\")</code></p> </li> <li> <p>Apply Transformations:    <code>python    znimg_downsampled = znimg.downsample(level=2)    znimg_cropped = znimg_downsampled.crop((0, 0, 0), (100, 100, 100))</code></p> </li> <li> <p>Convert Formats:    <code>python    znimg_cropped.to_nifti(\"output.nii\")</code></p> </li> </ol>"},{"location":"walkthrough/overview/#whats-next","title":"What\u2019s Next?","text":"<ul> <li>Getting Started: Step-by-step guide to installing and using ZarrNii.</li> <li>Basic Tasks: Learn how to read, write, and transform data.</li> </ul>"}]}