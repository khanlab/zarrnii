{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to the documentation for ZarrNii, a Python library for working with OME-Zarr, NIfTI, and Imaris formats. ZarrNii bridges the gap between these popular formats, enabling seamless data transformation, metadata preservation, and efficient processing of large biomedical images.</p>"},{"location":"#what-is-zarrnii","title":"What is ZarrNii?","text":"<p>ZarrNii is designed for researchers and engineers working with:</p> <ul> <li>OME-Zarr: A format for storing multidimensional image data, commonly used in microscopy.</li> <li>NIfTI: A standard format for neuroimaging data.</li> <li>Imaris: A microscopy file format (.ims) using HDF5 structure for 3D/4D image analysis.</li> </ul> <p>ZarrNii allows you to:</p> <ul> <li>Read and write OME-Zarr, NIfTI, and Imaris datasets.</li> <li>Work with 4D and 5D images, including time-series data (T,C,Z,Y,X).</li> <li>Perform transformations like cropping, downsampling, and interpolation.</li> <li>Select specific channels and timepoints from multidimensional datasets.</li> <li>Preserve and manipulate metadata from OME-Zarr (e.g., axes, coordinate transformations, OME annotations).</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Seamless Format Conversion: Easily convert between OME-Zarr, NIfTI, and Imaris while preserving spatial metadata.</li> <li>5D Image Support: Work with time-series data in (T,C,Z,Y,X) format with timepoint and channel selection.</li> <li>Transformations: Apply common operations like affine transformations, downsampling, and upsampling.</li> <li>Multiscale Support: Work with multiscale OME-Zarr pyramids.</li> <li>Metadata Handling: Access and modify OME-Zarr metadata like axes and transformations.</li> <li>Lazy Loading: Leverage Dask arrays for efficient processing of large datasets.</li> <li>Segmentation Plugins: Extensible plugin architecture for image segmentation algorithms.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from zarrnii import ZarrNii\n\n# Load an OME-Zarr dataset\nznimg = ZarrNii.from_ome_zarr(\"path/to/zarr_dataset.ome.zarr\")\n\n# Or load from Imaris (requires optional dependency)\n# znimg = ZarrNii.from_imaris(\"path/to/microscopy_data.ims\")\n\n# Load with specific timepoints and channels (5D support)\nznimg_subset = ZarrNii.from_ome_zarr(\"timeseries.zarr\", timepoints=[0, 2], channels=[1])\n\n# Perform a transformation (e.g., downsample)\ndownsampled_znimg = znimg.downsample(level=2)\n\n# Apply segmentation using Otsu thresholding\nsegmented_znimg = znimg.segment_otsu(nbins=256)\n\n# Save as NIfTI\ndownsampled_znimg.to_nifti(\"output_dataset.nii\")\nsegmented_znimg.to_nifti(\"segmented_dataset.nii\")\n</code></pre>"},{"location":"#learn-more","title":"Learn More","text":"<p>Explore the documentation to get started:</p> <ul> <li>Walkthrough: Overview: Understand the core concepts.</li> <li>API Reference: Dive into the technical details.</li> <li>Examples: Learn through practical examples.</li> <li>Segmentation Plugin Examples: Learn how to use and create segmentation plugins.</li> <li>FAQ: Find answers to common questions.</li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to ZarrNii will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Near-isotropic downsampling: New <code>downsample_near_isotropic</code> parameter in <code>from_ome_zarr()</code> automatically downsamples dimensions with higher resolution to create more isotropic voxels</li> <li>Comprehensive documentation with examples and API reference</li> <li>Multi-resolution OME-Zarr support with pyramid creation</li> <li>Enhanced transformation pipeline with composite operations</li> <li>Memory-efficient processing with Dask integration</li> <li>Support for multi-channel and time-series data</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Migration from Poetry to uv: Faster dependency management and builds with modern Python packaging</li> <li>Automated versioning: Version now derived from git tags using setuptools-scm</li> <li>Enhanced CI/CD: Updated workflows with trusted publishing to PyPI</li> <li>Improved performance for large dataset operations</li> <li>Enhanced metadata preservation across format conversions</li> <li>Optimized chunk sizing for better I/O performance</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>NumPy 2.0 Compatibility: Fixed deprecated np.product usage</li> <li>Documentation build issues with missing files</li> <li>Improved error handling for malformed input files</li> <li>Better memory management for large transformations</li> </ul>"},{"location":"changelog/#010-initial-development","title":"[0.1.0] - Initial Development","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Core ZarrNii class for unified OME-Zarr and NIfTI handling</li> <li>Affine and displacement transformation support</li> <li>Basic downsampling and upsampling operations</li> <li>Format conversion between OME-Zarr and NIfTI</li> <li>Spatial coordinate system management</li> <li>Integration with nibabel and zarr libraries</li> </ul>"},{"location":"changelog/#features","title":"Features","text":"<ul> <li>Lazy loading with Dask arrays</li> <li>Metadata preservation during transformations</li> <li>Multi-scale image pyramid support</li> <li>Flexible resampling and interpolation methods</li> <li>Comprehensive test suite</li> </ul>"},{"location":"changelog/#documentation","title":"Documentation","text":"<ul> <li>Getting started guide</li> <li>API reference</li> <li>Example workflows</li> <li>Installation instructions</li> </ul>"},{"location":"changelog/#development-notes","title":"Development Notes","text":"<p>This project is under active development. The API may change between versions as we refine the interface based on user feedback and use cases.</p>"},{"location":"changelog/#contributing","title":"Contributing","text":"<p>See Contributing for information about contributing to ZarrNii development.</p>"},{"location":"cli/","title":"Command Line Interface (CLI)","text":"<p>ZarrNii provides convenient command-line tools for converting between OME-Zarr and NIfTI formats. These console scripts are simple wrappers around the main ZarrNii API, making it easy to perform conversions without writing Python code.</p>"},{"location":"cli/#installation","title":"Installation","text":"<p>The CLI scripts are automatically installed when you install ZarrNii:</p> <pre><code>pip install zarrnii\n</code></pre> <p>After installation, you'll have access to two console commands: - <code>z2n</code> - Convert OME-Zarr to NIfTI or TIFF stack format - <code>n2z</code> - Convert NIfTI to OME-Zarr</p>"},{"location":"cli/#z2n-ome-zarr-to-niftitiff-conversion","title":"z2n: OME-Zarr to NIfTI/TIFF Conversion","text":"<p>The <code>z2n</code> script converts OME-Zarr datasets to NIfTI format or TIFF stack format. TIFF stack export is particularly useful for compatibility with napari plugins like cellseg3d that don't support OME-Zarr multiscale data.</p>"},{"location":"cli/#basic-usage","title":"Basic Usage","text":"<pre><code># NIfTI output\nz2n input.ome.zarr output.nii.gz\n\n# TIFF stack output (auto-detected by pattern)\nz2n input.ome.zarr output_z{z:04d}.tif\n</code></pre>"},{"location":"cli/#output-format-selection","title":"Output Format Selection","text":"<p>The output format is automatically detected based on the output filename: - <code>.nii</code> or <code>.nii.gz</code> extensions \u2192 NIfTI format - <code>.tif</code> or <code>.tiff</code> extensions with <code>{z}</code> pattern \u2192 TIFF stack format - Other extensions default to NIfTI format</p> <p>TIFF Stack Format: Each Z-slice is saved as a separate 2D TIFF file. This format is useful for: - Napari plugins that don't support OME-Zarr (e.g., cellseg3d) - Tools that expect individual image files - Visual inspection of individual slices</p> <p>\u26a0\ufe0f Performance Note: TIFF stack export loads all data into memory. Consider cropping or downsampling large datasets first.</p>"},{"location":"cli/#options","title":"Options","text":"<pre><code>z2n --help\n</code></pre> <pre><code>usage: z2n [-h] [--level LEVEL] [--channels CHANNELS] \n           [--channel-labels CHANNEL_LABELS [CHANNEL_LABELS ...]]\n           [--timepoints TIMEPOINTS] [--axes-order {ZYX,XYZ}] \n           [--orientation ORIENTATION] [--downsample-near-isotropic] \n           [--chunks CHUNKS] [--rechunk]\n           input output\n\nConvert OME-Zarr to NIfTI format\n\npositional arguments:\n  input                 Input OME-Zarr path or store\n  output                Output NIfTI file (.nii or .nii.gz)\n\noptions:\n  -h, --help            show this help message and exit\n  --level LEVEL         Pyramid level to load (0 = highest resolution) (default: 0)\n  --channels CHANNELS   Comma-separated channel indices to load (e.g., '0,2,3')\n  --channel-labels CHANNEL_LABELS [CHANNEL_LABELS ...]\n                        Channel names to load by label\n  --timepoints TIMEPOINTS\n                        Comma-separated timepoint indices to load (e.g., '0,1,2')\n  --axes-order {ZYX,XYZ}\n                        Spatial axes order for processing (default: ZYX)\n  --orientation ORIENTATION\n                        Anatomical orientation string (default: RAS)\n  --downsample-near-isotropic\n                        Apply near-isotropic downsampling\n  --chunks CHUNKS       Chunk specification for dask arrays (default: auto)\n  --rechunk             Rechunk data arrays\n</code></pre>"},{"location":"cli/#examples","title":"Examples","text":"<pre><code># Basic conversion\nz2n input.ome.zarr output.nii.gz\n\n# Convert from ZIP store with specific pyramid level\nz2n input.ome.zarr.zip output.nii.gz --level 1\n\n# Select specific channels and reorder axes\nz2n input.ome.zarr output.nii.gz --channels 0,2 --axes-order ZYX\n\n# Change orientation and apply near-isotropic downsampling\nz2n input.ome.zarr output.nii.gz --orientation LPI --downsample-near-isotropic\n\n# TIFF Stack Export\n# Save each Z-slice as a separate TIFF file for compatibility with napari plugins\nz2n input.ome.zarr output_z{z:04d}.tif --tiff-channel 0\n\n# Export to TIFF stack with custom pattern and settings\nz2n input.ome.zarr slices/brain_{z:03d}.tiff --tiff-timepoint 0 --tiff-no-compress\n\n# Export multi-channel data as multi-channel TIFFs (no channel selection)\nz2n input.ome.zarr multichannel_z{z:04d}.tif\n\n# Load specific timepoints\nz2n input.ome.zarr output.nii.gz --timepoints 0,5,10\n\n# Use channel labels instead of indices\nz2n input.ome.zarr output.nii.gz --channel-labels DAPI GFP\n</code></pre>"},{"location":"cli/#n2z-nifti-to-ome-zarr-conversion","title":"n2z: NIfTI to OME-Zarr Conversion","text":"<p>The <code>n2z</code> script converts NIfTI files to OME-Zarr format with multiscale pyramid generation. It's a wrapper around <code>ZarrNii.from_nifti().to_ome_zarr()</code>.</p>"},{"location":"cli/#basic-usage_1","title":"Basic Usage","text":"<pre><code>n2z input.nii.gz output.ome.zarr\n</code></pre>"},{"location":"cli/#options_1","title":"Options","text":"<pre><code>n2z --help\n</code></pre> <pre><code>usage: n2z [-h] [--chunks CHUNKS] [--axes-order {XYZ,ZYX}] [--name NAME] \n           [--as-ref] [--zooms ZOOMS] [--max-layer MAX_LAYER] \n           [--scale-factors SCALE_FACTORS]\n           input output\n\nConvert NIfTI to OME-Zarr format\n\npositional arguments:\n  input                 Input NIfTI file (.nii or .nii.gz)\n  output                Output OME-Zarr path or store\n\noptions:\n  -h, --help            show this help message and exit\n  --chunks CHUNKS       Chunk specification for dask arrays (default: auto)\n  --axes-order {XYZ,ZYX}\n                        Spatial axes order for processing (default: XYZ)\n  --name NAME           Name for the dataset\n  --as-ref              Create as reference without loading data\n  --zooms ZOOMS         Custom voxel sizes as comma-separated floats (e.g., '2.0,2.0,2.0')\n  --max-layer MAX_LAYER\n                        Maximum number of pyramid levels (default: 4)\n  --scale-factors SCALE_FACTORS\n                        Custom scale factors as comma-separated integers (e.g., '2,4,8')\n</code></pre>"},{"location":"cli/#examples_1","title":"Examples","text":"<pre><code># Basic conversion\nn2z input.nii.gz output.ome.zarr\n\n# Create compressed ZIP store with more pyramid levels\nn2z input.nii.gz output.ome.zarr.zip --max-layer 6\n\n# Use custom voxel sizes and create as reference\nn2z input.nii.gz output.ome.zarr --as-ref --zooms 1.5,1.5,2.5\n\n# Specify custom scale factors for pyramid\nn2z input.nii.gz output.ome.zarr --scale-factors 2,4,8,16\n\n# Change axes order and add dataset name\nn2z input.nii.gz output.ome.zarr --axes-order ZYX --name \"brain_scan\"\n\n# Custom chunking specification\nn2z input.nii.gz output.ome.zarr --chunks 64,64,32\n</code></pre>"},{"location":"cli/#common-workflows","title":"Common Workflows","text":""},{"location":"cli/#round-trip-conversion","title":"Round-trip Conversion","text":"<p>Convert from NIfTI to OME-Zarr and back to verify data integrity:</p> <pre><code># Original NIfTI to OME-Zarr\nn2z original.nii.gz intermediate.ome.zarr\n\n# OME-Zarr back to NIfTI\nz2n intermediate.ome.zarr final.nii.gz\n</code></pre>"},{"location":"cli/#working-with-zip-stores","title":"Working with ZIP Stores","text":"<p>OME-Zarr supports compressed ZIP format for efficient storage and sharing:</p> <pre><code># Create compressed OME-Zarr\nn2z input.nii.gz output.ome.zarr.zip\n\n# Convert from compressed OME-Zarr\nz2n input.ome.zarr.zip output.nii.gz\n</code></pre>"},{"location":"cli/#multiscale-processing","title":"Multiscale Processing","text":"<p>Create OME-Zarr with multiple resolution levels for efficient visualization:</p> <pre><code># Create 6 pyramid levels\nn2z input.nii.gz output.ome.zarr --max-layer 6\n\n# Extract a lower resolution level\nz2n output.ome.zarr downsampled.nii.gz --level 2\n</code></pre>"},{"location":"cli/#channel-and-timepoint-selection","title":"Channel and Timepoint Selection","text":"<p>For multi-dimensional datasets:</p> <pre><code># Select specific channels during conversion\nz2n multichannel.ome.zarr selected_channels.nii.gz --channels 0,2,4\n\n# Select specific timepoints\nz2n timeseries.ome.zarr timepoint_subset.nii.gz --timepoints 0,10,20\n</code></pre>"},{"location":"cli/#error-handling","title":"Error Handling","text":"<p>The CLI scripts provide informative error messages for common issues:</p> <ul> <li>File not found: Clear error message if input file doesn't exist</li> <li>Invalid arguments: Helpful guidance for malformed arguments</li> <li>Conversion errors: Detailed error information for debugging</li> </ul> <p>Example error handling:</p> <pre><code># Missing input file\n$ z2n missing.ome.zarr output.nii.gz\nError: Input path 'missing.ome.zarr' does not exist\n\n# Invalid zoom specification\n$ n2z input.nii.gz output.ome.zarr --zooms 1.0,2.0\nError during conversion: Expected exactly 3 comma-separated floats\n</code></pre>"},{"location":"cli/#integration-with-python-api","title":"Integration with Python API","text":"<p>The CLI scripts use the same underlying API as the Python interface, so you can easily switch between command-line and programmatic usage:</p> <pre><code># Equivalent Python code for: n2z input.nii.gz output.ome.zarr\nfrom zarrnii import ZarrNii\n\nznimg = ZarrNii.from_nifti(\"input.nii.gz\")\nznimg.to_ome_zarr(\"output.ome.zarr\")\n</code></pre> <pre><code># Equivalent Python code for: z2n input.ome.zarr output.nii.gz --level 1\nfrom zarrnii import ZarrNii\n\nznimg = ZarrNii.from_ome_zarr(\"input.ome.zarr\", level=1)\nznimg.to_nifti(\"output.nii.gz\")\n</code></pre> <p>This consistency makes it easy to prototype with the CLI and then integrate the same functionality into your Python workflows.</p>"},{"location":"contributing/","title":"Contributing to ZarrNii","text":"<p>Thank you for your interest in contributing to ZarrNii! This document provides guidelines for contributing to the project.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11 or higher</li> <li>Git</li> <li>uv package manager (recommended) or pip</li> </ul>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Clone the repository:    <code>bash    git clone https://github.com/khanlab/zarrnii.git    cd zarrnii</code></p> </li> <li> <p>Install dependencies:    ```bash    # Using uv (recommended)    uv sync --dev</p> </li> </ol> <p># Or using pip    pip install -e \".[dev]\"    ```</p> <ol> <li>Set up pre-commit hooks:    <code>bash    uv run pre-commit install</code></li> </ol>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#code-style-and-quality","title":"Code Style and Quality","text":"<p>ZarrNii follows Python best practices and uses several tools to maintain code quality:</p> <ul> <li>Black: Code formatting (line-length: 88)</li> <li>isort: Import sorting (profile: black, line-length: 88)  </li> <li>flake8: Linting and style checking (max-line-length: 88, extend-ignore: E203,W503)</li> <li>pre-commit: Automated checks before commits</li> </ul> <p>Run quality checks:</p> <pre><code># Format code  \nuv run black .\n\n# Sort imports\nuv run isort .\n\n# Lint code\nuv run flake8 .\n\n# Run all quality checks (matches CI exactly)\n./scripts/quality-check.sh\n\n# Or use justfile for convenience (if just is installed)\njust format\njust lint  \njust quality\n</code></pre> <p>Important: All tools are configured with consistent settings: - Line length: 88 characters (matches Black's default) - Import profile: black (ensures compatibility) - Flake8 ignores: E203, W503 (compatibility with Black)</p>"},{"location":"contributing/#testing","title":"Testing","text":"<p>ZarrNii uses pytest for testing. Tests are located in the <code>tests/</code> directory.</p> <pre><code># Run all tests\nuv run pytest -v\n\n# Run with coverage\nuv run pytest --cov=zarrnii\n\n# Run with detailed coverage report\nuv run pytest --cov=zarrnii --cov-report=term-missing\n\n# Generate HTML coverage report\nuv run pytest --cov=zarrnii --cov-report=html\n\n# Run specific test file\nuv run pytest tests/test_io.py\n\n# Or use justfile\njust test\n</code></pre>"},{"location":"contributing/#coverage-requirements","title":"Coverage Requirements","text":"<p>ZarrNii maintains code coverage standards: - Minimum coverage: 70% (CI requirement) - Target coverage: 85%+ - Current coverage: 86%+ - Coverage reports exclude <code>_version.py</code> and test files - New features must include comprehensive tests - Pull requests should not decrease overall coverage significantly</p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>Documentation is built with MkDocs and hosted on GitHub Pages.</p> <pre><code># Serve documentation locally\nuv run mkdocs serve\n\n# Build documentation\nuv run mkdocs build\n\n# Deploy to GitHub Pages (maintainers only)\nuv run mkdocs gh-deploy\n\n# Or use justfile\njust serve-docs\njust build-docs\n</code></pre>"},{"location":"contributing/#contribution-types","title":"Contribution Types","text":""},{"location":"contributing/#bug-reports","title":"Bug Reports","text":"<p>When reporting bugs, please include:</p> <ul> <li>Python version and platform</li> <li>ZarrNii version</li> <li>Minimal code example that reproduces the issue</li> <li>Expected vs. actual behavior</li> <li>Full error traceback if applicable</li> </ul>"},{"location":"contributing/#feature-requests","title":"Feature Requests","text":"<p>For feature requests, please provide:</p> <ul> <li>Clear description of the proposed feature</li> <li>Use case and motivation</li> <li>Potential implementation approach</li> <li>Any relevant literature or references</li> </ul>"},{"location":"contributing/#code-contributions","title":"Code Contributions","text":""},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li> <p>Fork the repository and create a feature branch:    <code>bash    git checkout -b feature/your-feature-name</code></p> </li> <li> <p>Make your changes:</p> </li> <li>Follow existing code style and conventions</li> <li>Add tests for new functionality</li> <li>Update documentation as needed</li> <li> <p>Ensure all tests pass</p> </li> <li> <p>Commit your changes:    <code>bash    git add .    git commit -m \"Add: brief description of your changes\"</code></p> </li> <li> <p>Push and create a Pull Request:    <code>bash    git push origin feature/your-feature-name</code></p> </li> </ol>"},{"location":"contributing/#code-review-guidelines","title":"Code Review Guidelines","text":"<ul> <li>All code must pass CI checks</li> <li>New features require tests and documentation</li> <li>Breaking changes need detailed justification</li> <li>Performance implications should be considered</li> </ul>"},{"location":"contributing/#documentation-contributions","title":"Documentation Contributions","text":"<p>Documentation improvements are always welcome! This includes:</p> <ul> <li>Fixing typos and grammatical errors</li> <li>Adding examples and tutorials</li> <li>Improving API documentation</li> <li>Translating content</li> </ul>"},{"location":"contributing/#development-guidelines","title":"Development Guidelines","text":""},{"location":"contributing/#code-organization","title":"Code Organization","text":"<ul> <li>Core functionality: <code>zarrnii/core.py</code></li> <li>Transformations: <code>zarrnii/transform.py</code> </li> <li>Utilities: <code>zarrnii/utils.py</code></li> <li>Enumerations: <code>zarrnii/enums.py</code></li> </ul>"},{"location":"contributing/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>Use descriptive variable and function names</li> <li>Follow PEP 8 naming conventions</li> <li>Use type hints where appropriate</li> <li>Document complex algorithms and edge cases</li> </ul>"},{"location":"contributing/#error-handling","title":"Error Handling","text":"<ul> <li>Raise informative exceptions with clear messages</li> <li>Use appropriate exception types</li> <li>Handle common error scenarios gracefully</li> <li>Log warnings for non-fatal issues</li> </ul>"},{"location":"contributing/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Use Dask for lazy evaluation when possible</li> <li>Minimize memory allocation in hot paths</li> <li>Profile performance-critical code</li> <li>Consider memory usage for large datasets</li> </ul>"},{"location":"contributing/#api-design-principles","title":"API Design Principles","text":""},{"location":"contributing/#consistency","title":"Consistency","text":"<ul> <li>Methods should have predictable interfaces</li> <li>Similar operations should use similar naming patterns</li> <li>Return types should be consistent across methods</li> </ul>"},{"location":"contributing/#flexibility","title":"Flexibility","text":"<ul> <li>Support multiple input formats where reasonable</li> <li>Provide sensible defaults for optional parameters</li> <li>Allow customization through optional arguments</li> </ul>"},{"location":"contributing/#usability","title":"Usability","text":"<ul> <li>Prioritize common use cases in the main API</li> <li>Provide clear error messages</li> <li>Include comprehensive docstrings</li> </ul>"},{"location":"contributing/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"contributing/#test-categories","title":"Test Categories","text":"<ul> <li>Unit tests: Test individual functions and methods</li> <li>Integration tests: Test component interactions</li> <li>End-to-end tests: Test complete workflows</li> <li>Performance tests: Benchmark critical operations</li> </ul>"},{"location":"contributing/#test-data","title":"Test Data","text":"<ul> <li>Use synthetic data when possible</li> <li>Keep test files small</li> <li>Document data requirements clearly</li> <li>Provide utilities for generating test data</li> </ul>"},{"location":"contributing/#test-structure","title":"Test Structure","text":"<pre><code>def test_feature_description():\n    \"\"\"Test that feature works correctly under normal conditions.\"\"\"\n    # Arrange\n    input_data = create_test_data()\n\n    # Act\n    result = function_under_test(input_data)\n\n    # Assert\n    assert result.shape == expected_shape\n    assert np.allclose(result.data, expected_data)\n</code></pre>"},{"location":"contributing/#release-process","title":"Release Process","text":""},{"location":"contributing/#version-numbering","title":"Version Numbering","text":"<p>ZarrNii uses Semantic Versioning: - MAJOR: Incompatible API changes - MINOR: New functionality, backwards compatible - PATCH: Bug fixes, backwards compatible</p>"},{"location":"contributing/#release-checklist","title":"Release Checklist","text":"<ol> <li>Ensure all tests pass and CI is green</li> <li>Update changelog with new features and fixes</li> <li>Build and test documentation</li> <li>Create and push a git tag:    <code>bash    git tag v1.0.0    git push origin v1.0.0</code></li> <li>GitHub Actions will automatically:</li> <li>Build the package using uv</li> <li>Deploy to PyPI using trusted publishing</li> <li>Update GitHub release notes</li> </ol>"},{"location":"contributing/#community-guidelines","title":"Community Guidelines","text":""},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>We are committed to providing a welcoming and inclusive environment. Please:</p> <ul> <li>Be respectful and considerate</li> <li>Use inclusive language</li> <li>Focus on constructive feedback</li> <li>Help create a positive community</li> </ul>"},{"location":"contributing/#communication","title":"Communication","text":"<ul> <li>GitHub Issues: Bug reports and feature requests</li> <li>GitHub Discussions: Questions and general discussion</li> <li>Pull Requests: Code contributions and reviews</li> </ul>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<p>If you need help with development:</p> <ol> <li>Check existing documentation and examples</li> <li>Search through GitHub issues</li> <li>Create a new issue with your question</li> <li>Join community discussions</li> </ol>"},{"location":"contributing/#recognition","title":"Recognition","text":"<p>Contributors will be recognized in: - Project README - Release notes - Documentation acknowledgments</p> <p>Thank you for helping make ZarrNii better!</p>"},{"location":"faq/","title":"FAQ: Frequently Asked Questions","text":"<p>This page addresses common questions and provides troubleshooting tips for using ZarrNii.</p>"},{"location":"faq/#general-questions","title":"General Questions","text":""},{"location":"faq/#1-what-is-zarrnii","title":"1. What is ZarrNii?","text":"<p>ZarrNii is a Python library that bridges the gap between OME-Zarr and NIfTI formats, enabling seamless conversion, transformations, and metadata handling for multidimensional biomedical images.</p>"},{"location":"faq/#2-what-formats-does-zarrnii-support","title":"2. What formats does ZarrNii support?","text":"<p>ZarrNii supports: - OME-Zarr: A format for storing chunked, multidimensional microscopy images. - NIfTI: A format commonly used for neuroimaging data.</p>"},{"location":"faq/#3-can-zarrnii-handle-large-datasets","title":"3. Can ZarrNii handle large datasets?","text":"<p>Yes! ZarrNii uses Dask arrays to handle datasets that don't fit into memory. Most transformations are lazy, meaning computations are only performed when explicitly triggered using <code>.compute()</code>.</p>"},{"location":"faq/#installation-issues","title":"Installation Issues","text":""},{"location":"faq/#1-i-installed-zarrnii-but-i-cant-import-it","title":"1. I installed ZarrNii, but I can't import it.","text":"<p>Ensure that ZarrNii is installed in the correct Python environment. Use <code>uv tree</code> or <code>pip show zarrnii</code> to verify the installation.</p> <p>If you're still encountering issues, try reinstalling the library:</p> <pre><code>uv sync --dev\n</code></pre>"},{"location":"faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"faq/#performance-tips","title":"Performance Tips","text":""},{"location":"faq/#1-how-can-i-speed-up-transformations-on-large-datasets","title":"1. How can I speed up transformations on large datasets?","text":"<ul> <li>Use appropriate chunk sizes with <code>.rechunk()</code> for operations like downsampling or interpolation.</li> <li>Trigger computations only when necessary using <code>.compute()</code>.</li> </ul>"},{"location":"faq/#2-how-do-i-optimize-multiscale-processing","title":"2. How do I optimize multiscale processing?","text":"<p>For OME-Zarr datasets with multiscale pyramids: 1. Use the appropriate <code>level</code> when loading the dataset.</p> <pre><code>znimg = ZarrNii.from_ome_zarr(\"path/to/dataset.zarr\", level=2)\n</code></pre>"},{"location":"faq/#metadata-questions","title":"Metadata Questions","text":""},{"location":"faq/#1-how-do-i-access-ome-zarr-metadata","title":"1. How do I access OME-Zarr metadata?","text":"<p>ZarrNii provides attributes for accessing metadata:</p> <pre><code>print(\"Axes:\", znimg.axes)\nprint(\"Coordinate transformations:\", znimg.coordinate_transformations)\nprint(\"Omero metadata:\", znimg.omero)\n</code></pre>"},{"location":"faq/#2-does-zarrnii-preserve-metadata-during-transformations","title":"2. Does ZarrNii preserve metadata during transformations?","text":"<p>Yes, ZarrNii updates the metadata to remain consistent with transformations like cropping, downsampling, or affine transformations.</p>"},{"location":"faq/#getting-help","title":"Getting Help","text":"<p>If you encounter issues not covered here: 1. Check the API Reference for detailed information about ZarrNii methods. 2. Open an issue on the GitHub repository.</p>"},{"location":"faq/#summary","title":"Summary","text":"<p>This FAQ covers common questions about ZarrNii, troubleshooting tips, and best practices for working with large datasets and metadata. For more in-depth information, explore: - Examples - API Reference</p>"},{"location":"reference/","title":"API Reference","text":"<p>Zarr-based image with NIfTI compatibility using NgffImage internally.</p> <p>This class provides chainable operations on OME-Zarr data while maintaining compatibility with NIfTI workflows. It uses NgffImage objects internally for better multiscale support and metadata preservation.</p> <p>Attributes:</p> <ul> <li> <code>ngff_image</code>               (<code>NgffImage</code>)           \u2013            <p>The internal NgffImage object containing data and metadata.</p> </li> <li> <code>axes_order</code>               (<code>str</code>)           \u2013            <p>The order of the axes for NIfTI compatibility ('ZYX' or 'XYZ').</p> </li> <li> <code>xyz_orientation</code>               (<code>str</code>)           \u2013            <p>The anatomical orientation string in XYZ axes order (e.g., 'RAS', 'LPI').</p> </li> </ul> <p>Constructor with backward compatibility for old signature.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If affine parameter is provided</p> </li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def __init__(\n    self,\n    darr=None,\n    axes_order=\"ZYX\",\n    orientation=\"RAS\",\n    xyz_orientation=None,\n    ngff_image=None,\n    spacing: Tuple[float, float, float] = (1.0, 1.0, 1.0),\n    origin: Tuple[float, float, float] = (0.0, 0.0, 0.0),\n    name: str = \"image\",\n    _omero: Optional[object] = None,\n    affine: Optional[AffineTransform] = None,\n    **kwargs,\n):\n    \"\"\"\n    Constructor with backward compatibility for old signature.\n\n    Raises:\n        ValueError: If affine parameter is provided\n    \"\"\"\n    # Check for deprecated affine parameter\n    if affine is not None:\n        raise ValueError(\n            \"The 'affine' parameter is no longer supported in ZarrNii(). \"\n            \"Please use 'spacing' and 'origin' parameters instead. \"\n            \"If you need to specify a full affine transformation, use from_nifti() \"\n            \"or construct the NgffImage directly.\"\n        )\n\n    # Handle backwards compatibility: if xyz_orientation is provided, use it\n    # Otherwise, use orientation for backwards compatibility\n    final_orientation = (\n        xyz_orientation if xyz_orientation is not None else orientation\n    )\n\n    if ngff_image is not None:\n        # New signature\n        object.__setattr__(self, \"ngff_image\", ngff_image)\n        object.__setattr__(self, \"axes_order\", axes_order)\n        object.__setattr__(self, \"xyz_orientation\", final_orientation)\n        object.__setattr__(self, \"_omero\", _omero)\n    elif darr is not None:\n        # Legacy signature - delegate to from_darr\n        instance = self.from_darr(\n            darr=darr,\n            axes_order=axes_order,\n            orientation=final_orientation,\n            spacing=spacing,\n            origin=origin,\n            name=name,\n            omero=_omero,\n            **kwargs,\n        )\n        object.__setattr__(self, \"ngff_image\", instance.ngff_image)\n        object.__setattr__(self, \"axes_order\", instance.axes_order)\n        object.__setattr__(self, \"xyz_orientation\", instance.xyz_orientation)\n        object.__setattr__(self, \"_omero\", instance._omero)\n    else:\n        raise ValueError(\"Must provide either ngff_image or darr\")\n</code></pre> <p>               Bases: <code>ZarrNii</code></p> <p>Brain atlas with segmentation image and region lookup table.</p> <p>Represents a brain atlas consisting of a segmentation image (dseg) that assigns integer labels to brain regions, and a lookup table (tsv) that maps these labels to region names and other metadata.</p> <p>Extension of ZarrNii to support atlas label tables.</p> <p>Inherits all functionality from ZarrNii and adds support for storing region/label metadata in a pandas DataFrame.</p> <p>               Bases: <code>Transform</code></p> <p>Affine transformation for spatial coordinate mapping.</p> <p>Represents a 4x4 affine transformation matrix that can be used to transform 3D coordinates between different coordinate systems. Supports various operations including matrix multiplication, inversion, and point transformation.</p> <p>Attributes:</p> <ul> <li> <code>matrix</code>               (<code>ndarray</code>)           \u2013            <p>4x4 affine transformation matrix</p> </li> </ul> <p>               Bases: <code>Transform</code></p> <p>Non-linear displacement field transformation.</p> <p>Represents a displacement field transformation where each point in space has an associated displacement vector. Uses interpolation to compute displacements for arbitrary coordinates.</p> <p>Attributes:</p> <ul> <li> <code>disp_xyz</code>               (<code>ndarray</code>)           \u2013            <p>Displacement vectors at grid points (4D array: x, y, z, vector_component)</p> </li> <li> <code>disp_grid</code>               (<code>Tuple[ndarray, ...]</code>)           \u2013            <p>Grid coordinates for displacement field</p> </li> <li> <code>disp_affine</code>               (<code>AffineTransform</code>)           \u2013            <p>Affine transformation from world to displacement field coordinates</p> </li> </ul>"},{"location":"reference/#zarrnii.ZarrNii-attributes","title":"Attributes","text":""},{"location":"reference/#zarrnii.ZarrNii.data","title":"<code>zarrnii.ZarrNii.data</code>  <code>property</code> <code>writable</code>","text":"<p>Access the image data (dask array).</p>"},{"location":"reference/#zarrnii.ZarrNii.darr","title":"<code>zarrnii.ZarrNii.darr</code>  <code>property</code> <code>writable</code>","text":"<p>Legacy property name for image data.</p>"},{"location":"reference/#zarrnii.ZarrNii.shape","title":"<code>zarrnii.ZarrNii.shape</code>  <code>property</code>","text":"<p>Shape of the image data.</p>"},{"location":"reference/#zarrnii.ZarrNii.dims","title":"<code>zarrnii.ZarrNii.dims</code>  <code>property</code>","text":"<p>Dimension names.</p>"},{"location":"reference/#zarrnii.ZarrNii.scale","title":"<code>zarrnii.ZarrNii.scale</code>  <code>property</code>","text":"<p>Scale information from NgffImage.</p>"},{"location":"reference/#zarrnii.ZarrNii.translation","title":"<code>zarrnii.ZarrNii.translation</code>  <code>property</code>","text":"<p>Translation information from NgffImage.</p>"},{"location":"reference/#zarrnii.ZarrNii.name","title":"<code>zarrnii.ZarrNii.name</code>  <code>property</code>","text":"<p>Image name from NgffImage.</p>"},{"location":"reference/#zarrnii.ZarrNii.orientation","title":"<code>zarrnii.ZarrNii.orientation</code>  <code>property</code> <code>writable</code>","text":"<p>Legacy property for backward compatibility.</p> <p>Returns the xyz_orientation attribute to maintain backward compatibility with code that expects the 'orientation' property.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The anatomical orientation string in XYZ axes order</p> </li> </ul>"},{"location":"reference/#zarrnii.ZarrNii.affine","title":"<code>zarrnii.ZarrNii.affine</code>  <code>property</code>","text":"<p>Affine transformation matrix derived from NgffImage scale and translation.</p> <p>Returns:</p> <ul> <li> <code>AffineTransform</code> (              <code>AffineTransform</code> )          \u2013            <p>4x4 affine transformation matrix in axes order of self.</p> </li> </ul>"},{"location":"reference/#zarrnii.ZarrNii.axes","title":"<code>zarrnii.ZarrNii.axes</code>  <code>property</code>","text":"<p>Axes metadata - derived from NgffImage for compatibility.</p>"},{"location":"reference/#zarrnii.ZarrNii.coordinate_transformations","title":"<code>zarrnii.ZarrNii.coordinate_transformations</code>  <code>property</code>","text":"<p>Coordinate transformations - derived from NgffImage scale/translation.</p>"},{"location":"reference/#zarrnii.ZarrNii.omero","title":"<code>zarrnii.ZarrNii.omero</code>  <code>property</code>","text":"<p>Omero metadata object.</p>"},{"location":"reference/#zarrnii.ZarrNii-functions","title":"Functions","text":""},{"location":"reference/#zarrnii.ZarrNii.get_affine_transform","title":"<code>zarrnii.ZarrNii.get_affine_transform(axes_order=None)</code>","text":"<p>Get AffineTransform object from NgffImage metadata.</p> <p>Parameters:</p> <ul> <li> <code>axes_order</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Spatial axes order, defaults to self.axes_order</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AffineTransform</code>           \u2013            <p>AffineTransform object</p> </li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def get_affine_transform(self, axes_order: str = None) -&gt; AffineTransform:\n    \"\"\"\n    Get AffineTransform object from NgffImage metadata.\n\n    Args:\n        axes_order: Spatial axes order, defaults to self.axes_order\n\n    Returns:\n        AffineTransform object\n    \"\"\"\n    matrix = self.get_affine_matrix(axes_order)\n    return AffineTransform.from_array(matrix)\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.get_zarr_store_info","title":"<code>zarrnii.ZarrNii.get_zarr_store_info()</code>","text":"<p>Extract zarr store information from the dask array if available.</p> <p>Attempts to extract the underlying zarr store path and metadata from the dask array graph. This information can be used for direct zarr access without triggering dask compute() operations.</p> <p>Returns:</p> <ul> <li> <code>Optional[Dict[str, Any]]</code>           \u2013            <p>Dictionary containing store information if available: - 'store_path': Path or URI to the zarr store - 'dataset_path': Path to the dataset within the zarr group - 'array_shape': Shape of the full array</p> </li> <li> <code>Optional[Dict[str, Any]]</code>           \u2013            <p>Returns None if the data is not backed by a zarr store.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the dask array shape doesn't match the zarr array shape, indicating lazy operations that change shape (e.g., downsampling).</p> </li> </ul> Notes <ul> <li>Only works if the dask array was created from zarr using da.from_zarr()</li> <li>Returns None for in-memory arrays or arrays from other sources</li> <li>Validates that zarr array shape matches dask array shape to ensure   compatibility with direct zarr access</li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def get_zarr_store_info(\n    self,\n) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Extract zarr store information from the dask array if available.\n\n    Attempts to extract the underlying zarr store path and metadata from\n    the dask array graph. This information can be used for direct zarr\n    access without triggering dask compute() operations.\n\n    Returns:\n        Dictionary containing store information if available:\n            - 'store_path': Path or URI to the zarr store\n            - 'dataset_path': Path to the dataset within the zarr group\n            - 'array_shape': Shape of the full array\n        Returns None if the data is not backed by a zarr store.\n\n    Raises:\n        ValueError: If the dask array shape doesn't match the zarr array shape,\n            indicating lazy operations that change shape (e.g., downsampling).\n\n    Notes:\n        - Only works if the dask array was created from zarr using da.from_zarr()\n        - Returns None for in-memory arrays or arrays from other sources\n        - Validates that zarr array shape matches dask array shape to ensure\n          compatibility with direct zarr access\n    \"\"\"\n    try:\n        # Check if the dask array has a graph\n        graph = self.data.__dask_graph__()\n\n        # Look for zarr array in the graph\n        # The first key in a from_zarr graph typically contains the zarr array\n        for key in graph.keys():\n            task = graph[key]\n            # Check if this is a zarr array\n            if hasattr(task, \"store\") and hasattr(task, \"name\"):\n                # Extract store information\n                import zarr\n                import zarr.storage\n\n                store = task.store\n                dataset_path = task.name.strip(\"/\")\n\n                # Determine store path based on store type\n                # Handle both zarr v2 and v3 store types\n                store_path = None\n\n                # Try zarr v3 LocalStore first (has 'root' attribute)\n                if hasattr(store, \"root\"):\n                    store_path = store.root\n                # Try zarr v2 DirectoryStore (has 'path' attribute)\n                elif hasattr(store, \"path\"):\n                    store_path = store.path\n                # Try string representation as fallback\n                elif isinstance(store, str):\n                    store_path = store\n                # Try str(store) which works for LocalStore\n                else:\n                    store_str = str(store)\n                    # LocalStore repr is like \"file:///path/to/store\"\n                    if store_str.startswith(\"file://\"):\n                        store_path = store_str.replace(\"file://\", \"\")\n                    else:\n                        store_path = store_str\n\n                if store_path:\n                    # Validate that the zarr array shape matches the dask array shape\n                    # This ensures no lazy operations have changed the shape\n                    try:\n                        # Convert store_path to string in case it's a Path object\n                        store_path_str = str(store_path)\n\n                        # Open the zarr store to get the actual array shape\n                        if store_path_str.endswith(\".zip\"):\n                            zarr_store = zarr.storage.ZipStore(\n                                store_path_str, mode=\"r\"\n                            )\n                            root = zarr.open_group(zarr_store, mode=\"r\")\n                            zarr_array = root[dataset_path]\n                            zarr_store.close()\n                        else:\n                            root = zarr.open_group(store_path_str, mode=\"r\")\n                            zarr_array = root[dataset_path]\n\n                        zarr_shape = zarr_array.shape\n                        dask_shape = self.shape\n\n                        # Check if spatial dimensions match\n                        # Extract indices of spatial dimensions (x, y, z)\n                        spatial_dims = [\"x\", \"y\", \"z\"]\n                        spatial_indices = [\n                            i\n                            for i, dim in enumerate(self.dims)\n                            if dim.lower() in spatial_dims\n                        ]\n\n                        # Compare only spatial dimensions\n                        zarr_spatial_shape = tuple(\n                            zarr_shape[i] for i in spatial_indices\n                        )\n                        dask_spatial_shape = tuple(\n                            dask_shape[i] for i in spatial_indices\n                        )\n\n                        if zarr_spatial_shape != dask_spatial_shape:\n                            raise ValueError(\n                                f\"Cannot use direct zarr access for apply_transform: \"\n                                f\"the floating image has lazy operations that change its shape. \"\n                                f\"Zarr array shape: {zarr_shape}, but dask array shape: {dask_shape}. \"\n                                f\"Spatial dimensions - Zarr: {zarr_spatial_shape}, Dask: {dask_spatial_shape}. \"\n                                f\"This typically happens when using downsample levels beyond what exists \"\n                                f\"in the zarr store, or when using downsample_near_isotropic option. \"\n                                f\"To fix this, save the floating image to an intermediate zarr file first:\\n\"\n                                f\"  flo_znimg.to_ome_zarr('intermediate.zarr')\\n\"\n                                f\"  flo_znimg = ZarrNii.from_ome_zarr('intermediate.zarr')\\n\"\n                                f\"  transformed = flo_znimg.apply_transform(...)\"\n                            )\n\n                    except (KeyError, FileNotFoundError) as e:\n                        # Dataset doesn't exist at the specified path\n                        raise ValueError(\n                            f\"Cannot use direct zarr access for apply_transform: \"\n                            f\"the specified dataset '{dataset_path}' does not exist in the zarr store \"\n                            f\"at '{store_path}'. This may happen when using a downsample level that \"\n                            f\"doesn't exist in the zarr store. \"\n                            f\"To fix this, save the floating image to an intermediate zarr file first:\\n\"\n                            f\"  flo_znimg.to_ome_zarr('intermediate.zarr')\\n\"\n                            f\"  flo_znimg = ZarrNii.from_ome_zarr('intermediate.zarr')\\n\"\n                            f\"  transformed = flo_znimg.apply_transform(...)\"\n                        ) from e\n\n                    return {\n                        \"store_path\": store_path,\n                        \"dataset_path\": dataset_path,\n                        \"array_shape\": self.shape,\n                    }\n    except ValueError:\n        # Re-raise ValueError (our validation errors)\n        raise\n    except Exception:\n        # If we can't extract store info for other reasons, return None\n        pass\n\n    return None\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.from_ngff_image","title":"<code>zarrnii.ZarrNii.from_ngff_image(ngff_image, axes_order='ZYX', xyz_orientation='RAS', omero=None)</code>  <code>classmethod</code>","text":"<p>Create ZarrNii from an existing NgffImage.</p> <p>Parameters:</p> <ul> <li> <code>ngff_image</code>               (<code>NgffImage</code>)           \u2013            <p>NgffImage to wrap</p> </li> <li> <code>axes_order</code>               (<code>str</code>, default:                   <code>'ZYX'</code> )           \u2013            <p>Spatial axes order for NIfTI compatibility</p> </li> <li> <code>xyz_orientation</code>               (<code>str</code>, default:                   <code>'RAS'</code> )           \u2013            <p>Anatomical orientation string in XYZ axes order</p> </li> <li> <code>omero</code>               (<code>Optional[object]</code>, default:                   <code>None</code> )           \u2013            <p>Optional omero metadata object</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>'ZarrNii'</code>           \u2013            <p>ZarrNii instance</p> </li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>@classmethod\ndef from_ngff_image(\n    cls,\n    ngff_image: nz.NgffImage,\n    axes_order: str = \"ZYX\",\n    xyz_orientation: str = \"RAS\",\n    omero: Optional[object] = None,\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Create ZarrNii from an existing NgffImage.\n\n    Args:\n        ngff_image: NgffImage to wrap\n        axes_order: Spatial axes order for NIfTI compatibility\n        xyz_orientation: Anatomical orientation string in XYZ axes order\n        omero: Optional omero metadata object\n\n    Returns:\n        ZarrNii instance\n    \"\"\"\n    return cls(\n        ngff_image=ngff_image,\n        axes_order=axes_order,\n        xyz_orientation=xyz_orientation,\n        _omero=omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.from_darr","title":"<code>zarrnii.ZarrNii.from_darr(darr, axes_order='ZYX', orientation='RAS', spacing=(1.0, 1.0, 1.0), origin=(0.0, 0.0, 0.0), name='image', omero=None, affine=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create ZarrNii from dask array (legacy compatibility constructor).</p> <p>Parameters:</p> <ul> <li> <code>darr</code>               (<code>Array</code>)           \u2013            <p>Dask array containing image data</p> </li> <li> <code>axes_order</code>               (<code>str</code>, default:                   <code>'ZYX'</code> )           \u2013            <p>Spatial axes order</p> </li> <li> <code>orientation</code>               (<code>str</code>, default:                   <code>'RAS'</code> )           \u2013            <p>Anatomical orientation string</p> </li> <li> <code>spacing</code>               (<code>Tuple[float, float, float]</code>, default:                   <code>(1.0, 1.0, 1.0)</code> )           \u2013            <p>Voxel spacing, in axes_order</p> </li> <li> <code>origin</code>               (<code>Tuple[float, float, float]</code>, default:                   <code>(0.0, 0.0, 0.0)</code> )           \u2013            <p>Origin offset, in axes_order</p> </li> <li> <code>name</code>               (<code>str</code>, default:                   <code>'image'</code> )           \u2013            <p>Image name</p> </li> <li> <code>omero</code>               (<code>Optional[object]</code>, default:                   <code>None</code> )           \u2013            <p>Optional omero metadata</p> </li> <li> <code>affine</code>               (<code>Optional[AffineTransform]</code>, default:                   <code>None</code> )           \u2013            <p>Deprecated parameter - no longer supported</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>'ZarrNii'</code>           \u2013            <p>ZarrNii instance</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If affine parameter is provided</p> </li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>@classmethod\ndef from_darr(\n    cls,\n    darr: da.Array,\n    axes_order: str = \"ZYX\",\n    orientation: str = \"RAS\",\n    spacing: Tuple[float, float, float] = (1.0, 1.0, 1.0),\n    origin: Tuple[float, float, float] = (0.0, 0.0, 0.0),\n    name: str = \"image\",\n    omero: Optional[object] = None,\n    affine: Optional[AffineTransform] = None,\n    **kwargs,\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Create ZarrNii from dask array (legacy compatibility constructor).\n\n    Args:\n        darr: Dask array containing image data\n        axes_order: Spatial axes order\n        orientation: Anatomical orientation string\n        spacing: Voxel spacing, in axes_order\n        origin: Origin offset, in axes_order\n        name: Image name\n        omero: Optional omero metadata\n        affine: Deprecated parameter - no longer supported\n\n    Returns:\n        ZarrNii instance\n\n    Raises:\n        ValueError: If affine parameter is provided\n    \"\"\"\n    # Check for deprecated affine parameter\n    if affine is not None:\n        raise ValueError(\n            \"The 'affine' parameter is no longer supported in from_darr(). \"\n            \"Please use 'spacing' and 'origin' parameters instead. \"\n            \"If you need to specify a full affine transformation, use from_nifti() \"\n            \"or construct the NgffImage directly.\"\n        )\n\n    # Use spacing and origin\n    if axes_order == \"ZYX\":\n        scale = {\"z\": spacing[0], \"y\": spacing[1], \"x\": spacing[2]}\n        translation = {\"z\": origin[0], \"y\": origin[1], \"x\": origin[2]}\n    else:  # XYZ\n        scale = {\"x\": spacing[0], \"y\": spacing[1], \"z\": spacing[2]}\n        translation = {\"x\": origin[0], \"y\": origin[1], \"z\": origin[2]}\n\n    # Create dimensions based on data shape after dimension adjustments\n    final_ndim = len(darr.shape)\n    if final_ndim == 4:\n        # 4D: (c, z, y, x) or (c, x, y, z) - standard case\n        dims = [\"c\"] + list(axes_order.lower())\n    elif final_ndim == 5:\n        # 5D: (t, c, z, y, x) or (t, c, x, y, z) - time dimension included\n        dims = [\"t\", \"c\"] + list(axes_order.lower())\n    else:\n        # Fallback for other cases\n        dims = [\"c\"] + list(axes_order.lower())\n\n    # Create NgffImage\n    ngff_image = nz.NgffImage(\n        data=darr, dims=dims, scale=scale, translation=translation, name=name\n    )\n\n    return cls(\n        ngff_image=ngff_image,\n        axes_order=axes_order,\n        xyz_orientation=orientation,\n        _omero=omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.from_ome_zarr","title":"<code>zarrnii.ZarrNii.from_ome_zarr(store_or_path, level=0, channels=None, channel_labels=None, timepoints=None, storage_options=None, axes_order='ZYX', orientation=None, downsample_near_isotropic=False, chunks='auto', rechunk=False)</code>  <code>classmethod</code>","text":"<p>Load ZarrNii from OME-Zarr store with flexible options.</p> <p>Creates a ZarrNii instance from an OME-Zarr store, supporting multiscale pyramids, channel/timepoint selection, and various storage backends. Automatically handles metadata extraction and format conversion.</p> <p>Parameters:</p> <ul> <li> <code>store_or_path</code>               (<code>Union[str, Any]</code>)           \u2013            <p>Store or path to OME-Zarr file. Supports: - Local file paths - Remote URLs (s3://, http://, etc.) - ZIP files (.zip extension) - Zarr store objects</p> </li> <li> <code>level</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Pyramid level to load (0 = highest resolution). If level exceeds available levels, applies lazy downsampling</p> </li> <li> <code>channels</code>               (<code>Optional[List[int]]</code>, default:                   <code>None</code> )           \u2013            <p>List of channel indices to load (0-based). Mutually exclusive with channel_labels</p> </li> <li> <code>channel_labels</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of channel names to load by label. Requires OMERO metadata. Mutually exclusive with channels</p> </li> <li> <code>timepoints</code>               (<code>Optional[List[int]]</code>, default:                   <code>None</code> )           \u2013            <p>List of timepoint indices to load (0-based). If None, loads all available timepoints</p> </li> <li> <code>storage_options</code>               (<code>Optional[Dict[str, Any]]</code>, default:                   <code>None</code> )           \u2013            <p>Additional options for zarr storage backend (e.g., credentials for cloud storage)</p> </li> <li> <code>axes_order</code>               (<code>str</code>, default:                   <code>'ZYX'</code> )           \u2013            <p>Spatial axis order for NIfTI compatibility. Either \"ZYX\" or \"XYZ\"</p> </li> <li> <code>orientation</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Default anatomical orientation if not in metadata. Standard orientations like \"RAS\", \"LPI\", etc. This is always interpreted in XYZ axes order for consistency. This setting will override any orientation defined in the OME zarr metadata</p> </li> <li> <code>downsample_near_isotropic</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, automatically downsample dimensions with smaller voxel sizes to achieve near-isotropic resolution</p> </li> <li> <code>chunks</code>               (<code>tuple[int, Ellipsis] | Literal['auto']</code>, default:                   <code>'auto'</code> )           \u2013            <p>chunking strategy, or explicit chunk sizes to use if not automatic</p> </li> <li> <code>rechunk</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, rechunks the dataset after lazy loading, based on the chunks parameter</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>'ZarrNii'</code>           \u2013            <p>ZarrNii instance with loaded data and metadata</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If both channels and channel_labels are specified, or if invalid level/indices are provided</p> </li> <li> <code>FileNotFoundError</code>             \u2013            <p>If store_or_path does not exist</p> </li> <li> <code>KeyError</code>             \u2013            <p>If specified channel labels are not found</p> </li> <li> <code>IOError</code>             \u2013            <p>If unable to read from the storage backend</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Load full resolution data\n&gt;&gt;&gt; znii = ZarrNii.from_ome_zarr(\"/path/to/data.zarr\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Load specific channels and pyramid level\n&gt;&gt;&gt; znii = ZarrNii.from_ome_zarr(\n...     \"/path/to/data.zarr\",\n...     level=1,\n...     channels=[0, 2],\n...     orientation=\"LPI\"\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Load from cloud storage\n&gt;&gt;&gt; znii = ZarrNii.from_ome_zarr(\n...     \"s3://bucket/data.zarr\",\n...     storage_options={\"key\": \"access_key\", \"secret\": \"secret\"}\n... )\n</code></pre> Notes <p>Orientation Metadata Backwards Compatibility:</p> <p>This method implements backwards compatibility for orientation metadata:</p> <ol> <li> <p>Override: Setting the orientation here will override    any orientation defined in the OME Zarr metadata.</p> </li> <li> <p>Zarr Metadata: Checks for 'xyz_orientation' first (new format),    then falls back to 'orientation' (legacy format)</p> </li> <li> <p>Legacy Fallback: When only legacy 'orientation' is found, the    orientation string is automatically reversed to convert from ZYX-based    encoding (legacy) to XYZ-based encoding (current standard)</p> </li> <li> <p>Default Fallback: If no orientation metadata is found, uses RAS    orientation as the default.</p> </li> </ol> <p>Examples of the conversion: - Legacy 'orientation'='SAR' (ZYX) \u2192 'xyz_orientation'='RAS' (XYZ) - Legacy 'orientation'='IPL' (ZYX) \u2192 'xyz_orientation'='LPI' (XYZ)</p> <p>This ensures consistent orientation handling while maintaining backwards compatibility with existing OME-Zarr files that use the legacy format.</p> Source code in <code>zarrnii/core.py</code> <pre><code>@classmethod\ndef from_ome_zarr(\n    cls,\n    store_or_path: Union[str, Any],\n    level: int = 0,\n    channels: Optional[List[int]] = None,\n    channel_labels: Optional[List[str]] = None,\n    timepoints: Optional[List[int]] = None,\n    storage_options: Optional[Dict[str, Any]] = None,\n    axes_order: str = \"ZYX\",\n    orientation: Optional[str] = None,\n    downsample_near_isotropic: bool = False,\n    chunks: tuple[int, Ellipsis] | Literal[\"auto\"] = \"auto\",\n    rechunk: bool = False,\n) -&gt; \"ZarrNii\":\n    \"\"\"Load ZarrNii from OME-Zarr store with flexible options.\n\n    Creates a ZarrNii instance from an OME-Zarr store, supporting multiscale\n    pyramids, channel/timepoint selection, and various storage backends.\n    Automatically handles metadata extraction and format conversion.\n\n    Args:\n        store_or_path: Store or path to OME-Zarr file. Supports:\n            - Local file paths\n            - Remote URLs (s3://, http://, etc.)\n            - ZIP files (.zip extension)\n            - Zarr store objects\n        level: Pyramid level to load (0 = highest resolution). If level\n            exceeds available levels, applies lazy downsampling\n        channels: List of channel indices to load (0-based). Mutually\n            exclusive with channel_labels\n        channel_labels: List of channel names to load by label. Requires\n            OMERO metadata. Mutually exclusive with channels\n        timepoints: List of timepoint indices to load (0-based). If None,\n            loads all available timepoints\n        storage_options: Additional options for zarr storage backend\n            (e.g., credentials for cloud storage)\n        axes_order: Spatial axis order for NIfTI compatibility.\n            Either \"ZYX\" or \"XYZ\"\n        orientation: Default anatomical orientation if not in metadata.\n            Standard orientations like \"RAS\", \"LPI\", etc. This is always\n            interpreted in XYZ axes order for consistency. This setting will override\n            any orientation defined in the OME zarr metadata\n        downsample_near_isotropic: If True, automatically downsample\n            dimensions with smaller voxel sizes to achieve near-isotropic\n            resolution\n        chunks: chunking strategy, or explicit chunk sizes to use if not automatic\n        rechunk: If True, rechunks the dataset after lazy loading, based\n            on the chunks parameter\n\n    Returns:\n        ZarrNii instance with loaded data and metadata\n\n    Raises:\n        ValueError: If both channels and channel_labels are specified,\n            or if invalid level/indices are provided\n        FileNotFoundError: If store_or_path does not exist\n        KeyError: If specified channel labels are not found\n        IOError: If unable to read from the storage backend\n\n    Examples:\n        &gt;&gt;&gt; # Load full resolution data\n        &gt;&gt;&gt; znii = ZarrNii.from_ome_zarr(\"/path/to/data.zarr\")\n\n        &gt;&gt;&gt; # Load specific channels and pyramid level\n        &gt;&gt;&gt; znii = ZarrNii.from_ome_zarr(\n        ...     \"/path/to/data.zarr\",\n        ...     level=1,\n        ...     channels=[0, 2],\n        ...     orientation=\"LPI\"\n        ... )\n\n        &gt;&gt;&gt; # Load from cloud storage\n        &gt;&gt;&gt; znii = ZarrNii.from_ome_zarr(\n        ...     \"s3://bucket/data.zarr\",\n        ...     storage_options={\"key\": \"access_key\", \"secret\": \"secret\"}\n        ... )\n\n    Notes:\n        **Orientation Metadata Backwards Compatibility:**\n\n        This method implements backwards compatibility for orientation metadata:\n\n        1. **Override**: Setting the orientation here will override\n           any orientation defined in the OME Zarr metadata.\n\n        2. **Zarr Metadata**: Checks for 'xyz_orientation' first (new format),\n           then falls back to 'orientation' (legacy format)\n\n        3. **Legacy Fallback**: When only legacy 'orientation' is found, the\n           orientation string is automatically reversed to convert from ZYX-based\n           encoding (legacy) to XYZ-based encoding (current standard)\n\n        4. **Default Fallback**: If no orientation metadata is found, uses RAS\n           orientation as the default.\n\n        Examples of the conversion:\n        - Legacy 'orientation'='SAR' (ZYX) \u2192 'xyz_orientation'='RAS' (XYZ)\n        - Legacy 'orientation'='IPL' (ZYX) \u2192 'xyz_orientation'='LPI' (XYZ)\n\n        This ensures consistent orientation handling while maintaining backwards\n        compatibility with existing OME-Zarr files that use the legacy format.\n    \"\"\"\n    # Validate channel and timepoint selection arguments\n    if channels is not None and channel_labels is not None:\n        raise ValueError(\"Cannot specify both 'channels' and 'channel_labels'\")\n\n    # Load the multiscales object\n    try:\n        if isinstance(store_or_path, str):\n            # Handle ZIP files by creating a ZipStore\n            if store_or_path.endswith(\".zip\"):\n                import zarr\n\n                store = zarr.storage.ZipStore(store_or_path, mode=\"r\")\n                multiscales = nz.from_ngff_zarr(\n                    store, storage_options=storage_options or {}\n                )\n                # Note: We'll close the store after extracting metadata\n            else:\n                multiscales = nz.from_ngff_zarr(\n                    store_or_path, storage_options=storage_options or {}\n                )\n        else:\n            multiscales = nz.from_ngff_zarr(store_or_path)\n    except Exception as e:\n        # Fallback for older zarr/ngff_zarr versions\n        if isinstance(store_or_path, str):\n            if store_or_path.endswith(\".zip\"):\n                import zarr\n\n                store = zarr.storage.ZipStore(store_or_path, mode=\"r\")\n                multiscales = nz.from_ngff_zarr(store)\n            else:\n                store = fsspec.get_mapper(store_or_path, **storage_options or {})\n                multiscales = nz.from_ngff_zarr(store)\n        else:\n            store = store_or_path\n            multiscales = nz.from_ngff_zarr(store)\n\n    # Extract omero metadata if available\n    omero_metadata = None\n    try:\n        import zarr\n\n        if isinstance(store_or_path, str):\n            if store_or_path.endswith(\".zip\"):\n                zip_store = zarr.storage.ZipStore(store_or_path, mode=\"r\")\n                group = zarr.open_group(zip_store, mode=\"r\")\n                # Close zip store after getting group\n                zip_store.close()\n            else:\n                group = zarr.open_group(store_or_path, mode=\"r\")\n\n        else:\n            group = zarr.open_group(store_or_path, mode=\"r\")\n\n        if \"omero\" in group.attrs:\n            omero_dict = group.attrs[\"omero\"]\n\n            # Create a simple object to hold omero metadata\n            class OmeroMetadata:\n                def __init__(self, omero_dict):\n                    self.channels = []\n                    if \"channels\" in omero_dict:\n                        for ch_dict in omero_dict[\"channels\"]:\n                            # Create channel objects\n                            class ChannelMetadata:\n                                def __init__(self, ch_dict):\n                                    self.label = ch_dict.get(\"label\", \"\")\n                                    self.color = ch_dict.get(\"color\", \"\")\n                                    if \"window\" in ch_dict:\n\n                                        class WindowMetadata:\n                                            def __init__(self, win_dict):\n                                                self.min = win_dict.get(\"min\", 0.0)\n                                                self.max = win_dict.get(\n                                                    \"max\", 65535.0\n                                                )\n                                                self.start = win_dict.get(\n                                                    \"start\", 0.0\n                                                )\n                                                self.end = win_dict.get(\n                                                    \"end\", 65535.0\n                                                )\n\n                                        self.window = WindowMetadata(\n                                            ch_dict[\"window\"]\n                                        )\n                                    else:\n                                        self.window = None\n\n                            self.channels.append(ChannelMetadata(ch_dict))\n\n            omero_metadata = OmeroMetadata(omero_dict)\n    except Exception:\n        # If we can't load omero metadata, that's okay\n        pass\n\n    # Read orientation metadata with backwards compatibility support\n    # Priority: xyz_orientation (new) &gt; orientation (legacy, with reversal)\n    try:\n        import zarr\n\n        if orientation is None:\n\n            if isinstance(store_or_path, str):\n                if store_or_path.endswith(\".zip\"):\n                    zip_store = zarr.storage.ZipStore(store_or_path, mode=\"r\")\n                    group = zarr.open_group(zip_store, mode=\"r\")\n                    # Check for new xyz_orientation first, then fallback to legacy orientation\n                    if \"xyz_orientation\" in group.attrs:\n                        orientation = group.attrs[\"xyz_orientation\"]\n                    elif \"orientation\" in group.attrs:\n                        # Legacy orientation is ZYX-based, reverse it to get XYZ-based orientation\n                        legacy_orientation = group.attrs[\"orientation\"]\n                        orientation = reverse_orientation_string(legacy_orientation)\n                    # If neither found, use the provided default orientation\n                    zip_store.close()\n                else:\n                    group = zarr.open_group(store_or_path, mode=\"r\")\n                    # Check for new xyz_orientation first, then fallback to legacy orientation\n                    if \"xyz_orientation\" in group.attrs:\n                        orientation = group.attrs[\"xyz_orientation\"]\n                    elif \"orientation\" in group.attrs:\n                        # Legacy orientation is ZYX-based, reverse it to get XYZ-based orientation\n                        legacy_orientation = group.attrs[\"orientation\"]\n                        orientation = reverse_orientation_string(legacy_orientation)\n                    # If neither found, use the provided default orientation\n            else:\n                group = zarr.open_group(store_or_path, mode=\"r\")\n                # Check for new xyz_orientation first, then fallback to legacy orientation\n                if \"xyz_orientation\" in group.attrs:\n                    orientation = group.attrs[\"xyz_orientation\"]\n                elif \"orientation\" in group.attrs:\n                    # Legacy orientation is ZYX-based, reverse it to get XYZ-based orientation\n                    legacy_orientation = group.attrs[\"orientation\"]\n                    orientation = reverse_orientation_string(legacy_orientation)\n                # If neither found, use the provided default orientation\n\n    except Exception:\n        # If we can't read orientation metadata, use the provided default\n        pass\n\n    # If orientation is still None, use the fallback default\n    if orientation is None:\n        orientation = \"RAS\"\n    # Determine the available pyramid levels and handle lazy downsampling\n    max_level = len(multiscales.images) - 1\n    actual_level = min(level, max_level)\n    do_downsample = level &gt; max_level\n\n    # Get the highest available level\n    ngff_image = multiscales.images[actual_level]\n\n    # Handle channel and timepoint selection and filter omero metadata accordingly\n    filtered_omero = omero_metadata\n    if channels is not None or channel_labels is not None or timepoints is not None:\n        ngff_image, filtered_omero = _select_dimensions_from_image_with_omero(\n            ngff_image,\n            multiscales,\n            channels,\n            channel_labels,\n            timepoints,\n            omero_metadata,\n        )\n\n    # Create ZarrNii instance with xyz_orientation\n    znimg = cls(\n        ngff_image=ngff_image,\n        axes_order=axes_order,\n        xyz_orientation=orientation,\n        _omero=filtered_omero,\n    )\n\n    # Apply lazy downsampling if needed\n    if do_downsample:\n        level_ds = level - max_level\n        downsample_factor = 2**level_ds\n\n        # Get spatial dims based on axes order\n        spatial_dims = [\"z\", \"y\", \"x\"] if axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n\n        # Apply downsampling using the existing method\n        znimg = znimg.downsample(\n            factors=downsample_factor, spatial_dims=spatial_dims\n        )\n\n    # Apply near-isotropic downsampling if requested\n    if downsample_near_isotropic:\n        znimg = _apply_near_isotropic_downsampling(znimg, axes_order)\n\n    if rechunk:\n        znimg.data = znimg.data.rechunk(chunks)\n\n    return znimg\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.from_nifti","title":"<code>zarrnii.ZarrNii.from_nifti(path, chunks='auto', axes_order='XYZ', name=None, as_ref=False, zooms=None)</code>  <code>classmethod</code>","text":"<p>Load ZarrNii from NIfTI file with flexible loading options.</p> <p>Creates a ZarrNii instance from a NIfTI file, automatically converting the data to dask arrays and extracting spatial transformation information. Supports both full data loading and reference-only loading for memory efficiency. For 4D NIfTI files, the 4th dimension is treated as channels (XYZC ordering, analogous to CZYX in OME-Zarr).</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Union[str, bytes]</code>)           \u2013            <p>File path to NIfTI file (.nii, .nii.gz, .img/.hdr)</p> </li> <li> <code>chunks</code>               (<code>Union[str, Tuple[int, ...]]</code>, default:                   <code>'auto'</code> )           \u2013            <p>Dask array chunking strategy. Can be: - \"auto\": Automatic chunking based on file size - Tuple of ints: Manual chunk sizes for each dimension - Dict mapping axis to chunk size</p> </li> <li> <code>axes_order</code>               (<code>str</code>, default:                   <code>'XYZ'</code> )           \u2013            <p>Spatial axis ordering convention. Either: - \"XYZ\": X=left-right, Y=anterior-posterior, Z=inferior-superior - \"ZYX\": Z=inferior-superior, Y=anterior-posterior, X=left-right</p> </li> <li> <code>name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional name for the resulting NgffImage. If None, uses filename without extension</p> </li> <li> <code>as_ref</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, creates empty dask array with correct shape/metadata without loading actual image data (memory efficient for templates)</p> </li> <li> <code>zooms</code>               (<code>Optional[Tuple[float, float, float]]</code>, default:                   <code>None</code> )           \u2013            <p>Target voxel spacing as (x, y, z) in mm. Only valid when as_ref=True. Adjusts shape and affine accordingly</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>'ZarrNii'</code>           \u2013            <p>ZarrNii instance containing NIfTI data and spatial metadata. If the</p> </li> <li> <code>'ZarrNii'</code>           \u2013            <p>NIfTI file contains channel labels in header extensions, they will be</p> </li> <li> <code>'ZarrNii'</code>           \u2013            <p>preserved in OMERO metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If zooms specified with as_ref=False, or invalid axes_order</p> </li> <li> <code>FileNotFoundError</code>             \u2013            <p>If NIfTI file does not exist</p> </li> <li> <code>OSError</code>             \u2013            <p>If unable to read NIfTI file</p> </li> <li> <code>ImageFileError</code>             \u2013            <p>If file is not valid NIfTI</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Load full NIfTI data\n&gt;&gt;&gt; znii = ZarrNii.from_nifti(\"/path/to/brain.nii.gz\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Load with custom chunking and axis order\n&gt;&gt;&gt; znii = ZarrNii.from_nifti(\n...     \"/path/to/data.nii\",\n...     chunks=(64, 64, 64),\n...     axes_order=\"ZYX\"\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Load 4D NIfTI with multiple channels\n&gt;&gt;&gt; znii = ZarrNii.from_nifti(\"/path/to/multichannel.nii.gz\")\n&gt;&gt;&gt; print(znii.list_channels())  # Shows channel labels if stored\n</code></pre> <pre><code>&gt;&gt;&gt; # Create reference with target resolution\n&gt;&gt;&gt; znii_ref = ZarrNii.from_nifti(\n...     \"/path/to/template.nii.gz\",\n...     as_ref=True,\n...     zooms=(2.0, 2.0, 2.0)\n... )\n</code></pre> Notes <ul> <li>The method automatically handles NIfTI orientation codes and converts   them to the specified axes_order for consistency with OME-Zarr workflows</li> <li>For 4D NIfTI files, the 4th dimension is interpreted as channels (XYZC)</li> <li>Channel labels stored in NIfTI header extensions are automatically loaded</li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>@classmethod\ndef from_nifti(\n    cls,\n    path: Union[str, bytes],\n    chunks: Union[str, Tuple[int, ...]] = \"auto\",\n    axes_order: str = \"XYZ\",\n    name: Optional[str] = None,\n    as_ref: bool = False,\n    zooms: Optional[Tuple[float, float, float]] = None,\n) -&gt; \"ZarrNii\":\n    \"\"\"Load ZarrNii from NIfTI file with flexible loading options.\n\n    Creates a ZarrNii instance from a NIfTI file, automatically converting\n    the data to dask arrays and extracting spatial transformation information.\n    Supports both full data loading and reference-only loading for memory\n    efficiency. For 4D NIfTI files, the 4th dimension is treated as channels\n    (XYZC ordering, analogous to CZYX in OME-Zarr).\n\n    Args:\n        path: File path to NIfTI file (.nii, .nii.gz, .img/.hdr)\n        chunks: Dask array chunking strategy. Can be:\n            - \"auto\": Automatic chunking based on file size\n            - Tuple of ints: Manual chunk sizes for each dimension\n            - Dict mapping axis to chunk size\n        axes_order: Spatial axis ordering convention. Either:\n            - \"XYZ\": X=left-right, Y=anterior-posterior, Z=inferior-superior\n            - \"ZYX\": Z=inferior-superior, Y=anterior-posterior, X=left-right\n        name: Optional name for the resulting NgffImage. If None,\n            uses filename without extension\n        as_ref: If True, creates empty dask array with correct shape/metadata\n            without loading actual image data (memory efficient for templates)\n        zooms: Target voxel spacing as (x, y, z) in mm. Only valid when\n            as_ref=True. Adjusts shape and affine accordingly\n\n    Returns:\n        ZarrNii instance containing NIfTI data and spatial metadata. If the\n        NIfTI file contains channel labels in header extensions, they will be\n        preserved in OMERO metadata.\n\n    Raises:\n        ValueError: If zooms specified with as_ref=False, or invalid axes_order\n        FileNotFoundError: If NIfTI file does not exist\n        OSError: If unable to read NIfTI file\n        nibabel.filebasedimages.ImageFileError: If file is not valid NIfTI\n\n    Examples:\n        &gt;&gt;&gt; # Load full NIfTI data\n        &gt;&gt;&gt; znii = ZarrNii.from_nifti(\"/path/to/brain.nii.gz\")\n\n        &gt;&gt;&gt; # Load with custom chunking and axis order\n        &gt;&gt;&gt; znii = ZarrNii.from_nifti(\n        ...     \"/path/to/data.nii\",\n        ...     chunks=(64, 64, 64),\n        ...     axes_order=\"ZYX\"\n        ... )\n\n        &gt;&gt;&gt; # Load 4D NIfTI with multiple channels\n        &gt;&gt;&gt; znii = ZarrNii.from_nifti(\"/path/to/multichannel.nii.gz\")\n        &gt;&gt;&gt; print(znii.list_channels())  # Shows channel labels if stored\n\n        &gt;&gt;&gt; # Create reference with target resolution\n        &gt;&gt;&gt; znii_ref = ZarrNii.from_nifti(\n        ...     \"/path/to/template.nii.gz\",\n        ...     as_ref=True,\n        ...     zooms=(2.0, 2.0, 2.0)\n        ... )\n\n    Notes:\n        - The method automatically handles NIfTI orientation codes and converts\n          them to the specified axes_order for consistency with OME-Zarr workflows\n        - For 4D NIfTI files, the 4th dimension is interpreted as channels (XYZC)\n        - Channel labels stored in NIfTI header extensions are automatically loaded\n    \"\"\"\n    if not as_ref and zooms is not None:\n        raise ValueError(\"`zooms` can only be used when `as_ref=True`.\")\n\n    # Load NIfTI file\n    nifti_img = nib.load(path)\n    shape = nifti_img.header.get_data_shape()\n    affine_matrix = nifti_img.affine.copy()\n\n    # infer orientation from the affine\n    orientation = _affine_to_orientation(affine_matrix)\n\n    in_zooms = np.array(nifti_img.header.get_zooms())\n\n    # Adjust shape and affine if zooms are provided\n    if zooms is not None:\n        scaling_factor = in_zooms / zooms\n        new_shape = [\n            int(np.floor(shape[0] * scaling_factor[2])),  # Z\n            int(np.floor(shape[1] * scaling_factor[1])),  # Y\n            int(np.floor(shape[2] * scaling_factor[0])),  # X\n        ]\n        # create affine by specifying orientation, scale and translation\n        affine_matrix = _axcodes2aff(orientation, zooms, affine_matrix[:3, 3])\n        in_zooms = zooms\n    else:\n        new_shape = shape\n\n    if as_ref:\n        # Create an empty dask array with the adjusted shape\n        # Already add channel dimension here\n        darr = da.zeros((1, *new_shape), chunks=chunks, dtype=\"float32\")\n\n        # Mark that we already added channel dimension\n        has_channel_dim = True\n\n    else:\n        # Load the NIfTI data and convert to a dask array\n        array = nifti_img.get_fdata()\n        darr = da.from_array(array, chunks=chunks)\n        has_channel_dim = False\n\n    # NIfTI uses XYZ ordering, but we need to handle channels\n    # For 4D NIfTI: XYZC (4th dim is channels, analogous to CZYX in OME-Zarr)\n    original_ndim = len(darr.shape)\n\n    if has_channel_dim:\n        # Already has channel dimension from as_ref, don't modify\n        pass\n    elif original_ndim == 3:\n        # 3D data: add channel dimension -&gt; (c, z, y, x) or (c, x, y, z)\n        darr = darr[np.newaxis, ...]\n        # If axes_order is to ultimately be ZYX, transpose spatial XYZ to ZYX\n        if axes_order == \"ZYX\":\n            darr = darr.transpose(0, 3, 2, 1)  # CXYZ -&gt; CZYX\n    elif original_ndim == 4:\n        # 4D data: NIfTI stores as XYZC, we need CZYX or CXYZ\n        if axes_order == \"ZYX\":\n            # Transpose from XYZC to CZYX\n            darr = darr.transpose(3, 2, 1, 0)  # XYZC -&gt; CZYX\n        else:\n            # Transpose from XYZC to CXYZ\n            darr = darr.transpose(3, 0, 1, 2)  # XYZC -&gt; CXYZ\n    elif original_ndim == 5:\n        # 5D data: assume (t, z, y, x, c) and handle appropriately\n        pass  # Keep as is - 5D is already the target format\n    else:\n        # For 1D, 2D, or &gt;5D data, add channel dimension and let user handle\n        darr = darr[np.newaxis, ...]\n\n    # Create dimensions based on data shape after dimension adjustments\n    final_ndim = len(darr.shape)\n    if final_ndim == 4:\n        # 4D: (c, z, y, x) or (c, x, y, z) - standard case\n        dims = [\"c\"] + list(axes_order.lower())\n    elif final_ndim == 5:\n        # 5D: (t, c, z, y, x) or (t, c, x, y, z) - time dimension included\n        dims = [\"t\", \"c\"] + list(axes_order.lower())\n    else:\n        # Fallback for other cases\n        dims = [\"c\"] + list(axes_order.lower())\n\n    # Extract translation from affine, scale from the zooms\n    scale = {}\n    translation = {}\n    spatial_dims = [\"z\", \"y\", \"x\"] if axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n\n    for i, dim in enumerate(spatial_dims):\n        scale[dim] = float(in_zooms[i])\n        translation[dim] = affine_matrix[i, 3]\n\n    # Create NgffImage\n    if name is None:\n        name = f\"nifti_image_{path}\"\n\n    ngff_image = nz.NgffImage(\n        data=darr, dims=dims, scale=scale, translation=translation, name=name\n    )\n\n    # Extract channel labels from NIfTI header extensions if present\n    channel_labels = None\n    if (\n        hasattr(nifti_img.header, \"extensions\")\n        and len(nifti_img.header.extensions) &gt; 0\n    ):\n        import json\n\n        for ext in nifti_img.header.extensions:\n            try:\n                if ext.get_code() == 1:\n                    # Try to decode the extension content as JSON\n                    content = ext.get_content().decode(\"utf-8\")\n                    metadata = json.loads(content)\n\n                    # Look for channel_labels in the metadata\n                    if \"channel_labels\" in metadata:\n                        channel_labels = metadata[\"channel_labels\"]\n                        break\n            except (json.JSONDecodeError, UnicodeDecodeError, AttributeError):\n                # Skip extensions that aren't JSON or can't be decoded\n                continue\n\n    # Create ZarrNii instance\n    # Extract OMERO metadata for channel labels if present\n    omero_metadata = None\n    if channel_labels is not None and len(channel_labels) &gt; 0:\n        # Get the number of channels from the data\n        num_channels = darr.shape[0] if \"c\" in dims else 1\n\n        # Only use channel labels if count matches\n        if len(channel_labels) == num_channels:\n            # Create OMERO metadata with channel labels\n            try:\n                from ngff_zarr import Omero, OmeroChannel, OmeroWindow\n\n                # Create OMERO channels with labels\n                omero_channels = []\n                for label in channel_labels:\n                    # Create a minimal channel object with label\n                    # Use default color (white) and window values\n                    window = OmeroWindow(min=0.0, max=1.0, start=0.0, end=1.0)\n                    omero_channels.append(\n                        OmeroChannel(\n                            color=\"FFFFFF\", window=window, label=label  # white\n                        )\n                    )\n\n                # Create OMERO metadata\n                omero_metadata = Omero(channels=omero_channels)\n            except (ImportError, AttributeError, TypeError):\n                # If OMERO classes aren't available or fail, skip\n                pass\n\n    zarrnii_instance = cls(\n        ngff_image=ngff_image,\n        axes_order=axes_order,\n        xyz_orientation=orientation,\n        _omero=omero_metadata,\n    )\n\n    return zarrnii_instance\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.crop","title":"<code>zarrnii.ZarrNii.crop(bbox_min, bbox_max=None, spatial_dims=None, physical_coords=False)</code>","text":"<p>Extract a spatial region or multiple regions from the image.</p> <p>Crops the image to the specified bounding box coordinates, preserving all metadata and non-spatial dimensions (channels, time). The cropping is performed in voxel coordinates by default, or physical coordinates if specified. Can crop a single region or multiple regions at once.</p> <p>Parameters:</p> <ul> <li> <code>bbox_min</code>               (<code>Union[Tuple[float, ...], List[Tuple[Tuple[float, ...], Tuple[float, ...]]]]</code>)           \u2013            <p>Either: - Minimum corner coordinates of bounding box as tuple   (when bbox_max is provided). Length should match number of   spatial dimensions (x, y, z order) - List of (bbox_min, bbox_max) tuples for batch cropping   (when bbox_max is None)</p> </li> <li> <code>bbox_max</code>               (<code>Optional[Tuple[float, ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Maximum corner coordinates of bounding box as tuple. Length should match number of spatial dimensions (x, y, z order). Should be None when bbox_min is a list of bounding boxes.</p> </li> <li> <code>spatial_dims</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Names of spatial dimensions to crop. If None, automatically derived from axes_order (\"z\",\"y\",\"x\" for ZYX or \"x\",\"y\",\"z\" for XYZ)</p> </li> <li> <code>physical_coords</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, bbox_min and bbox_max are in physical/world coordinates (mm). If False, they are in voxel coordinates. Default is False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union['ZarrNii', List['ZarrNii']]</code>           \u2013            <p>New ZarrNii instance with cropped data (single crop) or list of</p> </li> <li> <code>Union['ZarrNii', List['ZarrNii']]</code>           \u2013            <p>ZarrNii instances (batch crop) with updated spatial metadata</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If bbox coordinates are invalid or out of bounds, or if both list and bbox_max are provided</p> </li> <li> <code>IndexError</code>             \u2013            <p>If bbox dimensions don't match spatial dimensions</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Crop 3D region (voxel coordinates)\n&gt;&gt;&gt; cropped = znii.crop((10, 20, 30), (110, 120, 130))\n</code></pre> <pre><code>&gt;&gt;&gt; # Crop with physical coordinates\n&gt;&gt;&gt; cropped = znii.crop((10.5, 20.5, 30.5), (110.5, 120.5, 130.5),\n...                      physical_coords=True)\n</code></pre> <pre><code>&gt;&gt;&gt; # Crop with explicit spatial dimensions\n&gt;&gt;&gt; cropped = znii.crop(\n...     (50, 60, 70), (150, 160, 170),\n...     spatial_dims=[\"x\", \"y\", \"z\"]\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Batch crop multiple regions\n&gt;&gt;&gt; bboxes = [\n...     ((10, 20, 30), (60, 70, 80)),\n...     ((100, 110, 120), (150, 160, 170))\n... ]\n&gt;&gt;&gt; cropped_list = znii.crop(bboxes, physical_coords=True)\n</code></pre> Notes <ul> <li>Coordinates are in voxel space (0-based indexing) by default</li> <li>Physical coordinates are in RAS orientation (Right-Anterior-Superior)</li> <li>The cropped region includes bbox_min but excludes bbox_max</li> <li>All non-spatial dimensions (channels, time) are preserved</li> <li>Spatial transformations are automatically updated</li> <li>When batch cropping, all patches share the same spatial_dims and   physical_coords settings</li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def crop(\n    self,\n    bbox_min: Union[\n        Tuple[float, ...], List[Tuple[Tuple[float, ...], Tuple[float, ...]]]\n    ],\n    bbox_max: Optional[Tuple[float, ...]] = None,\n    spatial_dims: Optional[List[str]] = None,\n    physical_coords: bool = False,\n) -&gt; Union[\"ZarrNii\", List[\"ZarrNii\"]]:\n    \"\"\"Extract a spatial region or multiple regions from the image.\n\n    Crops the image to the specified bounding box coordinates, preserving\n    all metadata and non-spatial dimensions (channels, time). The cropping\n    is performed in voxel coordinates by default, or physical coordinates\n    if specified. Can crop a single region or multiple regions at once.\n\n    Args:\n        bbox_min: Either:\n            - Minimum corner coordinates of bounding box as tuple\n              (when bbox_max is provided). Length should match number of\n              spatial dimensions (x, y, z order)\n            - List of (bbox_min, bbox_max) tuples for batch cropping\n              (when bbox_max is None)\n        bbox_max: Maximum corner coordinates of bounding box as tuple.\n            Length should match number of spatial dimensions (x, y, z order).\n            Should be None when bbox_min is a list of bounding boxes.\n        spatial_dims: Names of spatial dimensions to crop. If None,\n            automatically derived from axes_order (\"z\",\"y\",\"x\" for ZYX\n            or \"x\",\"y\",\"z\" for XYZ)\n        physical_coords: If True, bbox_min and bbox_max are in physical/world\n            coordinates (mm). If False, they are in voxel coordinates.\n            Default is False.\n\n    Returns:\n        New ZarrNii instance with cropped data (single crop) or list of\n        ZarrNii instances (batch crop) with updated spatial metadata\n\n    Raises:\n        ValueError: If bbox coordinates are invalid or out of bounds, or\n            if both list and bbox_max are provided\n        IndexError: If bbox dimensions don't match spatial dimensions\n\n    Examples:\n        &gt;&gt;&gt; # Crop 3D region (voxel coordinates)\n        &gt;&gt;&gt; cropped = znii.crop((10, 20, 30), (110, 120, 130))\n\n        &gt;&gt;&gt; # Crop with physical coordinates\n        &gt;&gt;&gt; cropped = znii.crop((10.5, 20.5, 30.5), (110.5, 120.5, 130.5),\n        ...                      physical_coords=True)\n\n        &gt;&gt;&gt; # Crop with explicit spatial dimensions\n        &gt;&gt;&gt; cropped = znii.crop(\n        ...     (50, 60, 70), (150, 160, 170),\n        ...     spatial_dims=[\"x\", \"y\", \"z\"]\n        ... )\n\n        &gt;&gt;&gt; # Batch crop multiple regions\n        &gt;&gt;&gt; bboxes = [\n        ...     ((10, 20, 30), (60, 70, 80)),\n        ...     ((100, 110, 120), (150, 160, 170))\n        ... ]\n        &gt;&gt;&gt; cropped_list = znii.crop(bboxes, physical_coords=True)\n\n    Notes:\n        - Coordinates are in voxel space (0-based indexing) by default\n        - Physical coordinates are in RAS orientation (Right-Anterior-Superior)\n        - The cropped region includes bbox_min but excludes bbox_max\n        - All non-spatial dimensions (channels, time) are preserved\n        - Spatial transformations are automatically updated\n        - When batch cropping, all patches share the same spatial_dims and\n          physical_coords settings\n    \"\"\"\n    # Check if this is batch cropping (list of bounding boxes)\n    # A batch crop is a list of (bbox_min, bbox_max) tuples\n    # Each element should be a tuple/list of two elements\n    is_batch_crop = (\n        isinstance(bbox_min, list)\n        and len(bbox_min) &gt; 0\n        and isinstance(bbox_min[0], (tuple, list))\n        and len(bbox_min[0]) == 2\n    )\n\n    if is_batch_crop:\n        if bbox_max is not None:\n            raise ValueError(\n                \"bbox_max should be None when bbox_min is a list of bounding boxes\"\n            )\n        # Batch crop: recursively call crop for each bounding box\n        return [\n            self.crop(bmin, bmax, spatial_dims, physical_coords)\n            for bmin, bmax in bbox_min\n        ]\n\n    # Single crop: original implementation\n    if bbox_max is None:\n        raise ValueError(\"bbox_max is required when bbox_min is not a list\")\n\n    if spatial_dims is None:\n        spatial_dims = (\n            [\"z\", \"y\", \"x\"] if self.axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n        )\n\n    # Convert physical coordinates to voxel coordinates if needed\n    if physical_coords:\n        # Physical coords are always in (x, y, z) order\n        # Convert to homogeneous coordinates\n        phys_min = np.array(list(bbox_min) + [1.0])\n        phys_max = np.array(list(bbox_max) + [1.0])\n\n        # Get inverse affine to convert from physical to voxel\n        affine_inv = np.linalg.inv(\n            self.get_affine_matrix(axes_order=\"XYZ\")\n        )  # TODO: should this always be xyz affine??\n\n        # Transform to voxel coordinates\n        voxel_min = affine_inv @ phys_min\n        voxel_max = affine_inv @ phys_max\n\n        # Extract voxel coordinates (x, y, z)\n        voxel_min_xyz = voxel_min[:3]\n        voxel_max_xyz = voxel_max[:3]\n\n        # Round to nearest integer voxel indices\n        voxel_min_xyz = np.round(voxel_min_xyz).astype(int)\n        voxel_max_xyz = np.round(voxel_max_xyz).astype(int)\n\n        # Ensure max &gt;= min\n        voxel_min_xyz = np.minimum(voxel_min_xyz, voxel_max_xyz)\n        voxel_max_xyz = np.maximum(\n            np.round(affine_inv @ phys_min).astype(int)[:3],\n            np.round(affine_inv @ phys_max).astype(int)[:3],\n        )\n\n        # Create mapping from x,y,z to voxel coordinates\n        bbox_min = voxel_min_xyz\n        bbox_max = voxel_max_xyz\n\n    # Create mapping from x,y,z to voxel coordinates\n    bbox_vox_min = {\n        \"x\": bbox_min[0],\n        \"y\": bbox_min[1],\n        \"z\": bbox_min[2],\n    }\n    bbox_vox_max = {\n        \"x\": bbox_max[0],\n        \"y\": bbox_max[1],\n        \"z\": bbox_max[2],\n    }\n\n    dim_flips = _axcodes2flips(self.orientation)\n    cropped_image = crop_ngff_image(\n        self.ngff_image, bbox_vox_min, bbox_vox_max, dim_flips\n    )\n    return ZarrNii(\n        ngff_image=cropped_image,\n        axes_order=self.axes_order,\n        xyz_orientation=self.xyz_orientation,\n        _omero=self._omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.crop_with_bounding_box","title":"<code>zarrnii.ZarrNii.crop_with_bounding_box(bbox_min, bbox_max, ras_coords=False)</code>","text":"<p>Legacy method name for crop.</p> <p>Parameters:</p> <ul> <li> <code>bbox_min</code>           \u2013            <p>Minimum corner coordinates</p> </li> <li> <code>bbox_max</code>           \u2013            <p>Maximum corner coordinates</p> </li> <li> <code>ras_coords</code>           \u2013            <p>If True, coordinates are in RAS physical space (deprecated, use physical_coords parameter of crop() instead)</p> </li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def crop_with_bounding_box(self, bbox_min, bbox_max, ras_coords=False):\n    \"\"\"Legacy method name for crop.\n\n    Args:\n        bbox_min: Minimum corner coordinates\n        bbox_max: Maximum corner coordinates\n        ras_coords: If True, coordinates are in RAS physical space (deprecated,\n            use physical_coords parameter of crop() instead)\n    \"\"\"\n    return self.crop(bbox_min, bbox_max, physical_coords=ras_coords)\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.crop_centered","title":"<code>zarrnii.ZarrNii.crop_centered(centers, patch_size, spatial_dims=None, fill_value=0.0)</code>","text":"<p>Extract fixed-size patches centered at specified coordinates.</p> <p>Crops the image to extract patches of a fixed size (in voxels) centered at the given physical coordinates. This is particularly useful for machine learning workflows where training patches must have consistent dimensions. The method can process a single center or multiple centers at once.</p> <p>Patches that extend beyond image boundaries are padded with the fill_value to ensure all patches have exactly the requested size.</p> <p>Parameters:</p> <ul> <li> <code>centers</code>               (<code>Union[Tuple[float, float, float], List[Tuple[float, float, float]]]</code>)           \u2013            <p>Either: - Single center coordinate as (x, y, z) tuple in physical space (mm) - List of center coordinates for batch processing</p> </li> <li> <code>patch_size</code>               (<code>Tuple[int, int, int]</code>)           \u2013            <p>Size of the patch in voxels as (x, y, z) tuple. This defines the dimensions of each cropped region in voxel space. All returned patches will have exactly this size.</p> </li> <li> <code>spatial_dims</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Names of spatial dimensions to crop. If None, automatically derived from axes_order (\"z\",\"y\",\"x\" for ZYX or \"x\",\"y\",\"z\" for XYZ). Default is None.</p> </li> <li> <code>fill_value</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Value to use for padding when patches extend beyond image boundaries. Default is 0.0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union['ZarrNii', List['ZarrNii']]</code>           \u2013            <p>Single ZarrNii instance (when centers is a single tuple) or list of</p> </li> <li> <code>Union['ZarrNii', List['ZarrNii']]</code>           \u2013            <p>ZarrNii instances (when centers is a list) with cropped data and</p> </li> <li> <code>Union['ZarrNii', List['ZarrNii']]</code>           \u2013            <p>updated spatial metadata. All patches will have exactly the shape</p> </li> <li> <code>Union['ZarrNii', List['ZarrNii']]</code>           \u2013            <p>specified by patch_size (plus any non-spatial dimensions).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If coordinates/dimensions are invalid</p> </li> <li> <code>IndexError</code>             \u2013            <p>If patch_size dimensions don't match spatial dimensions</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Extract single 256x256x256 voxel patch at a coordinate\n&gt;&gt;&gt; center = (50.0, 60.0, 70.0)  # physical coordinates in mm\n&gt;&gt;&gt; patch = znii.crop_centered(center, patch_size=(256, 256, 256))\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Extract multiple patches for ML training\n&gt;&gt;&gt; centers = [\n...     (50.0, 60.0, 70.0),\n...     (100.0, 110.0, 120.0),\n...     (150.0, 160.0, 170.0)\n... ]\n&gt;&gt;&gt; patches = znii.crop_centered(centers, patch_size=(128, 128, 128))\n&gt;&gt;&gt; # Returns list of 3 ZarrNii instances, all with shape (1, 128, 128, 128)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use with atlas sampling for ML training workflow\n&gt;&gt;&gt; centers = atlas.sample_region_patches(\n...     n_patches=100,\n...     region_ids=\"cortex\",\n...     seed=42\n... )\n&gt;&gt;&gt; patches = image.crop_centered(centers, patch_size=(256, 256, 256))\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use custom fill value for padding\n&gt;&gt;&gt; patch = znii.crop_centered(center, patch_size=(256, 256, 256), fill_value=-1.0)\n</code></pre> Notes <ul> <li>Centers are in physical/world coordinates (mm), always in (x, y, z) order</li> <li>patch_size is in voxels, in (x, y, z) order</li> <li>The patch is centered at the given coordinate, extending \u00b1patch_size/2</li> <li>If patch_size is odd, the center voxel is included</li> <li>Patches near boundaries are padded with fill_value to maintain size</li> <li>All patches are guaranteed to have exactly the requested size</li> <li>Useful for ML training where fixed patch sizes are required</li> <li>Coordinates from atlas.sample_region_patches() can be used directly</li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def crop_centered(\n    self,\n    centers: Union[Tuple[float, float, float], List[Tuple[float, float, float]]],\n    patch_size: Tuple[int, int, int],\n    spatial_dims: Optional[List[str]] = None,\n    fill_value: float = 0.0,\n) -&gt; Union[\"ZarrNii\", List[\"ZarrNii\"]]:\n    \"\"\"Extract fixed-size patches centered at specified coordinates.\n\n    Crops the image to extract patches of a fixed size (in voxels) centered\n    at the given physical coordinates. This is particularly useful for machine\n    learning workflows where training patches must have consistent dimensions.\n    The method can process a single center or multiple centers at once.\n\n    Patches that extend beyond image boundaries are padded with the fill_value\n    to ensure all patches have exactly the requested size.\n\n    Args:\n        centers: Either:\n            - Single center coordinate as (x, y, z) tuple in physical space (mm)\n            - List of center coordinates for batch processing\n        patch_size: Size of the patch in voxels as (x, y, z) tuple.\n            This defines the dimensions of each cropped region in voxel space.\n            All returned patches will have exactly this size.\n        spatial_dims: Names of spatial dimensions to crop. If None,\n            automatically derived from axes_order (\"z\",\"y\",\"x\" for ZYX\n            or \"x\",\"y\",\"z\" for XYZ). Default is None.\n        fill_value: Value to use for padding when patches extend beyond\n            image boundaries. Default is 0.0.\n\n    Returns:\n        Single ZarrNii instance (when centers is a single tuple) or list of\n        ZarrNii instances (when centers is a list) with cropped data and\n        updated spatial metadata. All patches will have exactly the shape\n        specified by patch_size (plus any non-spatial dimensions).\n\n    Raises:\n        ValueError: If coordinates/dimensions are invalid\n        IndexError: If patch_size dimensions don't match spatial dimensions\n\n    Examples:\n        &gt;&gt;&gt; # Extract single 256x256x256 voxel patch at a coordinate\n        &gt;&gt;&gt; center = (50.0, 60.0, 70.0)  # physical coordinates in mm\n        &gt;&gt;&gt; patch = znii.crop_centered(center, patch_size=(256, 256, 256))\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Extract multiple patches for ML training\n        &gt;&gt;&gt; centers = [\n        ...     (50.0, 60.0, 70.0),\n        ...     (100.0, 110.0, 120.0),\n        ...     (150.0, 160.0, 170.0)\n        ... ]\n        &gt;&gt;&gt; patches = znii.crop_centered(centers, patch_size=(128, 128, 128))\n        &gt;&gt;&gt; # Returns list of 3 ZarrNii instances, all with shape (1, 128, 128, 128)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Use with atlas sampling for ML training workflow\n        &gt;&gt;&gt; centers = atlas.sample_region_patches(\n        ...     n_patches=100,\n        ...     region_ids=\"cortex\",\n        ...     seed=42\n        ... )\n        &gt;&gt;&gt; patches = image.crop_centered(centers, patch_size=(256, 256, 256))\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Use custom fill value for padding\n        &gt;&gt;&gt; patch = znii.crop_centered(center, patch_size=(256, 256, 256), fill_value=-1.0)\n\n    Notes:\n        - Centers are in physical/world coordinates (mm), always in (x, y, z) order\n        - patch_size is in voxels, in (x, y, z) order\n        - The patch is centered at the given coordinate, extending \u00b1patch_size/2\n        - If patch_size is odd, the center voxel is included\n        - Patches near boundaries are padded with fill_value to maintain size\n        - All patches are guaranteed to have exactly the requested size\n        - Useful for ML training where fixed patch sizes are required\n        - Coordinates from atlas.sample_region_patches() can be used directly\n    \"\"\"\n    # Check if this is batch processing (list of centers)\n    is_batch = isinstance(centers, list)\n\n    if is_batch:\n        # Batch processing: recursively call crop_centered for each center\n        return [\n            self.crop_centered(center, patch_size, spatial_dims, fill_value)\n            for center in centers\n        ]\n\n    # Single center processing\n    if spatial_dims is None:\n        spatial_dims = (\n            [\"z\", \"y\", \"x\"] if self.axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n        )\n\n    # Convert center from physical to voxel coordinates\n    # Centers are always in (x, y, z) order\n    center_phys = np.array(list(centers) + [1.0])\n\n    # Get inverse affine to convert from physical to voxel\n    affine_inv = np.linalg.inv(self.get_affine_matrix(axes_order=\"XYZ\"))\n\n    # Transform to voxel coordinates\n    center_voxel = affine_inv @ center_phys\n    center_voxel_xyz = center_voxel[:3]\n\n    # patch_size is in voxels, in (x, y, z) order\n    patch_size_np = np.array(patch_size)\n    half_patch = patch_size_np / 2.0\n\n    # Calculate desired bounding box in voxel coordinates (may extend beyond image)\n    voxel_min_xyz = center_voxel_xyz - half_patch\n    voxel_max_xyz = center_voxel_xyz + half_patch\n\n    # Round to nearest integer voxel indices\n    voxel_min_xyz = np.round(voxel_min_xyz).astype(int)\n    voxel_max_xyz = np.round(voxel_max_xyz).astype(int)\n\n    # Ensure we get exactly the requested patch size\n    # Adjust max to ensure patch_size is respected\n    voxel_max_xyz = voxel_min_xyz + patch_size_np\n\n    # Get image dimensions in voxel space\n    # Map spatial dims to their indices\n    spatial_dim_indices = {}\n    for i, dim in enumerate(self.ngff_image.dims):\n        if dim.lower() in [d.lower() for d in spatial_dims]:\n            spatial_dim_indices[dim.lower()] = i\n\n    image_shape_xyz = np.array(\n        [\n            self.ngff_image.data.shape[spatial_dim_indices[\"x\"]],\n            self.ngff_image.data.shape[spatial_dim_indices[\"y\"]],\n            self.ngff_image.data.shape[spatial_dim_indices[\"z\"]],\n        ]\n    )\n\n    # Calculate the actual crop region (clipped to image bounds)\n    crop_min_xyz = np.maximum(voxel_min_xyz, 0)\n    crop_max_xyz = np.minimum(voxel_max_xyz, image_shape_xyz)\n\n    # Ensure crop_max &gt;= crop_min to avoid empty arrays\n    crop_max_xyz = np.maximum(crop_min_xyz, crop_max_xyz)\n\n    # Calculate padding needed on each side\n    pad_before_xyz = crop_min_xyz - voxel_min_xyz  # How much we're clipped at start\n    pad_after_xyz = voxel_max_xyz - crop_max_xyz  # How much we're clipped at end\n\n    # Check if the entire patch is outside the image bounds\n    # This happens when crop_min &gt;= crop_max in any dimension after clipping\n    is_completely_outside = np.any(crop_min_xyz &gt;= crop_max_xyz)\n\n    if is_completely_outside:\n        # The entire patch is outside the image bounds\n        # Create a completely padded array with the fill value\n        import dask.array as da\n\n        # Build the full patch shape\n        full_shape = []\n        spatial_idx = 0\n        for dim in self.ngff_image.dims:\n            if dim.lower() in [d.lower() for d in spatial_dims]:\n                full_shape.append(patch_size_np[spatial_idx])\n                spatial_idx += 1\n            else:\n                # Non-spatial dimension - keep original size\n                dim_idx = self.ngff_image.dims.index(dim)\n                full_shape.append(self.ngff_image.data.shape[dim_idx])\n\n        # Create array filled with fill_value\n        padded_data = da.full(\n            tuple(full_shape),\n            fill_value,\n            dtype=self.ngff_image.data.dtype,\n            chunks=self.ngff_image.data.chunksize,\n        )\n\n        # Calculate translation for the patch center\n        # The translation should be at voxel_min_xyz (the desired start of patch)\n        new_translation = {}\n        for dim in self.ngff_image.dims:\n            if dim.lower() in [d.lower() for d in spatial_dims]:\n                dim_lower = dim.lower()\n                if dim_lower == \"x\":\n                    voxel_start = voxel_min_xyz[0]\n                elif dim_lower == \"y\":\n                    voxel_start = voxel_min_xyz[1]\n                elif dim_lower == \"z\":\n                    voxel_start = voxel_min_xyz[2]\n                else:\n                    voxel_start = 0\n\n                # Translation is voxel_start * scale + original translation\n                new_translation[dim] = voxel_start * self.ngff_image.scale.get(\n                    dim, 1.0\n                ) + self.ngff_image.translation.get(dim, 0.0)\n            elif dim in self.ngff_image.translation:\n                new_translation[dim] = self.ngff_image.translation[dim]\n\n        # Create NgffImage with the padded data\n        padded_image = nz.NgffImage(\n            data=padded_data,\n            dims=self.ngff_image.dims,\n            scale=self.ngff_image.scale.copy(),\n            translation=new_translation,\n            name=self.ngff_image.name,\n        )\n\n        return ZarrNii(\n            ngff_image=padded_image,\n            axes_order=self.axes_order,\n            xyz_orientation=self.xyz_orientation,\n            _omero=self._omero,\n        )\n\n    # Create mapping from x,y,z to voxel coordinates for cropping\n    bbox_vox_min = {\n        \"x\": crop_min_xyz[0],\n        \"y\": crop_min_xyz[1],\n        \"z\": crop_min_xyz[2],\n    }\n    bbox_vox_max = {\n        \"x\": crop_max_xyz[0],\n        \"y\": crop_max_xyz[1],\n        \"z\": crop_max_xyz[2],\n    }\n\n    dim_flips = _axcodes2flips(self.orientation)\n    # Crop the actual image data that exists\n    cropped_image = crop_ngff_image(\n        self.ngff_image, bbox_vox_min, bbox_vox_max, dim_flips\n    )\n\n    # Check if padding is needed\n    needs_padding = np.any(pad_before_xyz &gt; 0) or np.any(pad_after_xyz &gt; 0)\n\n    if needs_padding:\n        # Build padding specification for all dimensions\n        pad_width = []\n        spatial_idx = 0\n\n        for dim in cropped_image.dims:\n            if dim.lower() in [d.lower() for d in spatial_dims]:\n                # Spatial dimension - may need padding\n                dim_lower = dim.lower()\n                if dim_lower == \"x\":\n                    pad_width.append((pad_before_xyz[0], pad_after_xyz[0]))\n                elif dim_lower == \"y\":\n                    pad_width.append((pad_before_xyz[1], pad_after_xyz[1]))\n                elif dim_lower == \"z\":\n                    pad_width.append((pad_before_xyz[2], pad_after_xyz[2]))\n                spatial_idx += 1\n            else:\n                # Non-spatial dimension - no padding\n                pad_width.append((0, 0))\n\n        # Apply padding\n        import dask.array as da\n\n        padded_data = da.pad(\n            cropped_image.data,\n            pad_width=pad_width,\n            mode=\"constant\",\n            constant_values=fill_value,\n        )\n\n        # Adjust translation for the padding\n        new_translation = cropped_image.translation.copy()\n\n        for i, dim in enumerate(bbox_vox_min.keys()):\n            new_translation[dim] = new_translation[dim] + dim_flips[\n                dim\n            ] * pad_before_xyz[i] * cropped_image.scale.get(dim, 1.0)\n\n        # Create padded NgffImage\n        cropped_image = nz.NgffImage(\n            data=padded_data,\n            dims=cropped_image.dims,\n            scale=cropped_image.scale,\n            translation=new_translation,\n            name=cropped_image.name,\n        )\n\n    return ZarrNii(\n        ngff_image=cropped_image,\n        axes_order=self.axes_order,\n        xyz_orientation=self.xyz_orientation,\n        _omero=self._omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.downsample","title":"<code>zarrnii.ZarrNii.downsample(factors=None, along_x=1, along_y=1, along_z=1, level=None, spatial_dims=None)</code>","text":"<p>Reduce image resolution by downsampling.</p> <p>Performs spatial downsampling by averaging blocks of voxels, effectively reducing image resolution and size. Multiple parameter options provide flexibility for different downsampling strategies.</p> <p>Parameters:</p> <ul> <li> <code>factors</code>               (<code>Optional[Union[int, List[int]]]</code>, default:                   <code>None</code> )           \u2013            <p>Downsampling factors for spatial dimensions. Can be: - int: Same factor applied to all spatial dimensions - List[int]: Per-dimension factors matching spatial_dims order - None: Use other parameters to determine factors</p> </li> <li> <code>along_x</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Downsampling factor for X dimension (legacy parameter)</p> </li> <li> <code>along_y</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Downsampling factor for Y dimension (legacy parameter)</p> </li> <li> <code>along_z</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Downsampling factor for Z dimension (legacy parameter)</p> </li> <li> <code>level</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Power-of-2 downsampling level (factors = 2^level). Takes precedence over along_* parameters</p> </li> <li> <code>spatial_dims</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Names of spatial dimensions. If None, derived from axes_order</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>'ZarrNii'</code>           \u2013            <p>New ZarrNii instance with downsampled data and updated metadata</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If conflicting parameters provided or invalid factors</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Isotropic downsampling by factor of 2\n&gt;&gt;&gt; downsampled = znii.downsample(factors=2)\n</code></pre> <pre><code>&gt;&gt;&gt; # Anisotropic downsampling\n&gt;&gt;&gt; downsampled = znii.downsample(factors=[1, 2, 2])\n</code></pre> <pre><code>&gt;&gt;&gt; # Using legacy parameters\n&gt;&gt;&gt; downsampled = znii.downsample(along_x=2, along_y=2, along_z=1)\n</code></pre> <pre><code>&gt;&gt;&gt; # Power-of-2 downsampling\n&gt;&gt;&gt; downsampled = znii.downsample(level=2)  # factors = 4\n</code></pre> Notes <ul> <li>Downsampling uses block averaging for anti-aliasing</li> <li>Spatial transformations are automatically scaled</li> <li>Non-spatial dimensions (channels, time) are preserved</li> <li>Original data remains unchanged (creates new instance)</li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def downsample(\n    self,\n    factors: Optional[Union[int, List[int]]] = None,\n    along_x: int = 1,\n    along_y: int = 1,\n    along_z: int = 1,\n    level: Optional[int] = None,\n    spatial_dims: Optional[List[str]] = None,\n) -&gt; \"ZarrNii\":\n    \"\"\"Reduce image resolution by downsampling.\n\n    Performs spatial downsampling by averaging blocks of voxels, effectively\n    reducing image resolution and size. Multiple parameter options provide\n    flexibility for different downsampling strategies.\n\n    Args:\n        factors: Downsampling factors for spatial dimensions. Can be:\n            - int: Same factor applied to all spatial dimensions\n            - List[int]: Per-dimension factors matching spatial_dims order\n            - None: Use other parameters to determine factors\n        along_x: Downsampling factor for X dimension (legacy parameter)\n        along_y: Downsampling factor for Y dimension (legacy parameter)\n        along_z: Downsampling factor for Z dimension (legacy parameter)\n        level: Power-of-2 downsampling level (factors = 2^level).\n            Takes precedence over along_* parameters\n        spatial_dims: Names of spatial dimensions. If None, derived\n            from axes_order\n\n    Returns:\n        New ZarrNii instance with downsampled data and updated metadata\n\n    Raises:\n        ValueError: If conflicting parameters provided or invalid factors\n\n    Examples:\n        &gt;&gt;&gt; # Isotropic downsampling by factor of 2\n        &gt;&gt;&gt; downsampled = znii.downsample(factors=2)\n\n        &gt;&gt;&gt; # Anisotropic downsampling\n        &gt;&gt;&gt; downsampled = znii.downsample(factors=[1, 2, 2])\n\n        &gt;&gt;&gt; # Using legacy parameters\n        &gt;&gt;&gt; downsampled = znii.downsample(along_x=2, along_y=2, along_z=1)\n\n        &gt;&gt;&gt; # Power-of-2 downsampling\n        &gt;&gt;&gt; downsampled = znii.downsample(level=2)  # factors = 4\n\n    Notes:\n        - Downsampling uses block averaging for anti-aliasing\n        - Spatial transformations are automatically scaled\n        - Non-spatial dimensions (channels, time) are preserved\n        - Original data remains unchanged (creates new instance)\n    \"\"\"\n    # Handle legacy parameters\n    if factors is None:\n        if level is not None:\n            factors = 2**level\n        else:\n            factors = (\n                [along_z, along_y, along_x]\n                if self.axes_order == \"ZYX\"\n                else [along_x, along_y, along_z]\n            )\n\n    if spatial_dims is None:\n        spatial_dims = (\n            [\"z\", \"y\", \"x\"] if self.axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n        )\n\n    downsampled_image = downsample_ngff_image(\n        self.ngff_image, factors, spatial_dims\n    )\n    return ZarrNii(\n        ngff_image=downsampled_image,\n        axes_order=self.axes_order,\n        xyz_orientation=self.xyz_orientation,\n        _omero=self._omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.upsample","title":"<code>zarrnii.ZarrNii.upsample(along_x=1, along_y=1, along_z=1, to_shape=None)</code>","text":"<p>Upsamples the ZarrNii instance using <code>scipy.ndimage.zoom</code>.</p> <p>Parameters:</p> <ul> <li> <code>along_x</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Upsampling factor along the X-axis (default: 1).</p> </li> <li> <code>along_y</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Upsampling factor along the Y-axis (default: 1).</p> </li> <li> <code>along_z</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Upsampling factor along the Z-axis (default: 1).</p> </li> <li> <code>to_shape</code>               (<code>tuple</code>, default:                   <code>None</code> )           \u2013            <p>Target shape for upsampling. Should include all dimensions                          (e.g., <code>(c, z, y, x)</code> for ZYX or <code>(c, x, y, z)</code> for XYZ).                          If provided, <code>along_x</code>, <code>along_y</code>, and <code>along_z</code> are ignored.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ZarrNii</code>          \u2013            <p>A new ZarrNii instance with the upsampled data and updated affine.</p> </li> </ul> Notes <ul> <li>This method supports both direct scaling via <code>along_*</code> factors or target shape via <code>to_shape</code>.</li> <li>If <code>to_shape</code> is provided, chunk sizes and scaling factors are dynamically calculated.</li> <li>The affine matrix is updated to reflect the new voxel size after upsampling.</li> </ul> Example Source code in <code>zarrnii/core.py</code> <pre><code>def upsample(self, along_x=1, along_y=1, along_z=1, to_shape=None):\n    \"\"\"\n    Upsamples the ZarrNii instance using `scipy.ndimage.zoom`.\n\n    Parameters:\n        along_x (int, optional): Upsampling factor along the X-axis (default: 1).\n        along_y (int, optional): Upsampling factor along the Y-axis (default: 1).\n        along_z (int, optional): Upsampling factor along the Z-axis (default: 1).\n        to_shape (tuple, optional): Target shape for upsampling. Should include all dimensions\n                                     (e.g., `(c, z, y, x)` for ZYX or `(c, x, y, z)` for XYZ).\n                                     If provided, `along_x`, `along_y`, and `along_z` are ignored.\n\n    Returns:\n        ZarrNii: A new ZarrNii instance with the upsampled data and updated affine.\n\n    Notes:\n        - This method supports both direct scaling via `along_*` factors or target shape via `to_shape`.\n        - If `to_shape` is provided, chunk sizes and scaling factors are dynamically calculated.\n        - The affine matrix is updated to reflect the new voxel size after upsampling.\n\n    Example:\n        # Upsample with scaling factors\n        upsampled_znimg = znimg.upsample(along_x=2, along_y=2, along_z=2)\n\n        # Upsample to a specific shape\n        upsampled_znimg = znimg.upsample(to_shape=(1, 256, 256, 256))\n    \"\"\"\n    # Determine scaling and chunks based on input parameters\n    if to_shape is None:\n        if self.axes_order == \"XYZ\":\n            scaling = (1, along_x, along_y, along_z)\n        else:\n            scaling = (1, along_z, along_y, along_x)\n\n        chunks_out = tuple(\n            tuple(c * scale for c in chunks_i)\n            for chunks_i, scale in zip(self.data.chunks, scaling)\n        )\n    else:\n        chunks_out, scaling = self.__get_upsampled_chunks(to_shape)\n\n    # Define block-wise upsampling function\n    def zoom_blocks(x, block_info=None):\n        \"\"\"\n        Scales blocks to the desired size using `scipy.ndimage.zoom`.\n\n        Parameters:\n            x (np.ndarray): Input block data.\n            block_info (dict, optional): Metadata about the current block.\n\n        Returns:\n            np.ndarray: The upscaled block.\n        \"\"\"\n        # Calculate scaling factors based on input and output chunk shapes\n        scaling = tuple(\n            out_n / in_n\n            for out_n, in_n in zip(block_info[None][\"chunk-shape\"], x.shape)\n        )\n        return zoom(x, scaling, order=1, prefilter=False)\n\n    # Perform block-wise upsampling\n    darr_scaled = da.map_blocks(\n        zoom_blocks, self.data, dtype=self.data.dtype, chunks=chunks_out\n    )\n\n    # Update the affine matrix to reflect the new voxel size\n    if self.axes_order == \"XYZ\":\n        scaling_matrix = np.diag(\n            (1 / scaling[1], 1 / scaling[2], 1 / scaling[3], 1)\n        )\n    else:\n        scaling_matrix = np.diag(\n            (1 / scaling[-1], 1 / scaling[-2], 1 / scaling[-3], 1)\n        )\n\n    # Create new NgffImage with upsampled data\n    dims = self.dims\n    if self.axes_order == \"XYZ\":\n        new_scale = {\n            dims[1]: self.scale[dims[1]] / scaling[1],\n            dims[2]: self.scale[dims[2]] / scaling[2],\n            dims[3]: self.scale[dims[3]] / scaling[3],\n        }\n    else:\n        new_scale = {\n            dims[1]: self.scale[dims[1]] / scaling[1],\n            dims[2]: self.scale[dims[2]] / scaling[2],\n            dims[3]: self.scale[dims[3]] / scaling[3],\n        }\n\n    upsampled_ngff = nz.to_ngff_image(\n        darr_scaled,\n        dims=dims,\n        scale=new_scale,\n        translation=self.translation.copy(),\n        name=self.name,\n    )\n\n    # Return a new ZarrNii instance with the upsampled data\n    return ZarrNii.from_ngff_image(\n        upsampled_ngff,\n        axes_order=self.axes_order,\n        xyz_orientation=self.xyz_orientation,\n        omero=self.omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.upsample--upsample-with-scaling-factors","title":"Upsample with scaling factors","text":"<p>upsampled_znimg = znimg.upsample(along_x=2, along_y=2, along_z=2)</p>"},{"location":"reference/#zarrnii.ZarrNii.upsample--upsample-to-a-specific-shape","title":"Upsample to a specific shape","text":"<p>upsampled_znimg = znimg.upsample(to_shape=(1, 256, 256, 256))</p>"},{"location":"reference/#zarrnii.ZarrNii.get_bounded_subregion","title":"<code>zarrnii.ZarrNii.get_bounded_subregion(points)</code>","text":"<p>Extracts a bounded subregion of the dask array containing the specified points, along with the grid points for interpolation.</p> <p>If the points extend beyond the domain of the dask array, the extent is capped at the boundaries. If all points are outside the domain, the function returns <code>(None, None)</code>.</p> <p>Parameters:</p> <ul> <li> <code>points</code>               (<code>ndarray</code>)           \u2013            <p>Nx3 or Nx4 array of coordinates in the array's space.                  If Nx4, the last column is assumed to be the homogeneous                  coordinate and is ignored.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple</code>          \u2013            <p>grid_points (tuple): A tuple of three 1D arrays representing the grid                      points along each axis (X, Y, Z) in the subregion. subvol (np.ndarray or None): The extracted subregion as a NumPy array.                              Returns <code>None</code> if all points are outside                              the array domain.</p> </li> </ul> Notes <ul> <li>The function uses <code>compute()</code> on the dask array to immediately load the   subregion, as Dask doesn't support the type of indexing required for   interpolation.</li> <li>A padding of 1 voxel is applied around the extent of the points.</li> </ul> Example <p>grid_points, subvol = znimg.get_bounded_subregion(points) if subvol is not None:     print(\"Subvolume shape:\", subvol.shape)</p> Source code in <code>zarrnii/core.py</code> <pre><code>def get_bounded_subregion(self, points: np.ndarray):\n    \"\"\"\n    Extracts a bounded subregion of the dask array containing the specified points,\n    along with the grid points for interpolation.\n\n    If the points extend beyond the domain of the dask array, the extent is capped\n    at the boundaries. If all points are outside the domain, the function returns\n    `(None, None)`.\n\n    Parameters:\n        points (np.ndarray): Nx3 or Nx4 array of coordinates in the array's space.\n                             If Nx4, the last column is assumed to be the homogeneous\n                             coordinate and is ignored.\n\n    Returns:\n        tuple:\n            grid_points (tuple): A tuple of three 1D arrays representing the grid\n                                 points along each axis (X, Y, Z) in the subregion.\n            subvol (np.ndarray or None): The extracted subregion as a NumPy array.\n                                         Returns `None` if all points are outside\n                                         the array domain.\n\n    Notes:\n        - The function uses `compute()` on the dask array to immediately load the\n          subregion, as Dask doesn't support the type of indexing required for\n          interpolation.\n        - A padding of 1 voxel is applied around the extent of the points.\n\n    Example:\n        grid_points, subvol = znimg.get_bounded_subregion(points)\n        if subvol is not None:\n            print(\"Subvolume shape:\", subvol.shape)\n    \"\"\"\n    pad = 1  # Padding around the extent of the points\n\n    # Compute the extent of the points in the array's coordinate space\n    min_extent = np.floor(points.min(axis=1)[:3] - pad).astype(\"int\")\n    max_extent = np.ceil(points.max(axis=1)[:3] + pad).astype(\"int\")\n\n    # Clip the extents to ensure they stay within the bounds of the array\n    clip_min = np.zeros_like(min_extent)\n    clip_max = np.array(self.darr.shape[-3:])  # Z, Y, X dimensions\n\n    min_extent = np.clip(min_extent, clip_min, clip_max)\n    max_extent = np.clip(max_extent, clip_min, clip_max)\n\n    # Check if all points are outside the domain\n    if np.any(max_extent &lt;= min_extent):\n        return None, None\n\n    # Extract the subvolume using the computed extents\n    subvol = self.darr[\n        :,\n        min_extent[0] : max_extent[0],\n        min_extent[1] : max_extent[1],\n        min_extent[2] : max_extent[2],\n    ].compute()\n\n    # Generate grid points for interpolation\n    grid_points = (\n        np.arange(min_extent[0], max_extent[0]),  # Z\n        np.arange(min_extent[1], max_extent[1]),  # Y\n        np.arange(min_extent[2], max_extent[2]),  # X\n    )\n\n    return grid_points, subvol\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.apply_transform","title":"<code>zarrnii.ZarrNii.apply_transform(*transforms, ref_znimg, spatial_dims=None)</code>","text":"<p>Apply spatial transformations to image data.</p> <p>Transforms the image data to align with a reference image space using the provided transformation(s). This enables registration, resampling, and coordinate system conversions.</p> <p>Parameters:</p> <ul> <li> <code>*transforms</code>               (<code>Transform</code>, default:                   <code>()</code> )           \u2013            <p>Variable number of Transform objects to apply sequentially. Supported transform types: - AffineTransform: Linear transformations (rotation, scaling, translation) - DisplacementTransform: Non-linear deformation fields</p> </li> <li> <code>ref_znimg</code>               (<code>'ZarrNii'</code>)           \u2013            <p>Reference ZarrNii image defining the target coordinate system, grid spacing, and field of view for the output</p> </li> <li> <code>spatial_dims</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Names of spatial dimensions for transformation. If None, automatically derived from axes_order</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>'ZarrNii'</code>           \u2013            <p>New ZarrNii instance with transformed data in reference space</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no transforms provided or reference image incompatible</p> </li> <li> <code>TypeError</code>             \u2013            <p>If transforms are not valid Transform objects</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Apply affine transformation\n&gt;&gt;&gt; affine = AffineTransform.from_txt(\"transform.txt\")\n&gt;&gt;&gt; transformed = moving.apply_transform(affine, ref_znimg=reference)\n</code></pre> <pre><code>&gt;&gt;&gt; # Apply multiple transforms sequentially\n&gt;&gt;&gt; affine = AffineTransform.identity()\n&gt;&gt;&gt; warp = DisplacementTransform.from_nifti(\"warp.nii.gz\")\n&gt;&gt;&gt; result = moving.apply_transform(affine, warp, ref_znimg=reference)\n</code></pre> Notes <ul> <li>Transformations are applied in the order specified</li> <li>Output data inherits spatial properties from ref_znimg</li> <li>Uses interpolation for non-integer coordinate mappings</li> <li>Non-spatial dimensions (channels, time) are preserved</li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def apply_transform(\n    self,\n    *transforms: Transform,\n    ref_znimg: \"ZarrNii\",\n    spatial_dims: Optional[List[str]] = None,\n) -&gt; \"ZarrNii\":\n    \"\"\"Apply spatial transformations to image data.\n\n    Transforms the image data to align with a reference image space using\n    the provided transformation(s). This enables registration, resampling,\n    and coordinate system conversions.\n\n    Args:\n        *transforms: Variable number of Transform objects to apply sequentially.\n            Supported transform types:\n            - AffineTransform: Linear transformations (rotation, scaling, translation)\n            - DisplacementTransform: Non-linear deformation fields\n        ref_znimg: Reference ZarrNii image defining the target coordinate system,\n            grid spacing, and field of view for the output\n        spatial_dims: Names of spatial dimensions for transformation. If None,\n            automatically derived from axes_order\n\n    Returns:\n        New ZarrNii instance with transformed data in reference space\n\n    Raises:\n        ValueError: If no transforms provided or reference image incompatible\n        TypeError: If transforms are not valid Transform objects\n\n    Examples:\n        &gt;&gt;&gt; # Apply affine transformation\n        &gt;&gt;&gt; affine = AffineTransform.from_txt(\"transform.txt\")\n        &gt;&gt;&gt; transformed = moving.apply_transform(affine, ref_znimg=reference)\n\n        &gt;&gt;&gt; # Apply multiple transforms sequentially\n        &gt;&gt;&gt; affine = AffineTransform.identity()\n        &gt;&gt;&gt; warp = DisplacementTransform.from_nifti(\"warp.nii.gz\")\n        &gt;&gt;&gt; result = moving.apply_transform(affine, warp, ref_znimg=reference)\n\n    Notes:\n        - Transformations are applied in the order specified\n        - Output data inherits spatial properties from ref_znimg\n        - Uses interpolation for non-integer coordinate mappings\n        - Non-spatial dimensions (channels, time) are preserved\n    \"\"\"\n    if spatial_dims is None:\n        spatial_dims = (\n            [\"z\", \"y\", \"x\"] if self.axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n        )\n\n    # Initialize the list of transformations to apply\n    tfms_to_apply = [ref_znimg.affine]  # Start with the reference image affine\n\n    # Append all transformations passed as arguments\n    tfms_to_apply.extend(transforms)\n\n    # Append the inverse of the current image's affine\n    tfms_to_apply.append(self.affine.invert())\n\n    # Create new NgffImage from ref image\n    interp_ngff_image = nz.NgffImage(\n        data=ref_znimg.data,\n        dims=ref_znimg.ngff_image.dims.copy(),\n        scale=ref_znimg.ngff_image.scale.copy(),\n        translation=ref_znimg.ngff_image.translation.copy(),\n        name=f\"{self.name}_transformed_to_{ref_znimg.name}\",\n    )\n\n    # Try to get zarr store information for direct access (avoids nested compute)\n    store_info = self.get_zarr_store_info()\n\n    # Lazily apply the transformations using dask\n    if store_info is not None:\n        # Use direct zarr access to avoid nested compute() calls\n        interp_ngff_image.data = da.map_blocks(\n            interp_by_block,  # Function to interpolate each block\n            ref_znimg.data,  # Reference image data\n            dtype=np.float32,  # Output data type\n            transforms=tfms_to_apply,  # Transformations to apply\n            flo_store_path=store_info[\"store_path\"],\n            flo_array_shape=store_info[\"array_shape\"],\n            flo_dataset_path=store_info[\"dataset_path\"],\n            flo_storage_options=None,  # TODO: Extract from dask array if available\n        )\n    else:\n        # Fall back to passing ZarrNii instance (legacy behavior with nested compute)\n        interp_ngff_image.data = da.map_blocks(\n            interp_by_block,  # Function to interpolate each block\n            ref_znimg.data,  # Reference image data\n            dtype=np.float32,  # Output data type\n            transforms=tfms_to_apply,  # Transformations to apply\n            flo_znimg=self,  # Floating image to align (legacy)\n        )\n\n    return ZarrNii.from_ngff_image(\n        interp_ngff_image,\n        axes_order=ref_znimg.axes_order,\n        xyz_orientation=ref_znimg.xyz_orientation,\n        omero=self.omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.to_ome_zarr","title":"<code>zarrnii.ZarrNii.to_ome_zarr(store_or_path, max_layer=4, scale_factors=None, backend='ome-zarr-py', **kwargs)</code>","text":"<p>Save to OME-Zarr store with multiscale pyramid.</p> <p>Creates an OME-Zarr dataset with automatic multiscale pyramid generation for efficient visualization and processing at multiple resolutions. Preserves spatial metadata and supports various storage backends.</p> <p>Parameters:</p> <ul> <li> <code>store_or_path</code>               (<code>Union[str, Any]</code>)           \u2013            <p>Target location for OME-Zarr store. Supports: - Local directory path - Remote URLs (s3://, gs://, etc.) - ZIP files (.zip extension for compressed storage) - Zarr store objects</p> </li> <li> <code>max_layer</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Maximum number of pyramid levels to create (including level 0). Higher values create more downsampled levels</p> </li> <li> <code>scale_factors</code>               (<code>Optional[List[int]]</code>, default:                   <code>None</code> )           \u2013            <p>Custom downsampling factors for each pyramid level. If None, uses powers of 2: [2, 4, 8, 16, ...]</p> </li> <li> <code>backend</code>               (<code>str</code>, default:                   <code>'ome-zarr-py'</code> )           \u2013            <p>Backend library to use for writing. Options: - 'ngff-zarr': Use ngff-zarr library (default) - 'ome-zarr-py': Use ome-zarr-py library for better dask integration</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments passed to the save function. For 'ngff-zarr': passed to to_ngff_zarr function For 'ome-zarr-py': passed to write_image (e.g., scaling_method, compute)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>'ZarrNii'</code>           \u2013            <p>Self for method chaining</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>OSError</code>             \u2013            <p>If unable to write to target location</p> </li> <li> <code>ValueError</code>             \u2013            <p>If invalid scale_factors or backend provided</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Save with default pyramid levels\n&gt;&gt;&gt; znii.to_ome_zarr(\"/path/to/output.zarr\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Save to compressed ZIP with custom pyramid\n&gt;&gt;&gt; znii.to_ome_zarr(\n...     \"/path/to/output.zarr.zip\",\n...     max_layer=3,\n...     scale_factors=[2, 4]\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Use ome-zarr-py backend for better dask performance\n&gt;&gt;&gt; znii.to_ome_zarr(\n...     \"/path/to/output.zarr\",\n...     backend=\"ome-zarr-py\",\n...     scaling_method=\"gaussian\"\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Chain with other operations\n&gt;&gt;&gt; result = (znii.downsample(2)\n...               .crop((0,0,0), (100,100,100))\n...               .to_ome_zarr(\"processed.zarr\"))\n</code></pre> Notes <ul> <li>OME-Zarr files are always saved in ZYX axis order</li> <li>Automatic axis reordering if current order is XYZ</li> <li>Spatial transformations and metadata are preserved</li> <li>Orientation information is stored using the new 'xyz_orientation'   metadata key for consistency and future compatibility</li> <li>The 'ome-zarr-py' backend provides better performance with dask   and dask distributed workflows</li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def to_ome_zarr(\n    self,\n    store_or_path: Union[str, Any],\n    max_layer: int = 4,\n    scale_factors: Optional[List[int]] = None,\n    backend: str = \"ome-zarr-py\",\n    **kwargs: Any,\n) -&gt; \"ZarrNii\":\n    \"\"\"Save to OME-Zarr store with multiscale pyramid.\n\n    Creates an OME-Zarr dataset with automatic multiscale pyramid generation\n    for efficient visualization and processing at multiple resolutions.\n    Preserves spatial metadata and supports various storage backends.\n\n    Args:\n        store_or_path: Target location for OME-Zarr store. Supports:\n            - Local directory path\n            - Remote URLs (s3://, gs://, etc.)\n            - ZIP files (.zip extension for compressed storage)\n            - Zarr store objects\n        max_layer: Maximum number of pyramid levels to create (including level 0).\n            Higher values create more downsampled levels\n        scale_factors: Custom downsampling factors for each pyramid level.\n            If None, uses powers of 2: [2, 4, 8, 16, ...]\n        backend: Backend library to use for writing. Options:\n            - 'ngff-zarr': Use ngff-zarr library (default)\n            - 'ome-zarr-py': Use ome-zarr-py library for better dask integration\n        **kwargs: Additional arguments passed to the save function.\n            For 'ngff-zarr': passed to to_ngff_zarr function\n            For 'ome-zarr-py': passed to write_image (e.g., scaling_method, compute)\n\n    Returns:\n        Self for method chaining\n\n    Raises:\n        OSError: If unable to write to target location\n        ValueError: If invalid scale_factors or backend provided\n\n    Examples:\n        &gt;&gt;&gt; # Save with default pyramid levels\n        &gt;&gt;&gt; znii.to_ome_zarr(\"/path/to/output.zarr\")\n\n        &gt;&gt;&gt; # Save to compressed ZIP with custom pyramid\n        &gt;&gt;&gt; znii.to_ome_zarr(\n        ...     \"/path/to/output.zarr.zip\",\n        ...     max_layer=3,\n        ...     scale_factors=[2, 4]\n        ... )\n\n        &gt;&gt;&gt; # Use ome-zarr-py backend for better dask performance\n        &gt;&gt;&gt; znii.to_ome_zarr(\n        ...     \"/path/to/output.zarr\",\n        ...     backend=\"ome-zarr-py\",\n        ...     scaling_method=\"gaussian\"\n        ... )\n\n        &gt;&gt;&gt; # Chain with other operations\n        &gt;&gt;&gt; result = (znii.downsample(2)\n        ...               .crop((0,0,0), (100,100,100))\n        ...               .to_ome_zarr(\"processed.zarr\"))\n\n    Notes:\n        - OME-Zarr files are always saved in ZYX axis order\n        - Automatic axis reordering if current order is XYZ\n        - Spatial transformations and metadata are preserved\n        - Orientation information is stored using the new 'xyz_orientation'\n          metadata key for consistency and future compatibility\n        - The 'ome-zarr-py' backend provides better performance with dask\n          and dask distributed workflows\n    \"\"\"\n    # Validate backend parameter\n    if backend not in [\"ngff-zarr\", \"ome-zarr-py\"]:\n        raise ValueError(\n            f\"Invalid backend '{backend}'. Must be 'ngff-zarr' or 'ome-zarr-py'\"\n        )\n\n    # Determine the image to save\n    if self.axes_order == \"XYZ\":\n        # Need to reorder data from XYZ to ZYX for OME-Zarr\n        ngff_image_to_save = self._create_zyx_ngff_image()\n    else:\n        # Already in ZYX order\n        ngff_image_to_save = self.ngff_image\n\n    # Use the appropriate save function based on backend\n    if backend == \"ngff-zarr\":\n        save_ngff_image(\n            ngff_image_to_save,\n            store_or_path,\n            max_layer,\n            scale_factors,\n            xyz_orientation=(\n                self.xyz_orientation if hasattr(self, \"xyz_orientation\") else None\n            ),\n            **kwargs,\n        )\n    elif backend == \"ome-zarr-py\":\n        save_ngff_image_with_ome_zarr(\n            ngff_image_to_save,\n            store_or_path,\n            max_layer,\n            scale_factors,\n            omero=self._omero,\n            xyz_orientation=(\n                self.xyz_orientation if hasattr(self, \"xyz_orientation\") else None\n            ),\n            **kwargs,\n        )\n\n    # Add orientation metadata to the zarr store (only for non-ZIP files)\n    # For ZIP files, orientation is handled inside save_ngff_image\n    if not (isinstance(store_or_path, str) and store_or_path.endswith(\".zip\")):\n        try:\n            import zarr\n\n            if isinstance(store_or_path, str):\n                group = zarr.open_group(store_or_path, mode=\"r+\")\n            else:\n                group = zarr.open_group(store_or_path, mode=\"r+\")\n\n            # Add metadata for xyz_orientation (new format)\n            if hasattr(self, \"xyz_orientation\") and self.xyz_orientation:\n                group.attrs[\"xyz_orientation\"] = self.xyz_orientation\n        except Exception:\n            # If we can't write orientation metadata, that's not critical\n            pass\n\n    return self\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.to_nifti","title":"<code>zarrnii.ZarrNii.to_nifti(filename=None)</code>","text":"<p>Convert to NIfTI format with automatic dimension handling.</p> <p>Converts the ZarrNii image to NIfTI-1 format, handling dimension reordering, singleton dimension removal, and spatial transformation conversion. NIfTI files are always written in XYZ axis order.</p> <p>For multi-channel data, the 4th dimension is used for channels (XYZC), and channel labels are preserved in NIfTI header extensions.</p> <p>Parameters:</p> <ul> <li> <code>filename</code>               (<code>Optional[Union[str, bytes]]</code>, default:                   <code>None</code> )           \u2013            <p>Output file path for saving. Supported extensions: - .nii: Uncompressed NIfTI - .nii.gz: Compressed NIfTI (recommended) If None, returns nibabel image object without saving</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[Nifti1Image, str]</code>           \u2013            <p>If filename is None: nibabel.Nifti1Image object</p> </li> <li> <code>Union[Nifti1Image, str]</code>           \u2013            <p>If filename provided: path to saved file</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If data has non-singleton time dimension (time is not supported in NIfTI output, but multiple channels are supported)</p> </li> <li> <code>OSError</code>             \u2013            <p>If unable to write to specified filename</p> </li> </ul> Notes <ul> <li>Automatically reorders data from ZYX to XYZ if necessary</li> <li>Removes singleton time dimensions automatically</li> <li>Supports multi-channel data via 4th dimension (XYZC ordering)</li> <li>Channel labels are saved in NIfTI header extensions as JSON</li> <li>Spatial transformations are converted to NIfTI affine format</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Save to compressed NIfTI file\n&gt;&gt;&gt; znii.to_nifti(\"output.nii.gz\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Get nibabel object without saving\n&gt;&gt;&gt; nifti_img = znii.to_nifti()\n&gt;&gt;&gt; print(nifti_img.shape)\n</code></pre> <pre><code>&gt;&gt;&gt; # Save multi-channel data with channel labels preserved\n&gt;&gt;&gt; znii.to_nifti(\"multichannel.nii.gz\")\n&gt;&gt;&gt; # Channel labels are automatically saved in header extensions\n</code></pre> <pre><code>&gt;&gt;&gt; # Select specific channels before saving\n&gt;&gt;&gt; znii.select_channels([0, 2]).to_nifti(\"selected.nii.gz\")\n</code></pre> Source code in <code>zarrnii/core.py</code> <pre><code>def to_nifti(\n    self, filename: Optional[Union[str, bytes]] = None\n) -&gt; Union[nib.Nifti1Image, str]:\n    \"\"\"Convert to NIfTI format with automatic dimension handling.\n\n    Converts the ZarrNii image to NIfTI-1 format, handling dimension\n    reordering, singleton dimension removal, and spatial transformation\n    conversion. NIfTI files are always written in XYZ axis order.\n\n    For multi-channel data, the 4th dimension is used for channels (XYZC),\n    and channel labels are preserved in NIfTI header extensions.\n\n    Args:\n        filename: Output file path for saving. Supported extensions:\n            - .nii: Uncompressed NIfTI\n            - .nii.gz: Compressed NIfTI (recommended)\n            If None, returns nibabel image object without saving\n\n    Returns:\n        If filename is None: nibabel.Nifti1Image object\n        If filename provided: path to saved file\n\n    Raises:\n        ValueError: If data has non-singleton time dimension (time is not\n            supported in NIfTI output, but multiple channels are supported)\n        OSError: If unable to write to specified filename\n\n    Notes:\n        - Automatically reorders data from ZYX to XYZ if necessary\n        - Removes singleton time dimensions automatically\n        - Supports multi-channel data via 4th dimension (XYZC ordering)\n        - Channel labels are saved in NIfTI header extensions as JSON\n        - Spatial transformations are converted to NIfTI affine format\n\n    Examples:\n        &gt;&gt;&gt; # Save to compressed NIfTI file\n        &gt;&gt;&gt; znii.to_nifti(\"output.nii.gz\")\n\n        &gt;&gt;&gt; # Get nibabel object without saving\n        &gt;&gt;&gt; nifti_img = znii.to_nifti()\n        &gt;&gt;&gt; print(nifti_img.shape)\n\n        &gt;&gt;&gt; # Save multi-channel data with channel labels preserved\n        &gt;&gt;&gt; znii.to_nifti(\"multichannel.nii.gz\")\n        &gt;&gt;&gt; # Channel labels are automatically saved in header extensions\n\n        &gt;&gt;&gt; # Select specific channels before saving\n        &gt;&gt;&gt; znii.select_channels([0, 2]).to_nifti(\"selected.nii.gz\")\n\n    Warnings:\n        Large images will be computed in memory during conversion.\n        Consider downsampling or cropping first for very large datasets.\n    \"\"\"\n    # Get data and dimensions\n    data = self.data.compute()\n\n    dims = self.dims\n\n    # Handle dimensional reduction for NIfTI compatibility\n    # NIfTI supports up to 4D, and we use 4th dimension for channels (XYZC)\n    squeeze_axes = []\n    new_dims = []\n\n    for i, dim in enumerate(dims):\n        if dim == \"t\" and data.shape[i] == 1:\n            # Remove singleton time dimension\n            squeeze_axes.append(i)\n        elif dim == \"t\" and data.shape[i] &gt; 1:\n            # Non-singleton time dimension - not supported\n            raise ValueError(\n                f\"NIfTI format doesn't support non-singleton time dimension. \"\n                f\"Dimension 't' has size {data.shape[i]}. \"\n                f\"Consider selecting a specific timepoint first.\"\n            )\n        elif dim == \"c\" and data.shape[i] == 1:\n            # Singleton channel - can be squeezed\n            squeeze_axes.append(i)\n            # Don't add to new_dims\n        else:\n            # Keep this dimension (spatial or multi-channel)\n            new_dims.append(dim)\n\n    # Squeeze out singleton dimensions\n    if squeeze_axes:\n        data = np.squeeze(data, axis=tuple(squeeze_axes))\n\n    # Check final dimensionality\n    if data.ndim &gt; 4:\n        raise ValueError(\n            f\"Resulting data has {data.ndim} dimensions, but NIfTI supports maximum 4D\"\n        )\n\n    # Now handle spatial reordering based on axes_order\n    # We need to reorder to XYZC for NIfTI (or XYZ for 3D)\n    if self.axes_order == \"ZYX\":\n        # Data spatial dimensions are in ZYX order, need to transpose to XYZ\n        if data.ndim == 3:\n            # Pure spatial data: ZYX -&gt; XYZ\n            data = data.transpose(2, 1, 0)\n        elif data.ndim == 4:\n            # Check what the dimension order is\n            if new_dims == [\"c\", \"z\", \"y\", \"x\"]:\n                # CZYX -&gt; XYZC\n                data = data.transpose(3, 2, 1, 0)\n            elif new_dims == [\"z\", \"y\", \"x\", \"c\"]:\n                # ZYXC -&gt; XYZC\n                data = data.transpose(2, 1, 0, 3)\n            else:\n                # Fallback: assume CZYX\n                data = data.transpose(3, 2, 1, 0)\n\n        # Get affine matrix in XYZ order\n        affine_matrix = self.get_affine_matrix(axes_order=\"XYZ\")\n    else:\n        # Data is in XYZ order\n        if data.ndim == 3:\n            # Pure spatial data: XYZ (no change needed)\n            pass\n        elif data.ndim == 4:\n            # Check what the dimension order is\n            if new_dims == [\"c\", \"x\", \"y\", \"z\"]:\n                # CXYZ -&gt; XYZC\n                data = data.transpose(1, 2, 3, 0)\n            elif new_dims == [\"x\", \"y\", \"z\", \"c\"]:\n                # XYZC -&gt; XYZC (already correct!)\n                pass\n            else:\n                # Fallback: assume CXYZ\n                data = data.transpose(1, 2, 3, 0)\n\n        affine_matrix = self.get_affine_matrix(axes_order=\"XYZ\")\n\n    # Create NIfTI image\n    nifti_img = nib.Nifti1Image(data, affine_matrix)\n\n    # Add channel labels to NIfTI header extensions if available\n    channel_labels = self.list_channels()\n    if channel_labels and len(channel_labels) &gt; 0 and data.ndim == 4:\n        # Only add channel labels if we have multi-channel 4D data\n        import json\n\n        channel_metadata = {\"channel_labels\": channel_labels}\n        ext = nib.nifti1.Nifti1Extension(\n            1,\n            json.dumps(channel_metadata).encode(\n                \"utf-8\"\n            ),  # code 1 is reserved/unspecified in NIfTI standard, suitable for custom metadata\n        )\n        nifti_img.header.extensions.append(ext)\n\n    if filename is not None:\n        nib.save(nifti_img, filename)\n        return filename\n    else:\n        return nifti_img\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.to_tiff_stack","title":"<code>zarrnii.ZarrNii.to_tiff_stack(filename_pattern, channel=None, timepoint=None, compress=True, dtype='uint16', rescale=True)</code>","text":"<p>Save data as a stack of 2D TIFF images.</p> <p>Saves the image data as a series of 2D TIFF files, with each Z-slice saved as a separate file. This format is useful for compatibility with tools that don't support OME-Zarr or napari plugins that require individual TIFF files.</p> <p>Parameters:</p> <ul> <li> <code>filename_pattern</code>               (<code>str</code>)           \u2013            <p>Output filename pattern. Should contain '{z:04d}' or similar format specifier for the Z-slice number. Examples: - \"output_z{z:04d}.tif\" - \"data/slice_{z:03d}.tiff\" If pattern doesn't contain format specifier, '_{z:04d}' is appended before the extension.</p> </li> <li> <code>channel</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Channel index to save (0-based). If None and data has multiple channels, all channels will be saved as separate channel dimensions in each TIFF file (multi-channel TIFFs).</p> </li> <li> <code>timepoint</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Timepoint index to save (0-based). If None and data has multiple timepoints, raises ValueError (must select single timepoint).</p> </li> <li> <code>compress</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use LZW compression (default: True)</p> </li> <li> <code>dtype</code>               (<code>Optional[str]</code>, default:                   <code>'uint16'</code> )           \u2013            <p>Output data type for TIFF files. Options: - 'uint8': 8-bit unsigned integer (0-255) - 'uint16': 16-bit unsigned integer (0-65535) [default] - 'int16': 16-bit signed integer (-32768 to 32767) - 'float32': 32-bit float (preserves original data) Default 'uint16' provides good range and compatibility.</p> </li> <li> <code>rescale</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to rescale data to fit the output dtype range. If True, data is linearly scaled from [min, max] to the full range of the output dtype. If False, data is clipped to the output dtype range. Default: True</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Base directory path where files were saved</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If data has multiple timepoints but none selected, or if selected channel/timepoint is out of range, or if dtype is not supported</p> </li> <li> <code>OSError</code>             \u2013            <p>If unable to write to specified directory</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Save as 16-bit with auto-rescaling (default, recommended)\n&gt;&gt;&gt; znii.to_tiff_stack(\"output_z{z:04d}.tif\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Save as 8-bit for smaller file sizes\n&gt;&gt;&gt; znii.to_tiff_stack(\"output_z{z:04d}.tif\", dtype='uint8')\n</code></pre> <pre><code>&gt;&gt;&gt; # Save specific channel without rescaling\n&gt;&gt;&gt; znii.to_tiff_stack(\"channel0_z{z:04d}.tif\", channel=0, rescale=False)\n</code></pre> <pre><code>&gt;&gt;&gt; # Save as float32 to preserve original precision\n&gt;&gt;&gt; znii.to_tiff_stack(\"precise_z{z:04d}.tif\", dtype='float32')\n</code></pre> Notes <ul> <li>Z-dimension becomes the stack (file) dimension</li> <li>Time and channel dimensions are handled as specified</li> <li>Spatial transformations are not preserved in TIFF format</li> <li>For 5D data (T,C,Z,Y,X), you must select a single timepoint</li> <li>Multi-channel data can be saved as multi-channel TIFFs or selected</li> <li>Data type conversion helps ensure compatibility with analysis tools</li> <li>uint16 is recommended for most scientific applications (good range + compatibility)</li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def to_tiff_stack(\n    self,\n    filename_pattern: str,\n    channel: Optional[int] = None,\n    timepoint: Optional[int] = None,\n    compress: bool = True,\n    dtype: Optional[str] = \"uint16\",\n    rescale: bool = True,\n) -&gt; str:\n    \"\"\"Save data as a stack of 2D TIFF images.\n\n    Saves the image data as a series of 2D TIFF files, with each Z-slice\n    saved as a separate file. This format is useful for compatibility with\n    tools that don't support OME-Zarr or napari plugins that require\n    individual TIFF files.\n\n    Args:\n        filename_pattern: Output filename pattern. Should contain '{z:04d}' or similar\n            format specifier for the Z-slice number. Examples:\n            - \"output_z{z:04d}.tif\"\n            - \"data/slice_{z:03d}.tiff\"\n            If pattern doesn't contain format specifier, '_{z:04d}' is appended\n            before the extension.\n        channel: Channel index to save (0-based). If None and data has multiple\n            channels, all channels will be saved as separate channel dimensions\n            in each TIFF file (multi-channel TIFFs).\n        timepoint: Timepoint index to save (0-based). If None and data has multiple\n            timepoints, raises ValueError (must select single timepoint).\n        compress: Whether to use LZW compression (default: True)\n        dtype: Output data type for TIFF files. Options:\n            - 'uint8': 8-bit unsigned integer (0-255)\n            - 'uint16': 16-bit unsigned integer (0-65535) [default]\n            - 'int16': 16-bit signed integer (-32768 to 32767)\n            - 'float32': 32-bit float (preserves original data)\n            Default 'uint16' provides good range and compatibility.\n        rescale: Whether to rescale data to fit the output dtype range.\n            If True, data is linearly scaled from [min, max] to the full\n            range of the output dtype. If False, data is clipped to the\n            output dtype range. Default: True\n\n    Returns:\n        Base directory path where files were saved\n\n    Raises:\n        ValueError: If data has multiple timepoints but none selected,\n            or if selected channel/timepoint is out of range,\n            or if dtype is not supported\n        OSError: If unable to write to specified directory\n\n    Examples:\n        &gt;&gt;&gt; # Save as 16-bit with auto-rescaling (default, recommended)\n        &gt;&gt;&gt; znii.to_tiff_stack(\"output_z{z:04d}.tif\")\n\n        &gt;&gt;&gt; # Save as 8-bit for smaller file sizes\n        &gt;&gt;&gt; znii.to_tiff_stack(\"output_z{z:04d}.tif\", dtype='uint8')\n\n        &gt;&gt;&gt; # Save specific channel without rescaling\n        &gt;&gt;&gt; znii.to_tiff_stack(\"channel0_z{z:04d}.tif\", channel=0, rescale=False)\n\n        &gt;&gt;&gt; # Save as float32 to preserve original precision\n        &gt;&gt;&gt; znii.to_tiff_stack(\"precise_z{z:04d}.tif\", dtype='float32')\n\n    Warnings:\n        This method loads all data into memory. For large datasets,\n        consider cropping or downsampling first to reduce memory usage.\n        The cellseg3d napari plugin and similar tools work best with\n        cropped regions rather than full-resolution whole-brain images.\n\n    Notes:\n        - Z-dimension becomes the stack (file) dimension\n        - Time and channel dimensions are handled as specified\n        - Spatial transformations are not preserved in TIFF format\n        - For 5D data (T,C,Z,Y,X), you must select a single timepoint\n        - Multi-channel data can be saved as multi-channel TIFFs or selected\n        - Data type conversion helps ensure compatibility with analysis tools\n        - uint16 is recommended for most scientific applications (good range + compatibility)\n    \"\"\"\n    try:\n        import tifffile\n    except ImportError:\n        raise ImportError(\n            \"tifffile is required for TIFF stack support. \"\n            \"Install with: pip install tifffile\"\n        )\n\n    # Get data and dimensions\n    data = self.data.compute()\n    dims = self.dims\n\n    # Create output directory if needed\n    import os\n\n    output_dir = os.path.dirname(filename_pattern)\n    if output_dir and not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Handle dimensional selection and validation\n    # Remove singleton dimensions first, similar to to_nifti\n    squeeze_axes = []\n    remaining_dims = []\n    time_dim_size = 1\n    channel_dim_size = 1\n\n    for i, dim in enumerate(dims):\n        if dim == \"t\":\n            time_dim_size = data.shape[i]\n            if data.shape[i] == 1:\n                squeeze_axes.append(i)\n            elif timepoint is None:\n                raise ValueError(\n                    f\"Data has {data.shape[i]} timepoints. \"\n                    f\"Must specify 'timepoint' parameter to select a single timepoint.\"\n                )\n            elif timepoint &gt;= data.shape[i]:\n                raise ValueError(\n                    f\"Timepoint {timepoint} is out of range (data has {data.shape[i]} timepoints)\"\n                )\n            else:\n                remaining_dims.append(dim)\n        elif dim == \"c\":\n            channel_dim_size = data.shape[i]\n            if data.shape[i] == 1:\n                squeeze_axes.append(i)\n            elif channel is None:\n                raise ValueError(\n                    f\"Data has {data.shape[i]} channels. \"\n                    f\"Must specify 'channel' parameter to select a single channel.\"\n                )\n            elif channel &gt;= data.shape[i]:\n                raise ValueError(\n                    f\"Channel {channel} is out of range (data has {data.shape[i]} channels)\"\n                )\n            else:\n                remaining_dims.append(dim)\n        else:\n            remaining_dims.append(dim)\n\n    # Select specific timepoint if needed\n    if time_dim_size &gt; 1 and timepoint is not None:\n        time_axis = dims.index(\"t\")\n        data = np.take(data, timepoint, axis=time_axis)\n        # Update dims list\n        dims = [d for i, d in enumerate(dims) if i != time_axis]\n\n    # Select specific channel if needed\n    if channel_dim_size &gt; 1 and channel is not None:\n        channel_axis = dims.index(\"c\")\n        data = np.take(data, channel, axis=channel_axis)\n        # Update dims list\n        dims = [d for i, d in enumerate(dims) if i != channel_axis]\n\n    # Squeeze singleton dimensions\n    if squeeze_axes:\n        # Recalculate squeeze axes after potential dimension removal\n        current_squeeze_axes = []\n        for axis in squeeze_axes:\n            # Count how many axes were removed before this one\n            removed_before = sum(\n                1\n                for removed_axis in [\n                    (\n                        dims.index(\"t\")\n                        if time_dim_size &gt; 1 and timepoint is not None\n                        else -1\n                    ),\n                    (\n                        dims.index(\"c\")\n                        if channel_dim_size &gt; 1 and channel is not None\n                        else -1\n                    ),\n                ]\n                if removed_axis != -1 and removed_axis &lt; axis\n            )\n            current_squeeze_axes.append(axis - removed_before)\n\n        data = np.squeeze(data, axis=tuple(current_squeeze_axes))\n        dims = [dim for i, dim in enumerate(dims) if i not in current_squeeze_axes]\n\n    # Find Z dimension for stacking\n    if \"z\" not in dims:\n        raise ValueError(\"Data must have a Z dimension for TIFF stack export\")\n\n    z_axis = dims.index(\"z\")\n    z_size = data.shape[z_axis]\n\n    # Check filename pattern contains format specifier\n    if \"{z\" not in filename_pattern:\n        # Add default z format before extension\n        name, ext = os.path.splitext(filename_pattern)\n        filename_pattern = f\"{name}_{{z:04d}}{ext}\"\n\n    # Move Z axis to first position for easy iteration\n    axes_order = list(range(data.ndim))\n    axes_order[0], axes_order[z_axis] = axes_order[z_axis], axes_order[0]\n    data = data.transpose(axes_order)\n\n    # Handle data type conversion and rescaling\n    supported_dtypes = {\n        \"uint8\": np.uint8,\n        \"uint16\": np.uint16,\n        \"int16\": np.int16,\n        \"float32\": np.float32,\n    }\n\n    if dtype not in supported_dtypes:\n        raise ValueError(\n            f\"Unsupported dtype '{dtype}'. Supported types: {list(supported_dtypes.keys())}\"\n        )\n\n    target_dtype = supported_dtypes[dtype]\n\n    if rescale and dtype != \"float32\":\n        # Get the data range\n        data_min = np.min(data)\n        data_max = np.max(data)\n\n        if data_min == data_max:\n            # Handle constant data case\n            data_scaled = np.zeros_like(data, dtype=target_dtype)\n        else:\n            # Get target range for the dtype\n            if dtype == \"uint8\":\n                target_min, target_max = 0, 255\n            elif dtype == \"uint16\":\n                target_min, target_max = 0, 65535\n            elif dtype == \"int16\":\n                target_min, target_max = -32768, 32767\n\n            # Convert data to float to avoid overflow during rescaling\n            # Linear rescaling formula:\n            # new_value = (value - data_min) * (target_max - target_min)\n            #             / (data_max - data_min) + target_min\n            data_float = data.astype(np.float64)\n            data_scaled = (\n                (data_float - data_min)\n                * (target_max - target_min)\n                / (data_max - data_min)\n                + target_min\n            ).astype(target_dtype)\n\n        print(\n            f\"Rescaled data from [{data_min:.3f}, {data_max:.3f}] to {dtype} range\"\n        )\n    else:\n        # No rescaling - just clip and convert\n        if dtype == \"uint8\":\n            data_scaled = np.clip(data, 0, 255).astype(target_dtype)\n        elif dtype == \"uint16\":\n            data_scaled = np.clip(data, 0, 65535).astype(target_dtype)\n        elif dtype == \"int16\":\n            data_scaled = np.clip(data, -32768, 32767).astype(target_dtype)\n        else:  # float32\n            data_scaled = data.astype(target_dtype)\n\n        if dtype != \"float32\":\n            print(f\"Converted data to {dtype} with clipping (no rescaling)\")\n\n    data = data_scaled\n\n    # Save each Z-slice as a separate TIFF file\n    compression = \"lzw\" if compress else None\n    saved_files = []\n\n    for z_idx in range(z_size):\n        slice_data = data[z_idx]\n\n        # Generate filename for this slice\n        filename = filename_pattern.format(z=z_idx)\n\n        # Save the 2D slice\n        tifffile.imwrite(filename, slice_data, compression=compression)\n        saved_files.append(filename)\n\n    print(f\"Saved {len(saved_files)} TIFF files to {output_dir or '.'}\")\n    print(\n        f\"Files: {os.path.basename(saved_files[0])} ... {os.path.basename(saved_files[-1])}\"\n    )\n\n    return output_dir or \".\"\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.from_imaris","title":"<code>zarrnii.ZarrNii.from_imaris(path, level=0, timepoint=0, channel=0, chunks='auto', axes_order='ZYX', orientation='RAS')</code>  <code>classmethod</code>","text":"<p>Load from Imaris (.ims) file format.</p> <p>Imaris files use HDF5 format with specific dataset structure. This method requires the 'imaris' extra dependency (h5py).</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>Path to Imaris (.ims) file</p> </li> <li> <code>level</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Resolution level to load (0 = full resolution)</p> </li> <li> <code>timepoint</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Time point to load (default: 0)</p> </li> <li> <code>channel</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Channel to load (default: 0)</p> </li> <li> <code>chunks</code>               (<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>Chunking strategy for dask array</p> </li> <li> <code>axes_order</code>               (<code>str</code>, default:                   <code>'ZYX'</code> )           \u2013            <p>Spatial axes order for compatibility (default: \"ZYX\")</p> </li> <li> <code>orientation</code>               (<code>str</code>, default:                   <code>'RAS'</code> )           \u2013            <p>Default orientation (default: \"RAS\")</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>'ZarrNii'</code>           \u2013            <p>ZarrNii instance</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If h5py is not available</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the file is not a valid Imaris file</p> </li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>@classmethod\ndef from_imaris(\n    cls,\n    path: str,\n    level: int = 0,\n    timepoint: int = 0,\n    channel: int = 0,\n    chunks: str = \"auto\",\n    axes_order: str = \"ZYX\",\n    orientation: str = \"RAS\",\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Load from Imaris (.ims) file format.\n\n    Imaris files use HDF5 format with specific dataset structure.\n    This method requires the 'imaris' extra dependency (h5py).\n\n    Args:\n        path: Path to Imaris (.ims) file\n        level: Resolution level to load (0 = full resolution)\n        timepoint: Time point to load (default: 0)\n        channel: Channel to load (default: 0)\n        chunks: Chunking strategy for dask array\n        axes_order: Spatial axes order for compatibility (default: \"ZYX\")\n        orientation: Default orientation (default: \"RAS\")\n\n    Returns:\n        ZarrNii instance\n\n    Raises:\n        ImportError: If h5py is not available\n        ValueError: If the file is not a valid Imaris file\n    \"\"\"\n    try:\n        import h5py\n    except ImportError:\n        raise ImportError(\n            \"h5py is required for Imaris support. \"\n            \"Install with: pip install zarrnii[imaris] or pip install h5py\"\n        )\n\n    # Open Imaris file\n    with h5py.File(path, \"r\") as f:\n        # Verify it's an Imaris file by checking for standard structure\n        if \"DataSet\" not in f:\n            raise ValueError(\n                f\"File {path} does not appear to be a valid Imaris file (missing DataSet group)\"\n            )\n\n        # Navigate to the specific dataset\n        dataset_group = f[\"DataSet\"]\n\n        # Find available resolution levels\n        resolution_levels = [\n            key for key in dataset_group.keys() if key.startswith(\"ResolutionLevel\")\n        ]\n        if not resolution_levels:\n            raise ValueError(\"No resolution levels found in Imaris file\")\n\n        # Validate level parameter\n        if level &gt;= len(resolution_levels):\n            raise ValueError(\n                f\"Level {level} not available. Available levels: 0-{len(resolution_levels)-1}\"\n            )\n\n        # Navigate to specified resolution level\n        res_level_key = f\"ResolutionLevel {level}\"\n        if res_level_key not in dataset_group:\n            raise ValueError(f\"Resolution level {level} not found\")\n\n        res_group = dataset_group[res_level_key]\n\n        # Find available timepoints\n        timepoints = [\n            key for key in res_group.keys() if key.startswith(\"TimePoint\")\n        ]\n        if not timepoints:\n            raise ValueError(\"No timepoints found in Imaris file\")\n\n        # Validate timepoint parameter\n        if timepoint &gt;= len(timepoints):\n            raise ValueError(\n                f\"Timepoint {timepoint} not available. Available timepoints: 0-{len(timepoints)-1}\"\n            )\n\n        # Navigate to specified timepoint\n        time_key = f\"TimePoint {timepoint}\"\n        if time_key not in res_group:\n            raise ValueError(f\"Timepoint {timepoint} not found\")\n\n        time_group = res_group[time_key]\n\n        # Find available channels\n        channels = [key for key in time_group.keys() if key.startswith(\"Channel\")]\n        if not channels:\n            raise ValueError(\"No channels found in Imaris file\")\n\n        # Validate channel parameter\n        if channel &gt;= len(channels):\n            raise ValueError(\n                f\"Channel {channel} not available. Available channels: 0-{len(channels)-1}\"\n            )\n\n        # Navigate to specified channel\n        channel_key = f\"Channel {channel}\"\n        if channel_key not in time_group:\n            raise ValueError(f\"Channel {channel} not found\")\n\n        channel_group = time_group[channel_key]\n\n        # Load the actual data\n        if \"Data\" not in channel_group:\n            raise ValueError(\"No Data dataset found in channel group\")\n\n        data_dataset = channel_group[\"Data\"]\n\n        # Load data into memory first (necessary because HDF5 file will be closed)\n        data_numpy = data_dataset[:]\n\n        # Create dask array from numpy array\n        data_array = da.from_array(data_numpy, chunks=chunks)\n\n        # Add channel dimension if not present\n        if len(data_array.shape) == 3:\n            data_array = data_array[np.newaxis, ...]\n\n        # Extract spatial metadata\n        # Try to get spacing information from Imaris metadata\n        spacing = [1.0, 1.0, 1.0]  # Default spacing\n        origin = [0.0, 0.0, 0.0]  # Default origin\n\n        # Look for ImageSizeX, ImageSizeY, ImageSizeZ attributes\n        try:\n            # Navigate back to get image info\n            if \"ImageSizeX\" in f.attrs:\n                x_size = f.attrs[\"ImageSizeX\"]\n                y_size = f.attrs[\"ImageSizeY\"]\n                z_size = f.attrs[\"ImageSizeZ\"]\n\n                # Calculate spacing based on physical size and voxel count\n                if data_array.shape[-1] &gt; 0:  # X dimension\n                    spacing[0] = x_size / data_array.shape[-1]\n                if data_array.shape[-2] &gt; 0:  # Y dimension\n                    spacing[1] = y_size / data_array.shape[-2]\n                if data_array.shape[-3] &gt; 0:  # Z dimension\n                    spacing[2] = z_size / data_array.shape[-3]\n        except (KeyError, IndexError):\n            # Use default spacing if metadata is not available\n            pass\n\n        # Create dimensions\n        dims = [\"c\"] + list(axes_order.lower())\n\n        # Create scale and translation dictionaries\n        scale_dict = {}\n        translation_dict = {}\n        spatial_dims = [\"z\", \"y\", \"x\"] if axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n\n        for i, dim in enumerate(spatial_dims):\n            scale_dict[dim] = spacing[i]\n            translation_dict[dim] = origin[i]\n\n        # Create NgffImage\n        ngff_image = nz.NgffImage(\n            data=data_array,\n            dims=dims,\n            scale=scale_dict,\n            translation=translation_dict,\n            name=f\"imaris_image_{path}_{level}_{timepoint}_{channel}\",\n        )\n\n    # Create and return ZarrNii instance\n    return cls(\n        ngff_image=ngff_image,\n        axes_order=axes_order,\n        xyz_orientation=orientation,\n        _omero=None,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.to_imaris","title":"<code>zarrnii.ZarrNii.to_imaris(path, compression='gzip', compression_opts=6)</code>","text":"<p>Save to Imaris (.ims) file format using HDF5.</p> <p>This method creates Imaris files compatible with Imaris software by following the exact HDF5 structure from correctly-formed reference files. All attributes use byte-array encoding as required by Imaris.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>Output path for Imaris (.ims) file</p> </li> <li> <code>compression</code>               (<code>str</code>, default:                   <code>'gzip'</code> )           \u2013            <p>HDF5 compression method (default: \"gzip\")</p> </li> <li> <code>compression_opts</code>               (<code>int</code>, default:                   <code>6</code> )           \u2013            <p>Compression level (default: 6)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>Path to the saved file</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If h5py is not available</p> </li> </ul> Notes <ul> <li>Imaris files are always saved in ZYX axis order</li> <li>Automatic axis reordering from XYZ to ZYX if needed</li> <li>Spatial transformations and metadata are preserved</li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def to_imaris(\n    self, path: str, compression: str = \"gzip\", compression_opts: int = 6\n) -&gt; str:\n    \"\"\"\n    Save to Imaris (.ims) file format using HDF5.\n\n    This method creates Imaris files compatible with Imaris software by\n    following the exact HDF5 structure from correctly-formed reference files.\n    All attributes use byte-array encoding as required by Imaris.\n\n    Args:\n        path: Output path for Imaris (.ims) file\n        compression: HDF5 compression method (default: \"gzip\")\n        compression_opts: Compression level (default: 6)\n\n    Returns:\n        str: Path to the saved file\n\n    Raises:\n        ImportError: If h5py is not available\n\n    Notes:\n        - Imaris files are always saved in ZYX axis order\n        - Automatic axis reordering from XYZ to ZYX if needed\n        - Spatial transformations and metadata are preserved\n    \"\"\"\n    try:\n        import h5py\n    except ImportError:\n        raise ImportError(\n            \"h5py is required for Imaris support. \"\n            \"Install with: pip install zarrnii[imaris] or pip install h5py\"\n        )\n\n    # Determine the image to save\n    if self.axes_order == \"XYZ\":\n        # Need to reorder data from XYZ to ZYX for Imaris\n        ngff_image_to_save = self._create_zyx_ngff_image()\n    else:\n        # Already in ZYX order\n        ngff_image_to_save = self.ngff_image\n\n    # Ensure path has .ims extension\n    if not path.endswith(\".ims\"):\n        path = path + \".ims\"\n\n    def _string_to_byte_array(s: str) -&gt; np.ndarray:\n        \"\"\"Convert string to byte array as required by Imaris.\"\"\"\n        return np.array([c.encode() for c in s])\n\n    # Get data and metadata\n    if hasattr(ngff_image_to_save.data, \"compute\"):\n        data = (\n            ngff_image_to_save.data.compute()\n        )  # Convert Dask array to numpy array\n    else:\n        data = np.asarray(ngff_image_to_save.data)  # Handle numpy arrays directly\n\n    # Handle dimensions: expect ZYX or CZYX\n    if len(data.shape) == 4:\n        # CZYX format\n        n_channels = data.shape[0]\n        z, y, x = data.shape[1:]\n    elif len(data.shape) == 3:\n        # ZYX format - single channel\n        n_channels = 1\n        z, y, x = data.shape\n        data = data[np.newaxis, ...]  # Add channel dimension\n    else:\n        raise ValueError(\n            f\"Unsupported data shape: {data.shape}. Expected 3D (ZYX) or 4D (CZYX)\"\n        )\n\n    # Create Imaris file structure exactly matching reference file\n    with h5py.File(path, \"w\") as f:\n        # Root attributes - use exact byte array format from reference\n        f.attrs[\"DataSetDirectoryName\"] = _string_to_byte_array(\"DataSet\")\n        f.attrs[\"DataSetInfoDirectoryName\"] = _string_to_byte_array(\"DataSetInfo\")\n        f.attrs[\"ImarisDataSet\"] = _string_to_byte_array(\"ImarisDataSet\")\n        f.attrs[\"ImarisVersion\"] = _string_to_byte_array(\"5.5.0\")\n        f.attrs[\"NumberOfDataSets\"] = np.array([1], dtype=np.uint32)\n        f.attrs[\"ThumbnailDirectoryName\"] = _string_to_byte_array(\"Thumbnail\")\n\n        # Create main DataSet group structure\n        dataset_group = f.create_group(\"DataSet\")\n        res_group = dataset_group.create_group(\"ResolutionLevel 0\")\n        time_group = res_group.create_group(\"TimePoint 0\")\n\n        # Create channels with proper attributes\n        for c in range(n_channels):\n            channel_group = time_group.create_group(f\"Channel {c}\")\n            channel_data = data[c]  # (Z, Y, X)\n\n            # Channel attributes - use byte array format exactly like reference\n            channel_group.attrs[\"ImageSizeX\"] = _string_to_byte_array(str(x))\n            channel_group.attrs[\"ImageSizeY\"] = _string_to_byte_array(str(y))\n            channel_group.attrs[\"ImageSizeZ\"] = _string_to_byte_array(str(z))\n            channel_group.attrs[\"ImageBlockSizeX\"] = _string_to_byte_array(str(x))\n            channel_group.attrs[\"ImageBlockSizeY\"] = _string_to_byte_array(str(y))\n            channel_group.attrs[\"ImageBlockSizeZ\"] = _string_to_byte_array(\n                str(min(z, 16))\n            )\n\n            # Histogram range attributes\n            data_min, data_max = float(channel_data.min()), float(\n                channel_data.max()\n            )\n            channel_group.attrs[\"HistogramMin\"] = _string_to_byte_array(\n                f\"{data_min:.3f}\"\n            )\n            channel_group.attrs[\"HistogramMax\"] = _string_to_byte_array(\n                f\"{data_max:.3f}\"\n            )\n\n            # Create data dataset with proper compression\n            # Preserve original data type but ensure it's compatible with Imaris\n            if channel_data.dtype == np.float32 or channel_data.dtype == np.float64:\n                # Keep float data as is for round-trip compatibility\n                data_for_storage = channel_data.astype(np.float32)\n            elif channel_data.dtype in [np.uint16, np.int16]:\n                # Keep 16-bit data as is\n                data_for_storage = channel_data\n            else:\n                # Convert other types to uint8\n                data_for_storage = channel_data.astype(np.uint8)\n\n            channel_group.create_dataset(\n                \"Data\",\n                data=data_for_storage,\n                compression=compression,\n                compression_opts=compression_opts,\n                chunks=True,\n            )\n\n            # Create histogram\n            hist_data, _ = np.histogram(\n                channel_data.flatten(), bins=256, range=(data_min, data_max)\n            )\n            channel_group.create_dataset(\n                \"Histogram\", data=hist_data.astype(np.uint64)\n            )\n\n        # Get spacing directly from scale dictionary with proper XYZ order\n        try:\n            # Extract voxel sizes directly from ngff_image scale dictionary\n            # This ensures we get X, Y, Z in the correct order regardless of axes_order\n            sx = ngff_image_to_save.scale.get(\"x\", 1.0)\n            sy = ngff_image_to_save.scale.get(\"y\", 1.0)\n            sz = ngff_image_to_save.scale.get(\"z\", 1.0)\n        except:\n            sx = sy = sz = 1.0\n\n        # Calculate extents (physical coordinates)\n        ext_x = sx * x\n        ext_y = sy * y\n        ext_z = sz * z\n\n        # Create comprehensive DataSetInfo structure matching reference\n        info_group = f.create_group(\"DataSetInfo\")\n\n        # Create channel info groups\n        for c in range(n_channels):\n            channel_info = info_group.create_group(f\"Channel {c}\")\n\n            # Essential channel attributes in byte array format\n            channel_info.attrs[\"Color\"] = _string_to_byte_array(\n                \"1.000 0.000 0.000\"\n                if c == 0\n                else f\"0.000 {1.0 if c == 1 else 0.0:.3f} {1.0 if c == 2 else 0.0:.3f}\"\n            )\n            channel_info.attrs[\"Name\"] = _string_to_byte_array(f\"Channel {c}\")\n            channel_info.attrs[\"ColorMode\"] = _string_to_byte_array(\"BaseColor\")\n            channel_info.attrs[\"ColorOpacity\"] = _string_to_byte_array(\"1.000\")\n            channel_info.attrs[\"ColorRange\"] = _string_to_byte_array(\"0 255\")\n            channel_info.attrs[\"GammaCorrection\"] = _string_to_byte_array(\"1.000\")\n            channel_info.attrs[\"LSMEmissionWavelength\"] = _string_to_byte_array(\n                \"500\"\n            )\n            channel_info.attrs[\"LSMExcitationWavelength\"] = _string_to_byte_array(\n                \"500\"\n            )\n            channel_info.attrs[\"LSMPhotons\"] = _string_to_byte_array(\"1\")\n            channel_info.attrs[\"LSMPinhole\"] = _string_to_byte_array(\"0\")\n\n            # Add description\n            description = f\"Channel {c} created by ZarrNii\"\n            channel_info.attrs[\"Description\"] = _string_to_byte_array(description)\n\n        # Create CRITICAL Image group with voxel size information (this was missing!)\n        image_info = info_group.create_group(\"Image\")\n\n        # Add essential image metadata with proper voxel size information\n        image_info.attrs[\"X\"] = _string_to_byte_array(str(x))\n        image_info.attrs[\"Y\"] = _string_to_byte_array(str(y))\n        image_info.attrs[\"Z\"] = _string_to_byte_array(str(z))\n        image_info.attrs[\"Unit\"] = _string_to_byte_array(\"um\")\n        image_info.attrs[\"Noc\"] = _string_to_byte_array(str(n_channels))\n\n        # CRITICAL: Set proper physical extents that define voxel size\n        # Imaris reads voxel size from these extent values\n        image_info.attrs[\"ExtMin0\"] = _string_to_byte_array(f\"{-ext_x/2:.3f}\")\n        image_info.attrs[\"ExtMax0\"] = _string_to_byte_array(f\"{ext_x/2:.3f}\")\n        image_info.attrs[\"ExtMin1\"] = _string_to_byte_array(f\"{-ext_y/2:.3f}\")\n        image_info.attrs[\"ExtMax1\"] = _string_to_byte_array(f\"{ext_y/2:.3f}\")\n        image_info.attrs[\"ExtMin2\"] = _string_to_byte_array(f\"{-ext_z/2:.3f}\")\n        image_info.attrs[\"ExtMax2\"] = _string_to_byte_array(f\"{ext_z/2:.3f}\")\n\n        # Add device/acquisition metadata\n        image_info.attrs[\"ManufactorString\"] = _string_to_byte_array(\"ZarrNii\")\n        image_info.attrs[\"ManufactorType\"] = _string_to_byte_array(\"Generic\")\n        image_info.attrs[\"LensPower\"] = _string_to_byte_array(\"\")\n        image_info.attrs[\"NumericalAperture\"] = _string_to_byte_array(\"\")\n        image_info.attrs[\"RecordingDate\"] = _string_to_byte_array(\n            \"2024-01-01 00:00:00.000\"\n        )\n        image_info.attrs[\"Filename\"] = _string_to_byte_array(path.split(\"/\")[-1])\n        image_info.attrs[\"Name\"] = _string_to_byte_array(\"ZarrNii Export\")\n        image_info.attrs[\"Compression\"] = _string_to_byte_array(\"5794\")\n\n        # Add description\n        description = (\n            f\"Imaris file created by ZarrNii from {self.axes_order} format data. \"\n            f\"Original shape: {self.darr.shape}. Converted to Imaris format \"\n            f\"with {n_channels} channel(s) and dimensions {z}x{y}x{x}. \"\n            f\"Voxel size: {sx:.3f} x {sy:.3f} x {sz:.3f} um.\"\n        )\n        image_info.attrs[\"Description\"] = _string_to_byte_array(description)\n\n        # Create Imaris metadata group\n        imaris_info = info_group.create_group(\"Imaris\")\n        imaris_info.attrs[\"Version\"] = _string_to_byte_array(\"7.0\")\n        imaris_info.attrs[\"ThumbnailMode\"] = _string_to_byte_array(\"thumbnailMIP\")\n        imaris_info.attrs[\"ThumbnailSize\"] = _string_to_byte_array(\"256\")\n\n        # Create ImarisDataSet metadata\n        dataset_info = info_group.create_group(\"ImarisDataSet\")\n        dataset_info.attrs[\"Creator\"] = _string_to_byte_array(\"Imaris\")\n        dataset_info.attrs[\"Version\"] = _string_to_byte_array(\"7.0\")\n        dataset_info.attrs[\"NumberOfImages\"] = _string_to_byte_array(\"1\")\n\n        # Add version-specific groups as seen in reference\n        dataset_info_ver = info_group.create_group(\"ImarisDataSet       0.0.0\")\n        dataset_info_ver.attrs[\"NumberOfImages\"] = _string_to_byte_array(\"1\")\n        dataset_info_ver2 = info_group.create_group(\"ImarisDataSet      0.0.0\")\n        dataset_info_ver2.attrs[\"NumberOfImages\"] = _string_to_byte_array(\"1\")\n\n        # Create TimeInfo group\n        time_info = info_group.create_group(\"TimeInfo\")\n        time_info.attrs[\"DatasetTimePoints\"] = _string_to_byte_array(\"1\")\n        time_info.attrs[\"FileTimePoints\"] = _string_to_byte_array(\"1\")\n        time_info.attrs[\"TimePoint1\"] = _string_to_byte_array(\n            \"2024-01-01 00:00:00.000\"\n        )\n\n        # Create Log group (basic processing log)\n        log_group = info_group.create_group(\"Log\")\n        log_group.attrs[\"Entries\"] = _string_to_byte_array(\"1\")\n        log_group.attrs[\"Entry0\"] = _string_to_byte_array(\n            f\"&lt;ZarrNiiExport channels=\\\"{' '.join(['on'] * n_channels)}\\\"/&gt;\"\n        )\n\n        # Create thumbnail group with proper multi-channel thumbnail\n        thumbnail_group = f.create_group(\"Thumbnail\")\n\n        # Create a combined thumbnail (256x1024 for multi-channel as in reference)\n        if n_channels &gt; 1:\n            # Multi-channel thumbnail: concatenate channels horizontally\n            thumb_width = 256 * n_channels\n            thumbnail_data = np.zeros((256, thumb_width), dtype=np.uint8)\n\n            for c in range(n_channels):\n                # Downsample each channel to 256x256\n                channel_data = data[c]\n                # Take MIP (Maximum Intensity Projection) along Z\n                mip = np.max(channel_data, axis=0)\n                # Resize to 256x256 (simple decimation)\n                step_y = max(1, mip.shape[0] // 256)\n                step_x = max(1, mip.shape[1] // 256)\n                thumb_channel = mip[::step_y, ::step_x]\n\n                # Pad or crop to exactly 256x256\n                if thumb_channel.shape[0] &lt; 256 or thumb_channel.shape[1] &lt; 256:\n                    padded = np.zeros((256, 256), dtype=thumb_channel.dtype)\n                    h, w = thumb_channel.shape\n                    padded[:h, :w] = thumb_channel\n                    thumb_channel = padded\n                else:\n                    thumb_channel = thumb_channel[:256, :256]\n\n                # Place in thumbnail\n                thumbnail_data[:, c * 256 : (c + 1) * 256] = thumb_channel\n        else:\n            # Single channel: 256x256 thumbnail\n            channel_data = data[0]\n            mip = np.max(channel_data, axis=0)\n            step_y = max(1, mip.shape[0] // 256)\n            step_x = max(1, mip.shape[1] // 256)\n            thumbnail_data = mip[::step_y, ::step_x]\n\n            if thumbnail_data.shape[0] &lt; 256 or thumbnail_data.shape[1] &lt; 256:\n                padded = np.zeros((256, 256), dtype=thumbnail_data.dtype)\n                h, w = thumbnail_data.shape\n                padded[:h, :w] = thumbnail_data\n                thumbnail_data = padded\n            else:\n                thumbnail_data = thumbnail_data[:256, :256]\n\n        thumbnail_group.create_dataset(\"Data\", data=thumbnail_data.astype(np.uint8))\n\n    return path\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.copy","title":"<code>zarrnii.ZarrNii.copy()</code>","text":"<p>Create a copy of this ZarrNii.</p> <p>Returns:</p> <ul> <li> <code>'ZarrNii'</code>           \u2013            <p>New ZarrNii with copied data</p> </li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def copy(self) -&gt; \"ZarrNii\":\n    \"\"\"\n    Create a copy of this ZarrNii.\n\n    Returns:\n        New ZarrNii with copied data\n    \"\"\"\n    # Create a new NgffImage with the same properties\n    copied_image = nz.NgffImage(\n        data=self.ngff_image.data,  # Dask arrays are lazy so this is efficient\n        dims=self.ngff_image.dims.copy(),\n        scale=self.ngff_image.scale.copy(),\n        translation=self.ngff_image.translation.copy(),\n        name=self.ngff_image.name,\n    )\n    return ZarrNii(\n        ngff_image=copied_image,\n        axes_order=self.axes_order,\n        xyz_orientation=self.xyz_orientation,\n        _omero=self._omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.compute","title":"<code>zarrnii.ZarrNii.compute()</code>","text":"<p>Compute the dask array and return the underlying NgffImage.</p> <p>This triggers computation of any lazy operations and returns the NgffImage with computed data.</p> <p>Returns:</p> <ul> <li> <code>NgffImage</code>           \u2013            <p>NgffImage with computed data</p> </li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def compute(self) -&gt; nz.NgffImage:\n    \"\"\"\n    Compute the dask array and return the underlying NgffImage.\n\n    This triggers computation of any lazy operations and returns\n    the NgffImage with computed data.\n\n    Returns:\n        NgffImage with computed data\n    \"\"\"\n    computed_data = self.ngff_image.data.compute()\n\n    # Create new NgffImage with computed data\n    computed_image = nz.NgffImage(\n        data=computed_data,\n        dims=self.ngff_image.dims,\n        scale=self.ngff_image.scale,\n        translation=self.ngff_image.translation,\n        name=self.ngff_image.name,\n    )\n    return computed_image\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.get_orientation","title":"<code>zarrnii.ZarrNii.get_orientation()</code>","text":"<p>Get the anatomical orientation of the dataset.</p> <p>This function returns the orientation string (e.g., 'RAS', 'LPI') of the dataset.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The orientation string corresponding to the dataset's anatomical orientation.</p> </li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def get_orientation(self) -&gt; str:\n    \"\"\"\n    Get the anatomical orientation of the dataset.\n\n    This function returns the orientation string (e.g., 'RAS', 'LPI') of the dataset.\n\n    Returns:\n        str: The orientation string corresponding to the dataset's anatomical orientation.\n    \"\"\"\n    return self.orientation\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.get_zooms","title":"<code>zarrnii.ZarrNii.get_zooms(axes_order=None)</code>","text":"<p>Get voxel spacing (zooms) from NgffImage scale.</p> <p>Parameters:</p> <ul> <li> <code>axes_order</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Spatial axes order, defaults to self.axes_order</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>Array of voxel spacings</p> </li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def get_zooms(self, axes_order: str = None) -&gt; np.ndarray:\n    \"\"\"\n    Get voxel spacing (zooms) from NgffImage scale.\n\n    Args:\n        axes_order: Spatial axes order, defaults to self.axes_order\n\n    Returns:\n        Array of voxel spacings\n    \"\"\"\n    if axes_order is None:\n        axes_order = self.axes_order\n\n    spatial_dims = [\"z\", \"y\", \"x\"] if axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n    zooms = []\n\n    for dim in spatial_dims:\n        if dim in self.ngff_image.scale:\n            zooms.append(self.ngff_image.scale[dim])\n        else:\n            zooms.append(1.0)\n\n    return np.array(zooms)\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.get_origin","title":"<code>zarrnii.ZarrNii.get_origin(axes_order=None)</code>","text":"<p>Get origin (translation) from NgffImage.</p> <p>Parameters:</p> <ul> <li> <code>axes_order</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Spatial axes order, defaults to self.axes_order</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>Array of origin coordinates</p> </li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def get_origin(self, axes_order: str = None) -&gt; np.ndarray:\n    \"\"\"\n    Get origin (translation) from NgffImage.\n\n    Args:\n        axes_order: Spatial axes order, defaults to self.axes_order\n\n    Returns:\n        Array of origin coordinates\n    \"\"\"\n    if axes_order is None:\n        axes_order = self.axes_order\n\n    spatial_dims = [\"z\", \"y\", \"x\"] if axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n    origin = []\n\n    for dim in spatial_dims:\n        if dim in self.ngff_image.translation:\n            origin.append(self.ngff_image.translation[dim])\n        else:\n            origin.append(0.0)\n\n    return np.array(origin)\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.get_affine_matrix","title":"<code>zarrnii.ZarrNii.get_affine_matrix(axes_order=None)</code>","text":"<p>Construct a 4x4 affine matrix from NGFF metadata (scale/translation), and align it to self.orientation (if provided) using nibabel.orientations.</p> <p>Parameters:</p> <ul> <li> <code>axes_order</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Spatial axes order, e.g. 'ZYX' or 'XYZ'. Defaults to 'XYZ'.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: 4x4 affine matrix.</p> </li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def get_affine_matrix(self, axes_order: str = None) -&gt; np.ndarray:\n    \"\"\"\n    Construct a 4x4 affine matrix from NGFF metadata (scale/translation),\n    and align it to self.orientation (if provided) using nibabel.orientations.\n\n    Args:\n        axes_order: Spatial axes order, e.g. 'ZYX' or 'XYZ'. Defaults to 'XYZ'.\n\n    Returns:\n        np.ndarray: 4x4 affine matrix.\n    \"\"\"\n    if axes_order is None:\n        axes_order = self.axes_order\n\n    if axes_order == \"ZYX\":\n        orientation = reverse_orientation_string(self.orientation)\n    else:\n        orientation = self.orientation\n\n    # Safely pull scale/translation from metadata (dict-like expected)\n    scale_meta = getattr(self.ngff_image, \"scale\", {}) or {}\n    trans_meta = getattr(self.ngff_image, \"translation\", {}) or {}\n\n    scale = np.ones(\n        3,\n    )\n    trans = np.zeros(\n        3,\n    )\n\n    for i, dim in enumerate(axes_order):\n        s = scale_meta.get(dim.lower())\n        if s is not None:\n            scale[i] = float(s)\n\n    for i, dim in enumerate(axes_order):\n        s = trans_meta.get(dim.lower())\n        if s is not None:\n            trans[i] = float(s)\n\n    affine = _axcodes2aff(orientation, scale=scale, translate=trans)\n\n    return affine\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.apply_transform_ref_to_flo_indices","title":"<code>zarrnii.ZarrNii.apply_transform_ref_to_flo_indices(*transforms, ref_znimg, indices)</code>","text":"<p>Transform indices from reference to floating space.</p> Source code in <code>zarrnii/core.py</code> <pre><code>def apply_transform_ref_to_flo_indices(self, *transforms, ref_znimg, indices):\n    \"\"\"Transform indices from reference to floating space.\"\"\"\n    # Placeholder implementation - would need full transform logic\n    return indices\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.apply_transform_flo_to_ref_indices","title":"<code>zarrnii.ZarrNii.apply_transform_flo_to_ref_indices(*transforms, ref_znimg, indices)</code>","text":"<p>Transform indices from floating to reference space.</p> Source code in <code>zarrnii/core.py</code> <pre><code>def apply_transform_flo_to_ref_indices(self, *transforms, ref_znimg, indices):\n    \"\"\"Transform indices from floating to reference space.\"\"\"\n    # Placeholder implementation - would need full transform logic\n    return indices\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.list_channels","title":"<code>zarrnii.ZarrNii.list_channels()</code>","text":"<p>Get list of available channel labels from OMERO metadata.</p> <p>Extracts channel labels from OMERO metadata if available, providing human-readable names for multi-channel datasets.</p> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List of channel label strings. Empty list if no OMERO metadata</p> </li> <li> <code>List[str]</code>           \u2013            <p>is available or no channels are defined.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Check available channels\n&gt;&gt;&gt; labels = znii.list_channels()\n&gt;&gt;&gt; print(f\"Available channels: {labels}\")\n&gt;&gt;&gt; # ['DAPI', 'GFP', 'RFP', 'Cy5']\n</code></pre> <pre><code>&gt;&gt;&gt; # Select specific channels by label\n&gt;&gt;&gt; selected = znii.select_channels(channel_labels=['DAPI', 'GFP'])\n</code></pre> Notes <ul> <li>Requires OMERO metadata to be present in the dataset</li> <li>Returns empty list for datasets without channel metadata</li> <li>Labels are extracted from the 'label' field of each channel</li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def list_channels(self) -&gt; List[str]:\n    \"\"\"Get list of available channel labels from OMERO metadata.\n\n    Extracts channel labels from OMERO metadata if available, providing\n    human-readable names for multi-channel datasets.\n\n    Returns:\n        List of channel label strings. Empty list if no OMERO metadata\n        is available or no channels are defined.\n\n    Examples:\n        &gt;&gt;&gt; # Check available channels\n        &gt;&gt;&gt; labels = znii.list_channels()\n        &gt;&gt;&gt; print(f\"Available channels: {labels}\")\n        &gt;&gt;&gt; # ['DAPI', 'GFP', 'RFP', 'Cy5']\n\n        &gt;&gt;&gt; # Select specific channels by label\n        &gt;&gt;&gt; selected = znii.select_channels(channel_labels=['DAPI', 'GFP'])\n\n    Notes:\n        - Requires OMERO metadata to be present in the dataset\n        - Returns empty list for datasets without channel metadata\n        - Labels are extracted from the 'label' field of each channel\n    \"\"\"\n    if self.omero is None or not hasattr(self.omero, \"channels\"):\n        return []\n\n    return [\n        ch.label if hasattr(ch, \"label\") else ch.get(\"label\", \"\")\n        for ch in self.omero.channels\n    ]\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.select_channels","title":"<code>zarrnii.ZarrNii.select_channels(channels=None, channel_labels=None)</code>","text":"<p>Select specific channels from multi-channel image data.</p> <p>Creates a new ZarrNii instance containing only the specified channels, reducing memory usage and focusing analysis on channels of interest. Supports selection by both numeric indices and human-readable labels.</p> <p>Parameters:</p> <ul> <li> <code>channels</code>               (<code>Optional[List[int]]</code>, default:                   <code>None</code> )           \u2013            <p>List of 0-based channel indices to select. Mutually exclusive with channel_labels</p> </li> <li> <code>channel_labels</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>List of channel names to select by label. Requires OMERO metadata. Mutually exclusive with channels</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>'ZarrNii'</code>           \u2013            <p>New ZarrNii instance with selected channels and updated metadata</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If both channels and channel_labels specified, or if channel_labels used without OMERO metadata, or if labels not found</p> </li> <li> <code>IndexError</code>             \u2013            <p>If channel indices are out of range</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Select channels by index\n&gt;&gt;&gt; selected = znii.select_channels(channels=[0, 2])\n</code></pre> <pre><code>&gt;&gt;&gt; # Select channels by label (requires OMERO metadata)\n&gt;&gt;&gt; selected = znii.select_channels(channel_labels=['DAPI', 'GFP'])\n</code></pre> <pre><code>&gt;&gt;&gt; # Check available labels first\n&gt;&gt;&gt; available = znii.list_channels()\n&gt;&gt;&gt; print(f\"Available: {available}\")\n&gt;&gt;&gt; selected = znii.select_channels(channel_labels=available[:2])\n</code></pre> Notes <ul> <li>Preserves all spatial dimensions and timepoints</li> <li>Updates OMERO metadata to reflect selected channels</li> <li>Maintains spatial transformations and other metadata</li> <li>Channel order in output matches selection order</li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def select_channels(\n    self,\n    channels: Optional[List[int]] = None,\n    channel_labels: Optional[List[str]] = None,\n) -&gt; \"ZarrNii\":\n    \"\"\"Select specific channels from multi-channel image data.\n\n    Creates a new ZarrNii instance containing only the specified channels,\n    reducing memory usage and focusing analysis on channels of interest.\n    Supports selection by both numeric indices and human-readable labels.\n\n    Args:\n        channels: List of 0-based channel indices to select.\n            Mutually exclusive with channel_labels\n        channel_labels: List of channel names to select by label.\n            Requires OMERO metadata. Mutually exclusive with channels\n\n    Returns:\n        New ZarrNii instance with selected channels and updated metadata\n\n    Raises:\n        ValueError: If both channels and channel_labels specified, or if\n            channel_labels used without OMERO metadata, or if labels not found\n        IndexError: If channel indices are out of range\n\n    Examples:\n        &gt;&gt;&gt; # Select channels by index\n        &gt;&gt;&gt; selected = znii.select_channels(channels=[0, 2])\n\n        &gt;&gt;&gt; # Select channels by label (requires OMERO metadata)\n        &gt;&gt;&gt; selected = znii.select_channels(channel_labels=['DAPI', 'GFP'])\n\n        &gt;&gt;&gt; # Check available labels first\n        &gt;&gt;&gt; available = znii.list_channels()\n        &gt;&gt;&gt; print(f\"Available: {available}\")\n        &gt;&gt;&gt; selected = znii.select_channels(channel_labels=available[:2])\n\n    Notes:\n        - Preserves all spatial dimensions and timepoints\n        - Updates OMERO metadata to reflect selected channels\n        - Maintains spatial transformations and other metadata\n        - Channel order in output matches selection order\n    \"\"\"\n    if channels is not None and channel_labels is not None:\n        raise ValueError(\"Cannot specify both 'channels' and 'channel_labels'\")\n\n    if channel_labels is not None:\n        if self.omero is None:\n            raise ValueError(\n                \"Channel labels were specified but no omero metadata found\"\n            )\n\n        available_labels = self.list_channels()\n        channel_indices = []\n        for label in channel_labels:\n            if label not in available_labels:\n                raise ValueError(f\"Channel label '{label}' not found\")\n            channel_indices.append(available_labels.index(label))\n        channels = channel_indices\n\n    if channels is None:\n        # Return a copy with all channels\n        return self.copy()\n\n    # Check if channel dimension exists\n    if \"c\" not in self.dims:\n        raise ValueError(\"No channel dimension found in the data\")\n\n    # Get channel dimension index\n    c_idx = self.dims.index(\"c\")\n\n    # Create slice objects for proper dimension indexing\n    slices = [slice(None)] * len(self.data.shape)\n    slices[c_idx] = channels\n\n    # Select channels from data using proper dimension indexing\n    selected_data = self.data[tuple(slices)]\n\n    # Create new NgffImage with selected data\n    new_ngff_image = nz.NgffImage(\n        data=selected_data,\n        dims=self.dims,\n        scale=self.scale,\n        translation=self.translation,\n        name=self.name,\n    )\n\n    # Filter omero metadata to match selected channels\n    filtered_omero = None\n    if self.omero is not None and hasattr(self.omero, \"channels\"):\n\n        class FilteredOmero:\n            def __init__(self, channels):\n                self.channels = channels\n\n        filtered_channels = [self.omero.channels[i] for i in channels]\n        filtered_omero = FilteredOmero(filtered_channels)\n\n    return ZarrNii(\n        ngff_image=new_ngff_image,\n        axes_order=self.axes_order,\n        xyz_orientation=self.xyz_orientation,\n        _omero=filtered_omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.select_timepoints","title":"<code>zarrnii.ZarrNii.select_timepoints(timepoints=None)</code>","text":"<p>Select timepoints from the image data and return a new ZarrNii instance.</p> <p>Parameters:</p> <ul> <li> <code>timepoints</code>               (<code>Optional[List[int]]</code>, default:                   <code>None</code> )           \u2013            <p>Timepoint indices to select</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>'ZarrNii'</code>           \u2013            <p>New ZarrNii instance with selected timepoints</p> </li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def select_timepoints(self, timepoints: Optional[List[int]] = None) -&gt; \"ZarrNii\":\n    \"\"\"\n    Select timepoints from the image data and return a new ZarrNii instance.\n\n    Args:\n        timepoints: Timepoint indices to select\n\n    Returns:\n        New ZarrNii instance with selected timepoints\n    \"\"\"\n    if timepoints is None:\n        # Return a copy with all timepoints\n        return self.copy()\n\n    # Check if time dimension exists\n    if \"t\" not in self.dims:\n        raise ValueError(\"No time dimension found in the data\")\n\n    # Get time dimension index\n    t_idx = self.dims.index(\"t\")\n\n    # Create slice objects\n    slices = [slice(None)] * len(self.data.shape)\n    slices[t_idx] = timepoints\n\n    # Select timepoints from data\n    selected_data = self.data[tuple(slices)]\n\n    # Create new NgffImage with selected data\n    new_ngff_image = nz.NgffImage(\n        data=selected_data,\n        dims=self.dims,\n        scale=self.scale,\n        translation=self.translation,\n        name=self.name,\n    )\n\n    return ZarrNii(\n        ngff_image=new_ngff_image,\n        axes_order=self.axes_order,\n        xyz_orientation=self.xyz_orientation,\n        _omero=self._omero,  # Timepoint selection doesn't affect omero metadata\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.to_ngff_image","title":"<code>zarrnii.ZarrNii.to_ngff_image(name=None)</code>","text":"<p>Convert to NgffImage object.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Optional name for the image</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NgffImage</code>           \u2013            <p>NgffImage representation</p> </li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def to_ngff_image(self, name: str = None) -&gt; nz.NgffImage:\n    \"\"\"\n    Convert to NgffImage object.\n\n    Args:\n        name: Optional name for the image\n\n    Returns:\n        NgffImage representation\n    \"\"\"\n    if name is None:\n        name = self.name\n\n    return nz.NgffImage(\n        data=self.data,\n        dims=self.dims,\n        scale=self.scale,\n        translation=self.translation,\n        name=name,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.segment","title":"<code>zarrnii.ZarrNii.segment(plugin, chunk_size=None, **kwargs)</code>","text":"<p>Apply segmentation plugin to the image using blockwise processing.</p> <p>This method applies a segmentation plugin to the image data using dask's blockwise processing for efficient computation on large datasets.</p> <p>Parameters:</p> <ul> <li> <code>plugin</code>           \u2013            <p>Segmentation plugin instance or class to apply</p> </li> <li> <code>chunk_size</code>               (<code>Optional[Tuple[int, ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional chunk size for dask processing. If None, uses current chunks.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments passed to the plugin</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>'ZarrNii'</code>           \u2013            <p>New ZarrNii instance with segmented data as labels</p> </li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def segment(\n    self, plugin, chunk_size: Optional[Tuple[int, ...]] = None, **kwargs\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Apply segmentation plugin to the image using blockwise processing.\n\n    This method applies a segmentation plugin to the image data using dask's\n    blockwise processing for efficient computation on large datasets.\n\n    Args:\n        plugin: Segmentation plugin instance or class to apply\n        chunk_size: Optional chunk size for dask processing. If None, uses current chunks.\n        **kwargs: Additional arguments passed to the plugin\n\n    Returns:\n        New ZarrNii instance with segmented data as labels\n    \"\"\"\n    from .plugins.segmentation import SegmentationPlugin\n\n    # Handle plugin instance or class\n    if isinstance(plugin, type) and issubclass(plugin, SegmentationPlugin):\n        plugin = plugin(**kwargs)\n    elif not isinstance(plugin, SegmentationPlugin):\n        raise TypeError(\n            \"Plugin must be an instance or subclass of SegmentationPlugin\"\n        )\n\n    # Prepare chunk size\n    if chunk_size is not None:\n        # Rechunk the data if different chunk size requested\n        data = self.data.rechunk(chunk_size)\n    else:\n        data = self.data\n\n    # Create metadata dict to pass to plugin\n    metadata = {\n        \"axes_order\": self.axes_order,\n        \"orientation\": self.xyz_orientation,\n        \"shape\": self.shape,\n        \"dims\": self.dims,\n        \"scale\": self.scale,\n        \"translation\": self.translation,\n    }\n\n    # Create a wrapper function for map_blocks\n    def segment_block(block):\n        \"\"\"Wrapper function to apply segmentation to a single block.\"\"\"\n        # Handle single blocks\n        return plugin.segment(block, metadata)\n\n    # Apply segmentation using dask map_blocks\n    segmented_data = da.map_blocks(\n        segment_block,\n        data,\n        dtype=np.uint8,  # Segmentation results are typically uint8\n        meta=np.array([], dtype=np.uint8),  # Provide meta information\n    )\n\n    # Create new NgffImage with segmented data\n    new_ngff_image = nz.NgffImage(\n        data=segmented_data,\n        dims=self.dims.copy(),\n        scale=self.scale.copy(),\n        translation=self.translation.copy(),\n        name=f\"{self.name}_segmented_{plugin.name.lower().replace(' ', '_')}\",\n    )\n\n    # Return new ZarrNii instance\n    return ZarrNii(\n        ngff_image=new_ngff_image,\n        axes_order=self.axes_order,\n        xyz_orientation=self.xyz_orientation,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.segment_otsu","title":"<code>zarrnii.ZarrNii.segment_otsu(nbins=256, chunk_size=None)</code>","text":"<p>Apply local Otsu thresholding segmentation to the image.</p> <p>Convenience method for local Otsu thresholding segmentation. This computes the threshold locally for each processing block.</p> <p>Parameters:</p> <ul> <li> <code>nbins</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>Number of bins for histogram computation (default: 256)</p> </li> <li> <code>chunk_size</code>               (<code>Optional[Tuple[int, ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional chunk size for dask processing</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>'ZarrNii'</code>           \u2013            <p>New ZarrNii instance with binary segmentation</p> </li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def segment_otsu(\n    self, nbins: int = 256, chunk_size: Optional[Tuple[int, ...]] = None\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Apply local Otsu thresholding segmentation to the image.\n\n    Convenience method for local Otsu thresholding segmentation.\n    This computes the threshold locally for each processing block.\n\n    Args:\n        nbins: Number of bins for histogram computation (default: 256)\n        chunk_size: Optional chunk size for dask processing\n\n    Returns:\n        New ZarrNii instance with binary segmentation\n    \"\"\"\n    from .plugins.segmentation import LocalOtsuSegmentation\n\n    plugin = LocalOtsuSegmentation(nbins=nbins)\n    return self.segment(plugin, chunk_size=chunk_size)\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.segment_threshold","title":"<code>zarrnii.ZarrNii.segment_threshold(thresholds, inclusive=True, chunk_size=None)</code>","text":"<p>Apply threshold-based segmentation to the image.</p> <p>Convenience method for threshold-based segmentation using either manual threshold values or computed thresholds.</p> <p>Parameters:</p> <ul> <li> <code>thresholds</code>               (<code>Union[float, List[float]]</code>)           \u2013            <p>Single threshold value or list of threshold values. For single threshold, creates binary segmentation (0/1). For multiple thresholds, creates multi-class segmentation (0/1/2/...).</p> </li> <li> <code>inclusive</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether thresholds are inclusive (default: True). If True, pixels &gt;= threshold are labeled as foreground. If False, pixels &gt; threshold are labeled as foreground.</p> </li> <li> <code>chunk_size</code>               (<code>Optional[Tuple[int, ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional chunk size for dask processing</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>'ZarrNii'</code>           \u2013            <p>New ZarrNii instance with labeled segmentation</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Binary threshold segmentation\n&gt;&gt;&gt; segmented = znimg.segment_threshold(0.5)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Multi-level threshold segmentation\n&gt;&gt;&gt; thresholds = znimg.compute_otsu_thresholds(classes=3)\n&gt;&gt;&gt; segmented = znimg.segment_threshold(thresholds[1:-1])  # Exclude min/max values\n</code></pre> Source code in <code>zarrnii/core.py</code> <pre><code>def segment_threshold(\n    self,\n    thresholds: Union[float, List[float]],\n    inclusive: bool = True,\n    chunk_size: Optional[Tuple[int, ...]] = None,\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Apply threshold-based segmentation to the image.\n\n    Convenience method for threshold-based segmentation using either\n    manual threshold values or computed thresholds.\n\n    Args:\n        thresholds: Single threshold value or list of threshold values.\n            For single threshold, creates binary segmentation (0/1).\n            For multiple thresholds, creates multi-class segmentation (0/1/2/...).\n        inclusive: Whether thresholds are inclusive (default: True).\n            If True, pixels &gt;= threshold are labeled as foreground.\n            If False, pixels &gt; threshold are labeled as foreground.\n        chunk_size: Optional chunk size for dask processing\n\n    Returns:\n        New ZarrNii instance with labeled segmentation\n\n    Examples:\n        &gt;&gt;&gt; # Binary threshold segmentation\n        &gt;&gt;&gt; segmented = znimg.segment_threshold(0.5)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Multi-level threshold segmentation\n        &gt;&gt;&gt; thresholds = znimg.compute_otsu_thresholds(classes=3)\n        &gt;&gt;&gt; segmented = znimg.segment_threshold(thresholds[1:-1])  # Exclude min/max values\n    \"\"\"\n    from .plugins.segmentation import ThresholdSegmentation\n\n    plugin = ThresholdSegmentation(thresholds=thresholds, inclusive=inclusive)\n    return self.segment(plugin, chunk_size=chunk_size)\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.compute_histogram","title":"<code>zarrnii.ZarrNii.compute_histogram(bins=None, range=None, mask=None, **kwargs)</code>","text":"<p>Compute histogram of the image.</p> <p>This method computes the histogram of image intensities, optionally using a mask to weight the computation. The histogram is computed using dask for efficient processing of large datasets.</p> <p>Parameters:</p> <ul> <li> <code>bins</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Number of histogram bins (default: bin width 1, bins=max - min + 1)</p> </li> <li> <code>range</code>               (<code>Optional[Tuple[float, float]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional tuple (min, max) defining histogram range. If None, uses the full range of the data</p> </li> <li> <code>mask</code>               (<code>Optional['ZarrNii']</code>, default:                   <code>None</code> )           \u2013            <p>Optional ZarrNii mask of same shape as image. Only pixels where mask &gt; 0 are included in histogram computation</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments passed to dask.array.histogram</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Array</code>           \u2013            <p>Tuple of (histogram_counts, bin_edges) where:</p> </li> <li> <code>Array</code>           \u2013            <ul> <li>histogram_counts: dask array of histogram bin counts</li> </ul> </li> <li> <code>Tuple[Array, Array]</code>           \u2013            <ul> <li>bin_edges: dask array of bin edge values (length = bins + 1)</li> </ul> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Compute histogram\n&gt;&gt;&gt; hist, bin_edges = znimg.compute_histogram(bins=128)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Compute histogram with mask\n&gt;&gt;&gt; mask = znimg &gt; 0.5\n&gt;&gt;&gt; hist_masked, _ = znimg.compute_histogram(mask=mask)\n</code></pre> Source code in <code>zarrnii/core.py</code> <pre><code>def compute_histogram(\n    self,\n    bins: Optional[int] = None,\n    range: Optional[Tuple[float, float]] = None,\n    mask: Optional[\"ZarrNii\"] = None,\n    **kwargs: Any,\n) -&gt; Tuple[da.Array, da.Array]:\n    \"\"\"\n    Compute histogram of the image.\n\n    This method computes the histogram of image intensities, optionally using\n    a mask to weight the computation. The histogram is computed using dask for\n    efficient processing of large datasets.\n\n    Args:\n        bins: Number of histogram bins (default: bin width 1, bins=max - min + 1)\n        range: Optional tuple (min, max) defining histogram range. If None,\n            uses the full range of the data\n        mask: Optional ZarrNii mask of same shape as image. Only pixels\n            where mask &gt; 0 are included in histogram computation\n        **kwargs: Additional arguments passed to dask.array.histogram\n\n    Returns:\n        Tuple of (histogram_counts, bin_edges) where:\n        - histogram_counts: dask array of histogram bin counts\n        - bin_edges: dask array of bin edge values (length = bins + 1)\n\n    Examples:\n        &gt;&gt;&gt; # Compute histogram\n        &gt;&gt;&gt; hist, bin_edges = znimg.compute_histogram(bins=128)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Compute histogram with mask\n        &gt;&gt;&gt; mask = znimg &gt; 0.5\n        &gt;&gt;&gt; hist_masked, _ = znimg.compute_histogram(mask=mask)\n    \"\"\"\n    from .analysis import compute_histogram\n\n    mask_data = mask.darr if mask is not None else None\n    return compute_histogram(\n        self.darr, bins=bins, range=range, mask=mask_data, **kwargs\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.compute_otsu_thresholds","title":"<code>zarrnii.ZarrNii.compute_otsu_thresholds(classes=2, bins=None, range=None, mask=None, return_figure=False)</code>","text":"<p>Compute Otsu multi-level thresholds for the image.</p> <p>This method first computes the histogram of the image, then uses scikit-image's threshold_multiotsu to compute optimal threshold values.</p> <p>Parameters:</p> <ul> <li> <code>classes</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>Number of classes to separate data into (default: 2). Must be &gt;= 2. For classes=2, returns 1 threshold. For classes=k, returns k-1 thresholds.</p> </li> <li> <code>bins</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Number of histogram bins (default: bin width 1, bins=max - min + 1)</p> </li> <li> <code>range</code>               (<code>Optional[Tuple[float, float]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional tuple (min, max) defining histogram range. If None, uses the full range of the data</p> </li> <li> <code>mask</code>               (<code>Optional['ZarrNii']</code>, default:                   <code>None</code> )           \u2013            <p>Optional ZarrNii mask of same shape as image. Only pixels where mask &gt; 0 are included in histogram computation</p> </li> <li> <code>return_figure</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns a tuple containing thresholds and a matplotlib figure with the histogram and annotated threshold lines (default: False).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[List[float], Tuple[List[float], Any]]</code>           \u2013            <p>If return_figure is False (default): List of threshold values. For classes=k, returns k+1 values: [0, threshold1, threshold2, ..., threshold_k-1, max_intensity] where 0 represents the minimum and max_intensity represents the maximum.</p> </li> <li> <code>Union[List[float], Tuple[List[float], Any]]</code>           \u2013            <p>If return_figure is True: Tuple of (thresholds, figure) where figure is a matplotlib Figure object showing the histogram with annotated threshold lines.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Compute binary threshold (2 classes)\n&gt;&gt;&gt; thresholds = znimg.compute_otsu_thresholds(classes=2)\n&gt;&gt;&gt; print(f\"Binary thresholds: {thresholds}\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Compute multi-level thresholds (3 classes)\n&gt;&gt;&gt; thresholds = znimg.compute_otsu_thresholds(classes=3)\n&gt;&gt;&gt; print(f\"Multi-level thresholds: {thresholds}\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get histogram data along with thresholds\n&gt;&gt;&gt; thresholds, (hist, bin_edges) = znimg.compute_otsu_thresholds(\n...     classes=2, return_histogram=True\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Generate a figure with annotated thresholds\n&gt;&gt;&gt; thresholds, fig = znimg.compute_otsu_thresholds(\n...     classes=2, return_figure=True\n... )\n&gt;&gt;&gt; fig.savefig('otsu_thresholds.png')\n</code></pre> Source code in <code>zarrnii/core.py</code> <pre><code>def compute_otsu_thresholds(\n    self,\n    classes: int = 2,\n    bins: Optional[int] = None,\n    range: Optional[Tuple[float, float]] = None,\n    mask: Optional[\"ZarrNii\"] = None,\n    return_figure: bool = False,\n) -&gt; Union[\n    List[float],\n    Tuple[List[float], Any],\n]:\n    \"\"\"\n    Compute Otsu multi-level thresholds for the image.\n\n    This method first computes the histogram of the image, then uses\n    scikit-image's threshold_multiotsu to compute optimal threshold values.\n\n    Args:\n        classes: Number of classes to separate data into (default: 2).\n            Must be &gt;= 2. For classes=2, returns 1 threshold. For classes=k,\n            returns k-1 thresholds.\n        bins: Number of histogram bins (default: bin width 1, bins=max - min + 1)\n        range: Optional tuple (min, max) defining histogram range. If None,\n            uses the full range of the data\n        mask: Optional ZarrNii mask of same shape as image. Only pixels\n            where mask &gt; 0 are included in histogram computation\n        return_figure: If True, returns a tuple containing thresholds and a\n            matplotlib figure with the histogram and annotated threshold lines\n            (default: False).\n\n    Returns:\n        If return_figure is False (default):\n            List of threshold values. For classes=k, returns k+1 values:\n            [0, threshold1, threshold2, ..., threshold_k-1, max_intensity]\n            where 0 represents the minimum and max_intensity represents the maximum.\n\n        If return_figure is True:\n            Tuple of (thresholds, figure) where figure is a matplotlib Figure\n            object showing the histogram with annotated threshold lines.\n\n    Examples:\n        &gt;&gt;&gt; # Compute binary threshold (2 classes)\n        &gt;&gt;&gt; thresholds = znimg.compute_otsu_thresholds(classes=2)\n        &gt;&gt;&gt; print(f\"Binary thresholds: {thresholds}\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Compute multi-level thresholds (3 classes)\n        &gt;&gt;&gt; thresholds = znimg.compute_otsu_thresholds(classes=3)\n        &gt;&gt;&gt; print(f\"Multi-level thresholds: {thresholds}\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Get histogram data along with thresholds\n        &gt;&gt;&gt; thresholds, (hist, bin_edges) = znimg.compute_otsu_thresholds(\n        ...     classes=2, return_histogram=True\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Generate a figure with annotated thresholds\n        &gt;&gt;&gt; thresholds, fig = znimg.compute_otsu_thresholds(\n        ...     classes=2, return_figure=True\n        ... )\n        &gt;&gt;&gt; fig.savefig('otsu_thresholds.png')\n    \"\"\"\n    from .analysis import compute_otsu_thresholds\n\n    # First compute histogram\n    hist, bin_edges = self.compute_histogram(bins=bins, range=range, mask=mask)\n\n    # Then compute thresholds with optional returns\n    return compute_otsu_thresholds(\n        hist,\n        classes=classes,\n        bin_edges=bin_edges,\n        return_figure=return_figure,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.create_mip","title":"<code>zarrnii.ZarrNii.create_mip(plane='axial', slab_thickness_um=100.0, slab_spacing_um=100.0, channel_colors=None, channel_ranges=None, channel_labels=None, return_slabs=False, scale_units='mm')</code>","text":"<p>Create Maximum Intensity Projection (MIP) visualizations across slabs.</p> <p>This method generates MIP visualizations by dividing the volume into slabs along the specified plane, computing the maximum intensity projection within each slab, then rendering with channel-specific colors. Returns lazy dask arrays that are computed only when explicitly requested.</p> <p>Parameters:</p> <ul> <li> <code>plane</code>               (<code>str</code>, default:                   <code>'axial'</code> )           \u2013            <p>Projection plane - one of 'axial', 'coronal', 'sagittal'. - 'axial': projects along z-axis (creates xy slices) - 'coronal': projects along y-axis (creates xz slices) - 'sagittal': projects along x-axis (creates yz slices)</p> </li> <li> <code>slab_thickness_um</code>               (<code>float</code>, default:                   <code>100.0</code> )           \u2013            <p>Thickness of each slab in microns (default: 100.0)</p> </li> <li> <code>slab_spacing_um</code>               (<code>float</code>, default:                   <code>100.0</code> )           \u2013            <p>Spacing between slab centers in microns (default: 100.0)</p> </li> <li> <code>channel_colors</code>               (<code>Optional[List[Union[str, Tuple[float, float, float], Tuple[float, float, float, float]]]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of colors for each channel. Each color can be: - Color name string (e.g., 'red', 'green', 'blue') - RGB tuple with values 0-1 (e.g., (1.0, 0.0, 0.0) for red) - RGBA tuple with values 0-1 (e.g., (1.0, 0.0, 0.0, 0.5) for semi-transparent red) If None and OMERO metadata is available, uses OMERO channel colors. Otherwise uses default colors: ['red', 'green', 'blue', 'cyan', 'magenta', 'yellow']</p> </li> <li> <code>channel_ranges</code>               (<code>Optional[List[Tuple[float, float]]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of (min, max) tuples specifying intensity range for each channel. If None and OMERO metadata is available, uses OMERO window settings. Otherwise uses auto-scaling based on data min/max.</p> </li> <li> <code>channel_labels</code>               (<code>Optional[List[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of channel label names to use for selecting channels from OMERO metadata. If provided, channels are filtered and reordered to match these labels. Requires OMERO metadata to be available.</p> </li> <li> <code>return_slabs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns tuple of (mip_list, slab_info_list) where slab_info_list contains metadata about each slab. If False (default), returns only the mip_list.</p> </li> <li> <code>scale_units</code>               (<code>str</code>, default:                   <code>'mm'</code> )           \u2013            <p>Units for scale values. Either \"mm\" (millimeters, default) or \"um\" (microns). The ZarrNii scale values from NGFF/NIfTI are in millimeters by default, so this should typically be left as \"mm\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[List[Array], Tuple[List[Array], List[dict]]]</code>           \u2013            <p>If return_slabs is False (default): List of 2D dask arrays, each containing an RGB MIP visualization for one slab. Each array has shape (height, width, 3) with RGB values in range [0, 1]. Arrays are lazy and will only be computed when explicitly requested.</p> </li> <li> <code>Union[List[Array], Tuple[List[Array], List[dict]]]</code>           \u2013            <p>If return_slabs is True: Tuple of (mip_list, slab_info_list) where: - mip_list: List of 2D RGB dask arrays as described above - slab_info_list: List of dictionaries with slab metadata including:     - 'start_um': Start position of slab in microns     - 'end_um': End position of slab in microns     - 'center_um': Center position of slab in microns     - 'start_idx': Start index in array coordinates     - 'end_idx': End index in array coordinates</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Create axial MIPs with custom intensity ranges\n&gt;&gt;&gt; mips = znimg.create_mip(\n...     plane='axial',\n...     slab_thickness_um=100.0,\n...     slab_spacing_um=100.0,\n...     channel_colors=['red', 'green'],\n...     channel_ranges=[(0.0, 1000.0), (0.0, 5000.0)]\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use OMERO metadata for colors and ranges\n&gt;&gt;&gt; mips = znimg.create_mip(\n...     plane='axial',\n...     channel_labels=['DAPI', 'GFP']\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use alpha transparency\n&gt;&gt;&gt; mips = znimg.create_mip(\n...     plane='axial',\n...     channel_colors=[(1.0, 0.0, 0.0, 0.7), (0.0, 1.0, 0.0, 0.5)]\n... )\n</code></pre> Source code in <code>zarrnii/core.py</code> <pre><code>def create_mip(\n    self,\n    plane: str = \"axial\",\n    slab_thickness_um: float = 100.0,\n    slab_spacing_um: float = 100.0,\n    channel_colors: Optional[\n        List[\n            Union[\n                str, Tuple[float, float, float], Tuple[float, float, float, float]\n            ]\n        ]\n    ] = None,\n    channel_ranges: Optional[List[Tuple[float, float]]] = None,\n    channel_labels: Optional[List[str]] = None,\n    return_slabs: bool = False,\n    scale_units: str = \"mm\",\n) -&gt; Union[List[da.Array], Tuple[List[da.Array], List[dict]]]:\n    \"\"\"\n    Create Maximum Intensity Projection (MIP) visualizations across slabs.\n\n    This method generates MIP visualizations by dividing the volume into slabs\n    along the specified plane, computing the maximum intensity projection within\n    each slab, then rendering with channel-specific colors. Returns lazy dask\n    arrays that are computed only when explicitly requested.\n\n    Args:\n        plane: Projection plane - one of 'axial', 'coronal', 'sagittal'.\n            - 'axial': projects along z-axis (creates xy slices)\n            - 'coronal': projects along y-axis (creates xz slices)\n            - 'sagittal': projects along x-axis (creates yz slices)\n        slab_thickness_um: Thickness of each slab in microns (default: 100.0)\n        slab_spacing_um: Spacing between slab centers in microns (default: 100.0)\n        channel_colors: Optional list of colors for each channel. Each color can be:\n            - Color name string (e.g., 'red', 'green', 'blue')\n            - RGB tuple with values 0-1 (e.g., (1.0, 0.0, 0.0) for red)\n            - RGBA tuple with values 0-1 (e.g., (1.0, 0.0, 0.0, 0.5) for semi-transparent red)\n            If None and OMERO metadata is available, uses OMERO channel colors.\n            Otherwise uses default colors: ['red', 'green', 'blue', 'cyan', 'magenta', 'yellow']\n        channel_ranges: Optional list of (min, max) tuples specifying intensity range\n            for each channel. If None and OMERO metadata is available, uses OMERO window\n            settings. Otherwise uses auto-scaling based on data min/max.\n        channel_labels: Optional list of channel label names to use for selecting\n            channels from OMERO metadata. If provided, channels are filtered and\n            reordered to match these labels. Requires OMERO metadata to be available.\n        return_slabs: If True, returns tuple of (mip_list, slab_info_list) where\n            slab_info_list contains metadata about each slab. If False (default),\n            returns only the mip_list.\n        scale_units: Units for scale values. Either \"mm\" (millimeters, default) or\n            \"um\" (microns). The ZarrNii scale values from NGFF/NIfTI are in millimeters\n            by default, so this should typically be left as \"mm\".\n\n    Returns:\n        If return_slabs is False (default):\n            List of 2D dask arrays, each containing an RGB MIP visualization for one slab.\n            Each array has shape (height, width, 3) with RGB values in range [0, 1].\n            Arrays are lazy and will only be computed when explicitly requested.\n\n        If return_slabs is True:\n            Tuple of (mip_list, slab_info_list) where:\n            - mip_list: List of 2D RGB dask arrays as described above\n            - slab_info_list: List of dictionaries with slab metadata including:\n                - 'start_um': Start position of slab in microns\n                - 'end_um': End position of slab in microns\n                - 'center_um': Center position of slab in microns\n                - 'start_idx': Start index in array coordinates\n                - 'end_idx': End index in array coordinates\n\n    Examples:\n        &gt;&gt;&gt; # Create axial MIPs with custom intensity ranges\n        &gt;&gt;&gt; mips = znimg.create_mip(\n        ...     plane='axial',\n        ...     slab_thickness_um=100.0,\n        ...     slab_spacing_um=100.0,\n        ...     channel_colors=['red', 'green'],\n        ...     channel_ranges=[(0.0, 1000.0), (0.0, 5000.0)]\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Use OMERO metadata for colors and ranges\n        &gt;&gt;&gt; mips = znimg.create_mip(\n        ...     plane='axial',\n        ...     channel_labels=['DAPI', 'GFP']\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Use alpha transparency\n        &gt;&gt;&gt; mips = znimg.create_mip(\n        ...     plane='axial',\n        ...     channel_colors=[(1.0, 0.0, 0.0, 0.7), (0.0, 1.0, 0.0, 0.5)]\n        ... )\n    \"\"\"\n    from .analysis import create_mip_visualization\n\n    return create_mip_visualization(\n        image=self.darr,\n        dims=self.dims,\n        scale=self.scale,\n        plane=plane,\n        slab_thickness_um=slab_thickness_um,\n        slab_spacing_um=slab_spacing_um,\n        channel_colors=channel_colors,\n        channel_ranges=channel_ranges,\n        omero_metadata=self.omero,\n        channel_labels=channel_labels,\n        return_slabs=return_slabs,\n        scale_units=scale_units,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.compute_centroids","title":"<code>zarrnii.ZarrNii.compute_centroids(depth=10, boundary='none', rechunk=None, output_path=None)</code>","text":"<p>Compute centroids of binary segmentation objects in physical coordinates.</p> <p>This method processes the binary image (typically output from a segmentation plugin) to identify connected components and compute their centroids in physical coordinates. It uses dask's map_overlap to efficiently process large images in chunks with overlap to handle objects that span chunk boundaries.</p> <p>For large datasets with many objects, use the output_path parameter to write centroids directly to a Parquet file on disk instead of returning them as a numpy array. This avoids memory issues when dealing with millions of objects.</p> <p>The input image should be binary (0/1 values) at the highest resolution. The function will: 1. Optionally rechunk the data for better processing efficiency 2. Add overlap padding to chunks (customizable via depth parameter) 3. Within each chunk:    - Label connected components using scikit-image    - Compute centroids using regionprops    - Convert to global voxel coordinates using block offsets    - Filter out centroids in overlap regions to avoid duplicates    - Convert to physical coordinates using the affine transform</p> <p>Parameters:</p> <ul> <li> <code>depth</code>               (<code>Union[int, Tuple[int, ...], Dict[int, int]]</code>, default:                   <code>10</code> )           \u2013            <p>Number of elements of overlap between chunks. Can be: - int: same depth for all dimensions (default: 10) - tuple: different depth per dimension - dict: mapping dimension index to depth</p> </li> <li> <code>boundary</code>               (<code>str</code>, default:                   <code>'none'</code> )           \u2013            <p>How to handle boundaries when adding overlap. Options include 'none', 'reflect', 'periodic', 'nearest', or constant values. Default is 'none' (no padding at array boundaries).</p> </li> <li> <code>rechunk</code>               (<code>Optional[Union[int, Tuple[int, ...]]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional rechunking specification before processing. Can be: - int: target chunk size for all dimensions - tuple: target chunk size per dimension - None: use existing chunks (default)</p> </li> <li> <code>output_path</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional path to write centroids to Parquet file instead of returning them in memory. If provided, centroids are written to this file path and None is returned. Use this for large datasets to avoid memory issues. The Parquet file will contain columns 'x', 'y', 'z' with physical coordinates. If None (default), centroids are returned as numpy array.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[ndarray]</code>           \u2013            <p>Optional[numpy.ndarray]: If output_path is None, returns Nx3 array of physical coordinates for N detected objects, where each row contains [x, y, z] coordinates in physical space. The array has dtype float64. If output_path is provided, writes to Parquet file and returns None.</p> </li> </ul> Notes <ul> <li>This method expects a binary image (e.g., from segment_threshold).</li> <li>Objects with centroids in overlap regions are filtered to avoid duplicates.</li> <li>Uses 26-connectivity (connectivity=3) for 3D connected component labeling.</li> <li>Empty chunks contribute no coordinates to the result.</li> <li>The result is computed immediately (not lazy).</li> <li>When using output_path, centroids are written in batches to avoid   memory overflow, making it suitable for datasets with millions of objects.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Apply threshold segmentation and compute centroids\n&gt;&gt;&gt; binary = znimg.segment_threshold(0.5)\n&gt;&gt;&gt; centroids = binary.compute_centroids(depth=5)\n&gt;&gt;&gt; print(f\"Found {len(centroids)} objects\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # With custom chunking\n&gt;&gt;&gt; centroids = binary.compute_centroids(\n...     depth=15,\n...     rechunk=(64, 64, 64)\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # For large datasets, write to Parquet file\n&gt;&gt;&gt; binary.compute_centroids(depth=5, output_path='centroids.parquet')\n&gt;&gt;&gt; # Read back with pandas or pyarrow\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.read_parquet('centroids.parquet')\n&gt;&gt;&gt; print(f\"Found {len(df)} objects\")\n</code></pre> Source code in <code>zarrnii/core.py</code> <pre><code>def compute_centroids(\n    self,\n    depth: Union[int, Tuple[int, ...], Dict[int, int]] = 10,\n    boundary: str = \"none\",\n    rechunk: Optional[Union[int, Tuple[int, ...]]] = None,\n    output_path: Optional[str] = None,\n) -&gt; Optional[np.ndarray]:\n    \"\"\"\n    Compute centroids of binary segmentation objects in physical coordinates.\n\n    This method processes the binary image (typically output from a segmentation\n    plugin) to identify connected components and compute their centroids in\n    physical coordinates. It uses dask's map_overlap to efficiently process\n    large images in chunks with overlap to handle objects that span chunk\n    boundaries.\n\n    For large datasets with many objects, use the output_path parameter to write\n    centroids directly to a Parquet file on disk instead of returning them as a\n    numpy array. This avoids memory issues when dealing with millions of objects.\n\n    The input image should be binary (0/1 values) at the highest resolution.\n    The function will:\n    1. Optionally rechunk the data for better processing efficiency\n    2. Add overlap padding to chunks (customizable via depth parameter)\n    3. Within each chunk:\n       - Label connected components using scikit-image\n       - Compute centroids using regionprops\n       - Convert to global voxel coordinates using block offsets\n       - Filter out centroids in overlap regions to avoid duplicates\n       - Convert to physical coordinates using the affine transform\n\n    Args:\n        depth: Number of elements of overlap between chunks. Can be:\n            - int: same depth for all dimensions (default: 10)\n            - tuple: different depth per dimension\n            - dict: mapping dimension index to depth\n        boundary: How to handle boundaries when adding overlap. Options include\n            'none', 'reflect', 'periodic', 'nearest', or constant values.\n            Default is 'none' (no padding at array boundaries).\n        rechunk: Optional rechunking specification before processing. Can be:\n            - int: target chunk size for all dimensions\n            - tuple: target chunk size per dimension\n            - None: use existing chunks (default)\n        output_path: Optional path to write centroids to Parquet file instead of\n            returning them in memory. If provided, centroids are written to this\n            file path and None is returned. Use this for large datasets to avoid\n            memory issues. The Parquet file will contain columns 'x', 'y', 'z' with\n            physical coordinates. If None (default), centroids are returned as numpy\n            array.\n\n    Returns:\n        Optional[numpy.ndarray]: If output_path is None, returns Nx3 array of\n            physical coordinates for N detected objects, where each row contains\n            [x, y, z] coordinates in physical space. The array has dtype float64.\n            If output_path is provided, writes to Parquet file and returns None.\n\n    Notes:\n        - This method expects a binary image (e.g., from segment_threshold).\n        - Objects with centroids in overlap regions are filtered to avoid duplicates.\n        - Uses 26-connectivity (connectivity=3) for 3D connected component labeling.\n        - Empty chunks contribute no coordinates to the result.\n        - The result is computed immediately (not lazy).\n        - When using output_path, centroids are written in batches to avoid\n          memory overflow, making it suitable for datasets with millions of objects.\n\n    Examples:\n        &gt;&gt;&gt; # Apply threshold segmentation and compute centroids\n        &gt;&gt;&gt; binary = znimg.segment_threshold(0.5)\n        &gt;&gt;&gt; centroids = binary.compute_centroids(depth=5)\n        &gt;&gt;&gt; print(f\"Found {len(centroids)} objects\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # With custom chunking\n        &gt;&gt;&gt; centroids = binary.compute_centroids(\n        ...     depth=15,\n        ...     rechunk=(64, 64, 64)\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # For large datasets, write to Parquet file\n        &gt;&gt;&gt; binary.compute_centroids(depth=5, output_path='centroids.parquet')\n        &gt;&gt;&gt; # Read back with pandas or pyarrow\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.read_parquet('centroids.parquet')\n        &gt;&gt;&gt; print(f\"Found {len(df)} objects\")\n    \"\"\"\n    from .analysis import compute_centroids\n\n    return compute_centroids(\n        self.darr,\n        affine=self.affine.matrix,\n        depth=depth,\n        boundary=boundary,\n        rechunk=rechunk,\n        output_path=output_path,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.apply_scaled_processing","title":"<code>zarrnii.ZarrNii.apply_scaled_processing(plugin, downsample_factor=4, chunk_size=None, upsampled_ome_zarr_path=None, **kwargs)</code>","text":"<p>Apply scaled processing plugin using multi-resolution approach.</p> <p>This method implements a multi-resolution processing pipeline where: 1. The image is downsampled for efficient computation 2. The plugin's lowres_func is applied to the downsampled data 3. The result is upsampled using dask-based upsampling 4. The plugin's highres_func applies the result to full-resolution data</p> <p>Parameters:</p> <ul> <li> <code>plugin</code>           \u2013            <p>ScaledProcessingPlugin instance or class to apply</p> </li> <li> <code>downsample_factor</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Factor for downsampling (default: 4)</p> </li> <li> <code>chunk_size</code>               (<code>Optional[Tuple[int, ...]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional chunk size for low-res processing. If None, uses (1, 10, 10, 10).</p> </li> <li> <code>upsampled_ome_zarr_path</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Path to save intermediate OME-Zarr, default saved in system temp directory.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments passed to the plugin</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>'ZarrNii'</code>           \u2013            <p>New ZarrNii instance with processed data</p> </li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def apply_scaled_processing(\n    self,\n    plugin,\n    downsample_factor: int = 4,\n    chunk_size: Optional[Tuple[int, ...]] = None,\n    upsampled_ome_zarr_path: Optional[str] = None,\n    **kwargs,\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Apply scaled processing plugin using multi-resolution approach.\n\n    This method implements a multi-resolution processing pipeline where:\n    1. The image is downsampled for efficient computation\n    2. The plugin's lowres_func is applied to the downsampled data\n    3. The result is upsampled using dask-based upsampling\n    4. The plugin's highres_func applies the result to full-resolution data\n\n    Args:\n        plugin: ScaledProcessingPlugin instance or class to apply\n        downsample_factor: Factor for downsampling (default: 4)\n        chunk_size: Optional chunk size for low-res processing. If None, uses (1, 10, 10, 10).\n        upsampled_ome_zarr_path: Path to save intermediate OME-Zarr, default saved in system temp directory.\n        **kwargs: Additional arguments passed to the plugin\n\n    Returns:\n        New ZarrNii instance with processed data\n    \"\"\"\n    from .plugins.scaled_processing import ScaledProcessingPlugin\n\n    # Handle plugin instance or class\n    if isinstance(plugin, type) and issubclass(plugin, ScaledProcessingPlugin):\n        plugin = plugin(**kwargs)\n    elif not isinstance(plugin, ScaledProcessingPlugin):\n        raise TypeError(\n            \"Plugin must be an instance or subclass of ScaledProcessingPlugin\"\n        )\n\n    # Step 1: Downsample the data for low-resolution processing\n    lowres_znimg = self.downsample(level=int(np.log2(downsample_factor)))\n\n    # Convert to numpy array for lowres processing\n    lowres_array = lowres_znimg.data.compute()\n\n    # Step 2: Apply low-resolution function and prepare for upsampling\n    # Use chunk_size parameter for the low-res processing chunks\n    lowres_chunks = chunk_size if chunk_size is not None else (1, 10, 10, 10)\n    lowres_znimg.data = da.from_array(\n        plugin.lowres_func(lowres_array), chunks=lowres_chunks\n    )\n\n    # Use temporary OME-Zarr to break up dask graph for performance\n    import tempfile\n\n    if upsampled_ome_zarr_path is None:\n        upsampled_ome_zarr_path = tempfile.mkdtemp(suffix=\"_SPIM.ome.zarr\")\n\n    # Step 3: Upsample using dask-based upsampling, save to ome zarr\n    lowres_znimg.upsample(to_shape=self.shape).to_ome_zarr(\n        upsampled_ome_zarr_path, max_layer=1\n    )\n\n    upsampled_znimg = ZarrNii.from_ome_zarr(upsampled_ome_zarr_path)\n\n    corrected_znimg = self.copy()\n\n    # Step 4: Apply high-resolution function\n    # rechunk original data to use same chunksize as upsampled_data, before multiplying\n    corrected_znimg.data = plugin.highres_func(\n        self.data.rechunk(upsampled_znimg.data.chunks), upsampled_znimg.data\n    )\n\n    return corrected_znimg\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNiiAtlas--attributes","title":"Attributes","text":"<p>labels_df : pandas.DataFrame     DataFrame containing label information for the atlas. label_column : str     Name of the column in labels_df containing label indices. name_column : str     Name of the column in labels_df containing region names. abbrev_column : str     Name of the column in labels_df containing region abbreviations.</p> Source code in <code>zarrnii/core.py</code> <pre><code>def __init__(\n    self,\n    darr=None,\n    axes_order=\"ZYX\",\n    orientation=\"RAS\",\n    xyz_orientation=None,\n    ngff_image=None,\n    spacing: Tuple[float, float, float] = (1.0, 1.0, 1.0),\n    origin: Tuple[float, float, float] = (0.0, 0.0, 0.0),\n    name: str = \"image\",\n    _omero: Optional[object] = None,\n    affine: Optional[AffineTransform] = None,\n    **kwargs,\n):\n    \"\"\"\n    Constructor with backward compatibility for old signature.\n\n    Raises:\n        ValueError: If affine parameter is provided\n    \"\"\"\n    # Check for deprecated affine parameter\n    if affine is not None:\n        raise ValueError(\n            \"The 'affine' parameter is no longer supported in ZarrNii(). \"\n            \"Please use 'spacing' and 'origin' parameters instead. \"\n            \"If you need to specify a full affine transformation, use from_nifti() \"\n            \"or construct the NgffImage directly.\"\n        )\n\n    # Handle backwards compatibility: if xyz_orientation is provided, use it\n    # Otherwise, use orientation for backwards compatibility\n    final_orientation = (\n        xyz_orientation if xyz_orientation is not None else orientation\n    )\n\n    if ngff_image is not None:\n        # New signature\n        object.__setattr__(self, \"ngff_image\", ngff_image)\n        object.__setattr__(self, \"axes_order\", axes_order)\n        object.__setattr__(self, \"xyz_orientation\", final_orientation)\n        object.__setattr__(self, \"_omero\", _omero)\n    elif darr is not None:\n        # Legacy signature - delegate to from_darr\n        instance = self.from_darr(\n            darr=darr,\n            axes_order=axes_order,\n            orientation=final_orientation,\n            spacing=spacing,\n            origin=origin,\n            name=name,\n            omero=_omero,\n            **kwargs,\n        )\n        object.__setattr__(self, \"ngff_image\", instance.ngff_image)\n        object.__setattr__(self, \"axes_order\", instance.axes_order)\n        object.__setattr__(self, \"xyz_orientation\", instance.xyz_orientation)\n        object.__setattr__(self, \"_omero\", instance._omero)\n    else:\n        raise ValueError(\"Must provide either ngff_image or darr\")\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNiiAtlas-attributes","title":"Attributes","text":""},{"location":"reference/#zarrnii.ZarrNiiAtlas.dseg","title":"<code>zarrnii.ZarrNiiAtlas.dseg</code>  <code>property</code>","text":"<p>Return self as the segmentation image (for compatibility with API).</p>"},{"location":"reference/#zarrnii.ZarrNiiAtlas-functions","title":"Functions","text":""},{"location":"reference/#zarrnii.ZarrNiiAtlas.create_from_dseg","title":"<code>zarrnii.ZarrNiiAtlas.create_from_dseg(dseg, labels_df, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create ZarrNiiAtlas from a dseg ZarrNii and labels DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>dseg</code>               (<code>ZarrNii</code>)           \u2013            <p>ZarrNii segmentation image</p> </li> <li> <code>labels_df</code>               (<code>DataFrame</code>)           \u2013            <p>DataFrame containing label information</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments for label/name/abbrev columns</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>ZarrNiiAtlas instance</p> </li> </ul> Source code in <code>zarrnii/atlas.py</code> <pre><code>@classmethod\ndef create_from_dseg(cls, dseg: ZarrNii, labels_df: pd.DataFrame, **kwargs):\n    \"\"\"Create ZarrNiiAtlas from a dseg ZarrNii and labels DataFrame.\n\n    Args:\n        dseg: ZarrNii segmentation image\n        labels_df: DataFrame containing label information\n        **kwargs: Additional keyword arguments for label/name/abbrev columns\n\n    Returns:\n        ZarrNiiAtlas instance\n    \"\"\"\n    if not isinstance(dseg, ZarrNii):\n        raise TypeError(f\"dseg must be a ZarrNii instance, got {type(dseg)}\")\n\n    # Note: attrs strips leading underscore from _omero in __init__ signature\n    # so we pass it as 'omero' instead of '_omero'\n    return cls(\n        ngff_image=dseg.ngff_image,\n        axes_order=dseg.axes_order,\n        xyz_orientation=dseg.xyz_orientation,\n        omero=getattr(dseg, \"_omero\", None),\n        labels_df=labels_df,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNiiAtlas.from_files","title":"<code>zarrnii.ZarrNiiAtlas.from_files(dseg_path, labels_path, **kwargs)</code>  <code>classmethod</code>","text":"<p>Load ZarrNiiAtlas from dseg image and labels TSV files.</p> <p>Parameters:</p> <ul> <li> <code>dseg_path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>Path to segmentation image (NIfTI or OME-Zarr)</p> </li> <li> <code>labels_path</code>               (<code>Union[str, Path]</code>)           \u2013            <p>Path to labels TSV file</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional arguments passed to ZarrNii.from_file()</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>ZarrNiiAtlas instance</p> </li> </ul> Source code in <code>zarrnii/atlas.py</code> <pre><code>@classmethod\ndef from_files(\n    cls, dseg_path: Union[str, Path], labels_path: Union[str, Path], **kwargs\n):\n    \"\"\"Load ZarrNiiAtlas from dseg image and labels TSV files.\n\n    Args:\n        dseg_path: Path to segmentation image (NIfTI or OME-Zarr)\n        labels_path: Path to labels TSV file\n        **kwargs: Additional arguments passed to ZarrNii.from_file()\n\n    Returns:\n        ZarrNiiAtlas instance\n    \"\"\"\n    # Load segmentation image\n    dseg = ZarrNii.from_file(str(dseg_path), **kwargs)\n\n    # Load labels dataframe\n    labels_df = pd.read_csv(str(labels_path), sep=\"\\t\")\n\n    # Create atlas instance using create_from_dseg\n    return cls.create_from_dseg(dseg, labels_df)\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNiiAtlas.from_itksnap_lut","title":"<code>zarrnii.ZarrNiiAtlas.from_itksnap_lut(path, lut_path, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct from itksnap lut file.</p> Source code in <code>zarrnii/atlas.py</code> <pre><code>@classmethod\ndef from_itksnap_lut(cls, path, lut_path, **kwargs):\n    \"\"\"\n    Construct from itksnap lut file.\n    \"\"\"\n    znii = super().from_file(path, **kwargs)\n    labels_df = cls._import_itksnap_lut(lut_path)\n    return cls(\n        ngff_image=znii.ngff_image,\n        axes_order=znii.axes_order,\n        xyz_orientation=znii.xyz_orientation,\n        labels_df=labels_df,\n        omero=getattr(znii, \"_omero\", None),\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNiiAtlas.from_csv_lut","title":"<code>zarrnii.ZarrNiiAtlas.from_csv_lut(path, lut_path, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct from csv lut file.</p> Source code in <code>zarrnii/atlas.py</code> <pre><code>@classmethod\ndef from_csv_lut(cls, path, lut_path, **kwargs):\n    \"\"\"\n    Construct from csv lut file.\n    \"\"\"\n    znii = super().from_file(path, **kwargs)\n    labels_df = cls._import_csv_lut(lut_path)\n    return cls(\n        ngff_image=znii.ngff_image,\n        axes_order=znii.axes_order,\n        xyz_orientation=znii.xyz_orientation,\n        labels_df=labels_df,\n        omero=getattr(znii, \"_omero\", None),\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNiiAtlas.from_tsv_lut","title":"<code>zarrnii.ZarrNiiAtlas.from_tsv_lut(path, lut_path, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct from tsv lut file.</p> Source code in <code>zarrnii/atlas.py</code> <pre><code>@classmethod\ndef from_tsv_lut(cls, path, lut_path, **kwargs):\n    \"\"\"\n    Construct from tsv lut file.\n    \"\"\"\n    znii = super().from_file(path, **kwargs)\n    labels_df = cls._import_tsv_lut(lut_path)\n    return cls(\n        ngff_image=znii.ngff_image,\n        axes_order=znii.axes_order,\n        xyz_orientation=znii.xyz_orientation,\n        labels_df=labels_df,\n        omero=getattr(znii, \"_omero\", None),\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNiiAtlas.from_labelmapper_lut","title":"<code>zarrnii.ZarrNiiAtlas.from_labelmapper_lut(path, lut_path, **kwargs)</code>  <code>classmethod</code>","text":"<p>Construct from labelmapper lut file.</p> Source code in <code>zarrnii/atlas.py</code> <pre><code>@classmethod\ndef from_labelmapper_lut(cls, path, lut_path, **kwargs):\n    \"\"\"\n    Construct from labelmapper lut file.\n    \"\"\"\n    znii = super().from_file(path, **kwargs)\n    labels_df = cls._import_labelmapper_lut(lut_path)\n    return cls(\n        ngff_image=znii.ngff_image,\n        axes_order=znii.axes_order,\n        xyz_orientation=znii.xyz_orientation,\n        labels_df=labels_df,\n        omero=getattr(znii, \"_omero\", None),\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNiiAtlas.get_region_info","title":"<code>zarrnii.ZarrNiiAtlas.get_region_info(region_id)</code>","text":"<p>Get information about a specific region.</p> <p>Parameters:</p> <ul> <li> <code>region_id</code>               (<code>Union[int, str]</code>)           \u2013            <p>Region identifier (int label, name, or abbreviation)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[str, Any]</code>           \u2013            <p>Dictionary containing region information</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If region not found in atlas</p> </li> </ul> Source code in <code>zarrnii/atlas.py</code> <pre><code>def get_region_info(self, region_id: Union[int, str]) -&gt; Dict[str, Any]:\n    \"\"\"Get information about a specific region.\n\n    Args:\n        region_id: Region identifier (int label, name, or abbreviation)\n\n    Returns:\n        Dictionary containing region information\n\n    Raises:\n        ValueError: If region not found in atlas\n    \"\"\"\n    label = self._resolve_region_identifier(region_id)\n\n    # Find the region in labels DataFrame\n    region_row = self.labels_df[self.labels_df[self.label_column] == label]\n    if region_row.empty:\n        raise ValueError(f\"Region with label {label} not found in atlas\")\n\n    return region_row.iloc[0].to_dict()\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNiiAtlas.get_region_mask","title":"<code>zarrnii.ZarrNiiAtlas.get_region_mask(region_id)</code>","text":"<p>Create binary mask for a specific region.</p> <p>Parameters:</p> <ul> <li> <code>region_id</code>               (<code>Union[int, str]</code>)           \u2013            <p>Region identifier (int label, name, or abbreviation)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ZarrNii</code>           \u2013            <p>ZarrNii instance containing binary mask (1 for region, 0 elsewhere)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If region not found in atlas</p> </li> </ul> Source code in <code>zarrnii/atlas.py</code> <pre><code>def get_region_mask(self, region_id: Union[int, str]) -&gt; ZarrNii:\n    \"\"\"Create binary mask for a specific region.\n\n    Args:\n        region_id: Region identifier (int label, name, or abbreviation)\n\n    Returns:\n        ZarrNii instance containing binary mask (1 for region, 0 elsewhere)\n\n    Raises:\n        ValueError: If region not found in atlas\n    \"\"\"\n    label = self._resolve_region_identifier(region_id)\n\n    # Validate that the region exists in our labels_df\n    if not (self.labels_df[self.label_column] == label).any():\n        raise ValueError(f\"Region with label {label} not found in atlas\")\n\n    # Create binary mask\n    mask_data = (self.dseg.data == label).astype(np.uint8)\n\n    mask_ngff = nz.NgffImage(\n        data=mask_data,\n        dims=self.dseg.ngff_image.dims.copy(),\n        scale=self.dseg.ngff_image.scale.copy(),\n        translation=self.dseg.ngff_image.translation.copy(),\n        name=f\"{self.name}_masked\",\n    )\n\n    return ZarrNii.from_ngff_image(\n        mask_ngff,\n        xyz_orientation=self.dseg.xyz_orientation,\n        axes_order=self.dseg.axes_order,\n        omero=self.dseg.omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNiiAtlas.get_region_volume","title":"<code>zarrnii.ZarrNiiAtlas.get_region_volume(region_id)</code>","text":"<p>Calculate volume of a specific region in mm\u00b3.</p> <p>Parameters:</p> <ul> <li> <code>region_id</code>               (<code>Union[int, str]</code>)           \u2013            <p>Region identifier (int label, name, or abbreviation)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>Volume in cubic millimeters</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If region not found in atlas</p> </li> </ul> Source code in <code>zarrnii/atlas.py</code> <pre><code>def get_region_volume(self, region_id: Union[int, str]) -&gt; float:\n    \"\"\"Calculate volume of a specific region in mm\u00b3.\n\n    Args:\n        region_id: Region identifier (int label, name, or abbreviation)\n\n    Returns:\n        Volume in cubic millimeters\n\n    Raises:\n        ValueError: If region not found in atlas\n    \"\"\"\n    label = self._resolve_region_identifier(region_id)\n\n    # Count voxels with this label\n    dseg_data = self.dseg.data\n    if hasattr(dseg_data, \"compute\"):\n        voxel_count = int((dseg_data == label).sum().compute())\n    else:\n        voxel_count = int((dseg_data == label).sum())\n\n    # Calculate volume using voxel size from affine\n    # Volume per voxel = abs(det(affine[:3, :3]))\n    voxel_volume = abs(np.linalg.det(self.dseg.affine[:3, :3]))\n\n    return float(voxel_count * voxel_volume)\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNiiAtlas.aggregate_image_by_regions","title":"<code>zarrnii.ZarrNiiAtlas.aggregate_image_by_regions(image, aggregation_func='mean', background_label=0, column_name=None, column_suffix=None)</code>","text":"<p>Aggregate image values by atlas regions.</p> <p>Parameters:</p> <ul> <li> <code>image</code>               (<code>ZarrNii</code>)           \u2013            <p>Image to aggregate (must be compatible with atlas)</p> </li> <li> <code>aggregation_func</code>               (<code>str</code>, default:                   <code>'mean'</code> )           \u2013            <p>Aggregation function ('mean', 'sum', 'std', 'median', 'min', 'max')</p> </li> <li> <code>background_label</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Label value to treat as background (excluded from results)</p> </li> <li> <code>column_name</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>String to use for column name. If None, uses f\"{aggregation_func}_value\"</p> </li> <li> <code>column_suffix</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>(Deprecated) String suffix to append to column name. Use column_name instead. If provided, column_name will be set to f\"{aggregation_func}_{column_suffix}\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>DataFrame with columns: index, name, {column_name}, volume</p> </li> <li> <code>DataFrame</code>           \u2013            <p>(e.g., with defaults: index, name, mean_value, volume)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If image and atlas are incompatible</p> </li> </ul> <p>.. deprecated:: 0.2.0     The <code>column_suffix</code> parameter is deprecated. Use <code>column_name</code> instead.</p> Source code in <code>zarrnii/atlas.py</code> <pre><code>def aggregate_image_by_regions(\n    self,\n    image: ZarrNii,\n    aggregation_func: str = \"mean\",\n    background_label: int = 0,\n    column_name: str = None,\n    column_suffix: str = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Aggregate image values by atlas regions.\n\n    Args:\n        image: Image to aggregate (must be compatible with atlas)\n        aggregation_func: Aggregation function ('mean', 'sum', 'std', 'median', 'min', 'max')\n        background_label: Label value to treat as background (excluded from results)\n        column_name: String to use for column name. If None, uses f\"{aggregation_func}_value\"\n        column_suffix: (Deprecated) String suffix to append to column name.\n            Use column_name instead. If provided, column_name will be set to\n            f\"{aggregation_func}_{column_suffix}\".\n\n    Returns:\n        DataFrame with columns: index, name, {column_name}, volume\n        (e.g., with defaults: index, name, mean_value, volume)\n\n    Raises:\n        ValueError: If image and atlas are incompatible\n\n    .. deprecated:: 0.2.0\n        The `column_suffix` parameter is deprecated. Use `column_name` instead.\n    \"\"\"\n    # Handle deprecated column_suffix parameter\n    if column_suffix is not None:\n        warnings.warn(\n            \"The 'column_suffix' parameter is deprecated and will be removed in a \"\n            \"future version. Use 'column_name' instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        if column_name is None:\n            column_name = f\"{aggregation_func}_{column_suffix}\"\n\n    # Set default column name if not provided\n    if column_name is None:\n        column_name = f\"{aggregation_func}_value\"\n\n    # Validate image compatibility\n    if not np.array_equal(image.shape, self.dseg.shape):\n        raise ValueError(\n            f\"Image shape {image.shape} doesn't match atlas shape {self.dseg.shape}\"\n        )\n\n    if not np.allclose(image.affine, self.dseg.affine, atol=1e-6):\n        warnings.warn(\n            \"Image and atlas affines don't match exactly. \"\n            \"Results may be spatially inconsistent.\"\n        )\n\n    # Get all unique labels (excluding background)\n    dseg_data = self.dseg.data\n    if hasattr(dseg_data, \"compute\"):\n        dseg_data = dseg_data.compute()\n    unique_labels = np.unique(dseg_data)\n    unique_labels = unique_labels[unique_labels != background_label]\n\n    results = []\n    for label in unique_labels:\n        # Create mask for this region\n        mask = self.dseg.data == label\n\n        # Extract image values for this region\n        region_values = image.data[mask]\n\n        # Skip if no voxels (shouldn't happen with unique labels)\n        if region_values.size == 0:\n            continue\n\n        # Compute aggregation\n        if hasattr(region_values, \"compute\"):\n            # Dask array - need to compute\n            if aggregation_func == \"mean\":\n                agg_value = float(region_values.mean().compute())\n            elif aggregation_func == \"sum\":\n                agg_value = float(region_values.sum().compute())\n            elif aggregation_func == \"std\":\n                agg_value = float(region_values.std().compute())\n            elif aggregation_func == \"median\":\n                agg_value = float(np.median(region_values.compute()))\n            elif aggregation_func == \"min\":\n                agg_value = float(region_values.min().compute())\n            elif aggregation_func == \"max\":\n                agg_value = float(region_values.max().compute())\n            else:\n                raise ValueError(\n                    f\"Unknown aggregation function: {aggregation_func}. \"\n                    \"Supported: mean, sum, std, median, min, max\"\n                )\n        else:\n            # NumPy array - direct computation\n            if aggregation_func == \"mean\":\n                agg_value = float(region_values.mean())\n            elif aggregation_func == \"sum\":\n                agg_value = float(region_values.sum())\n            elif aggregation_func == \"std\":\n                agg_value = float(region_values.std())\n            elif aggregation_func == \"median\":\n                agg_value = float(np.median(region_values))\n            elif aggregation_func == \"min\":\n                agg_value = float(region_values.min())\n            elif aggregation_func == \"max\":\n                agg_value = float(region_values.max())\n            else:\n                raise ValueError(\n                    f\"Unknown aggregation function: {aggregation_func}. \"\n                    \"Supported: mean, sum, std, median, min, max\"\n                )\n\n        # Get region info\n        try:\n            region_info = self.get_region_info(int(label))\n            region_name = region_info[self.name_column]\n        except ValueError:\n            region_name = f\"Unknown_Region_{label}\"\n\n        # Calculate volume\n        volume = self.get_region_volume(int(label))\n\n        results.append(\n            {\n                self.label_column: int(label),\n                self.name_column: region_name,\n                column_name: agg_value,\n                \"volume\": volume,\n            }\n        )\n\n    return pd.DataFrame(results)\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNiiAtlas.create_feature_map","title":"<code>zarrnii.ZarrNiiAtlas.create_feature_map(feature_data, feature_column, label_column='index')</code>","text":"<p>Create feature map by assigning values to atlas regions.</p> <p>Parameters:</p> <ul> <li> <code>feature_data</code>               (<code>DataFrame</code>)           \u2013            <p>DataFrame with region labels and feature values</p> </li> <li> <code>feature_column</code>               (<code>str</code>)           \u2013            <p>Column name containing feature values to map</p> </li> <li> <code>label_column</code>               (<code>str</code>, default:                   <code>'index'</code> )           \u2013            <p>Column name containing region labels</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ZarrNii</code>           \u2013            <p>ZarrNii instance with feature values mapped to regions</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If required columns are missing</p> </li> </ul> Source code in <code>zarrnii/atlas.py</code> <pre><code>def create_feature_map(\n    self,\n    feature_data: pd.DataFrame,\n    feature_column: str,\n    label_column: str = \"index\",\n) -&gt; ZarrNii:\n    \"\"\"Create feature map by assigning values to atlas regions.\n\n    Args:\n        feature_data: DataFrame with region labels and feature values\n        feature_column: Column name containing feature values to map\n        label_column: Column name containing region labels\n\n    Returns:\n        ZarrNii instance with feature values mapped to regions\n\n    Raises:\n        ValueError: If required columns are missing\n    \"\"\"\n    # Validate input\n    required_cols = [label_column, feature_column]\n    missing_cols = [col for col in required_cols if col not in feature_data.columns]\n    if missing_cols:\n        raise ValueError(f\"Missing columns in feature_data: {missing_cols}\")\n\n    dseg_data = self.dseg.data.astype(\"int\")  # dask array of labels\n\n    # make a dense lookup array\n    max_label = int(feature_data[label_column].max())\n    lut = np.zeros(max_label + 1, dtype=np.float32)\n    lut[feature_data[label_column].to_numpy(dtype=int)] = feature_data[\n        feature_column\n    ].to_numpy(dtype=float)\n\n    # broadcast the mapping in one go\n    feature_map = dseg_data.map_blocks(lambda block: lut[block], dtype=np.float32)\n\n    feature_map_ngff = nz.NgffImage(\n        data=feature_map,\n        dims=self.dseg.ngff_image.dims.copy(),\n        scale=self.dseg.ngff_image.scale.copy(),\n        translation=self.dseg.ngff_image.translation.copy(),\n        name=f\"{self.name}_feature_map\",\n    )\n\n    return ZarrNii.from_ngff_image(\n        feature_map_ngff,\n        xyz_orientation=self.dseg.xyz_orientation,\n        axes_order=self.dseg.axes_order,\n        omero=self.dseg.omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNiiAtlas.get_region_bounding_box","title":"<code>zarrnii.ZarrNiiAtlas.get_region_bounding_box(region_ids=None, regex=None)</code>","text":"<p>Get bounding box in physical coordinates for selected regions.</p> <p>This method computes the spatial extents (bounding box) of one or more atlas regions in physical/world coordinates. The returned bounding box can be used directly with the crop method to extract a subvolume containing the selected regions.</p> <p>Parameters:</p> <ul> <li> <code>region_ids</code>               (<code>Union[int, str, List[Union[int, str]]]</code>, default:                   <code>None</code> )           \u2013            <p>Region identifier(s) to include in bounding box. Can be: - Single int: label index - Single str: region name or abbreviation - List[int/str]: multiple regions by index, name, or abbreviation - None: use regex parameter instead</p> </li> <li> <code>regex</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Regular expression to match region names. If provided, region_ids must be None. Case-insensitive matching.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tuple[float, float, float]</code>           \u2013            <p>Tuple of (bbox_min, bbox_max) where each is a tuple of (x, y, z)</p> </li> <li> <code>Tuple[float, float, float]</code>           \u2013            <p>coordinates in physical/world space (mm). These can be passed</p> </li> <li> <code>Tuple[Tuple[float, float, float], Tuple[float, float, float]]</code>           \u2013            <p>directly to ZarrNii.crop() method with physical_coords=True.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no regions match the selection criteria, or if both region_ids and regex are provided/omitted</p> </li> <li> <code>TypeError</code>             \u2013            <p>If region_ids contains invalid types</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Get bounding box for single region\n&gt;&gt;&gt; bbox_min, bbox_max = atlas.get_region_bounding_box(\"Hippocampus\")\n&gt;&gt;&gt; cropped = image.crop(bbox_min, bbox_max, physical_coords=True)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get bounding box for multiple regions\n&gt;&gt;&gt; bbox_min, bbox_max = atlas.get_region_bounding_box([\"Hippocampus\", \"Amygdala\"])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use regex to select regions\n&gt;&gt;&gt; bbox_min, bbox_max = atlas.get_region_bounding_box(regex=\"Hip.*\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Crop atlas itself to region\n&gt;&gt;&gt; cropped_atlas = atlas.crop(bbox_min, bbox_max, physical_coords=True)\n</code></pre> Notes <ul> <li>Bounding box is in physical coordinates (mm), not voxel indices</li> <li>Axes ordering is relative to self.axes_order (e.g. ZYX for ome zarr)</li> <li>The bounding box is the union of all selected regions</li> <li>Use the returned values with crop(physical_coords=True)</li> </ul> Source code in <code>zarrnii/atlas.py</code> <pre><code>def get_region_bounding_box(\n    self,\n    region_ids: Union[int, str, List[Union[int, str]]] = None,\n    regex: Optional[str] = None,\n) -&gt; Tuple[Tuple[float, float, float], Tuple[float, float, float]]:\n    \"\"\"Get bounding box in physical coordinates for selected regions.\n\n    This method computes the spatial extents (bounding box) of one or more\n    atlas regions in physical/world coordinates. The returned bounding box\n    can be used directly with the crop method to extract a subvolume\n    containing the selected regions.\n\n    Args:\n        region_ids: Region identifier(s) to include in bounding box. Can be:\n            - Single int: label index\n            - Single str: region name or abbreviation\n            - List[int/str]: multiple regions by index, name, or abbreviation\n            - None: use regex parameter instead\n        regex: Regular expression to match region names. If provided,\n            region_ids must be None. Case-insensitive matching.\n\n    Returns:\n        Tuple of (bbox_min, bbox_max) where each is a tuple of (x, y, z)\n        coordinates in physical/world space (mm). These can be passed\n        directly to ZarrNii.crop() method with physical_coords=True.\n\n    Raises:\n        ValueError: If no regions match the selection criteria, or if both\n            region_ids and regex are provided/omitted\n        TypeError: If region_ids contains invalid types\n\n    Examples:\n        &gt;&gt;&gt; # Get bounding box for single region\n        &gt;&gt;&gt; bbox_min, bbox_max = atlas.get_region_bounding_box(\"Hippocampus\")\n        &gt;&gt;&gt; cropped = image.crop(bbox_min, bbox_max, physical_coords=True)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Get bounding box for multiple regions\n        &gt;&gt;&gt; bbox_min, bbox_max = atlas.get_region_bounding_box([\"Hippocampus\", \"Amygdala\"])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Use regex to select regions\n        &gt;&gt;&gt; bbox_min, bbox_max = atlas.get_region_bounding_box(regex=\"Hip.*\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Crop atlas itself to region\n        &gt;&gt;&gt; cropped_atlas = atlas.crop(bbox_min, bbox_max, physical_coords=True)\n\n    Notes:\n        - Bounding box is in physical coordinates (mm), not voxel indices\n        - Axes ordering is relative to self.axes_order (e.g. ZYX for ome zarr)\n        - The bounding box is the union of all selected regions\n        - Use the returned values with crop(physical_coords=True)\n    \"\"\"\n    import re\n\n    import dask.array as da\n\n    # Validate input parameters\n    if region_ids is None and regex is None:\n        raise ValueError(\"Must provide either region_ids or regex parameter\")\n    if region_ids is not None and regex is not None:\n        raise ValueError(\"Cannot provide both region_ids and regex parameters\")\n\n    # Determine which labels to include\n    selected_labels = []\n\n    if regex is not None:\n        # Match regions using regex\n        pattern = re.compile(regex, re.IGNORECASE)\n        for _, row in self.labels_df.iterrows():\n            region_name = str(row[self.name_column])\n            if pattern.search(region_name):\n                selected_labels.append(int(row[self.label_column]))\n\n        if not selected_labels:\n            raise ValueError(f\"No regions matched regex pattern: {regex}\")\n    else:\n        # Convert region_ids to list if single value\n        if not isinstance(region_ids, list):\n            region_ids = [region_ids]\n\n        # Resolve each region identifier to label\n        for region_id in region_ids:\n            label = self._resolve_region_identifier(region_id)\n            selected_labels.append(label)\n\n    # Create union mask of all selected regions\n    dseg_data = self.dseg.data\n    mask = None\n    for label in selected_labels:\n        region_mask = dseg_data == label\n        if mask is None:\n            mask = region_mask\n        else:\n            mask = mask | region_mask\n\n    # Find voxel coordinates where mask is True\n    # da.where returns tuple of arrays (one per dimension in data array)\n    indices = da.where(mask)\n\n    # Compute the indices to get actual coordinates\n    indices_computed = [idx.compute() for idx in indices]\n\n    # Check if any voxels were found\n    if any(idx.size == 0 for idx in indices_computed):\n        raise ValueError(f\"No voxels found for selected regions: {selected_labels}\")\n\n    # Get the spatial dimensions from dims (skip non-spatial like 'c', 't')\n    spatial_dims_lower = [d.lower() for d in [\"x\", \"y\", \"z\"]]\n    spatial_indices = []\n    for i, dim in enumerate(self.dseg.dims):\n        if dim.lower() in spatial_dims_lower:\n            spatial_indices.append(i)\n\n    # Extract spatial coordinates from indices\n    # indices_computed has one array per dimension in data\n    voxel_coords = []\n    for spatial_idx in spatial_indices:\n        voxel_coords.append(indices_computed[spatial_idx])\n\n    # Get min and max for each spatial dimension\n    voxel_mins = [int(coords.min()) for coords in voxel_coords]\n    voxel_maxs = [\n        int(coords.max()) + 1 for coords in voxel_coords\n    ]  # +1 for inclusive max\n\n    # Now we have voxel coordinates in the order they appear in dims\n    # We don't need to convert to (x, y, z) order for physical coordinates\n    #  since the affine should do this already..\n\n    # make homog coords so we can matrix mult\n    voxel_min_xyz = np.array(\n        [\n            voxel_mins[0],\n            voxel_mins[1],\n            voxel_mins[2],\n            1.0,\n        ]\n    )\n    voxel_max_xyz = np.array(\n        [\n            voxel_maxs[0],\n            voxel_maxs[1],\n            voxel_maxs[2],\n            1.0,\n        ]\n    )\n\n    # Transform to physical coordinates using affine\n    affine_matrix = self.dseg.affine.matrix\n    physical_min = affine_matrix @ voxel_min_xyz\n    physical_max = affine_matrix @ voxel_max_xyz\n\n    # Return as tuples of (x, y, z) in physical space\n    bbox_min = tuple(physical_min[:3].tolist())\n    bbox_max = tuple(physical_max[:3].tolist())\n\n    return bbox_min, bbox_max\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNiiAtlas.sample_region_patches","title":"<code>zarrnii.ZarrNiiAtlas.sample_region_patches(n_patches, region_ids=None, regex=None, seed=None)</code>","text":"<p>Sample random coordinates (centers) within atlas regions.</p> <p>This method generates a list of center coordinates by randomly sampling voxels within the selected atlas regions. The returned coordinates are in physical/world space (mm) and can be used with crop_centered() to extract fixed-size patches for machine learning training or other workflows.</p> <p>Parameters:</p> <ul> <li> <code>n_patches</code>               (<code>int</code>)           \u2013            <p>Number of patch centers to sample</p> </li> <li> <code>region_ids</code>               (<code>Union[int, str, List[Union[int, str]]]</code>, default:                   <code>None</code> )           \u2013            <p>Region identifier(s) to sample from. Can be: - Single int: label index - Single str: region name or abbreviation - List[int/str]: multiple regions by index, name, or abbreviation - None: use regex parameter instead</p> </li> <li> <code>regex</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Regular expression to match region names. If provided, region_ids must be None. Case-insensitive matching.</p> </li> <li> <code>seed</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Random seed for reproducibility. If None, patches are sampled randomly each time.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Tuple[float, float, float]]</code>           \u2013            <p>List of (x, y, z) coordinates in physical/world space (mm).</p> </li> <li> <code>List[Tuple[float, float, float]]</code>           \u2013            <p>Each coordinate represents the center of a potential patch and</p> </li> <li> <code>List[Tuple[float, float, float]]</code>           \u2013            <p>can be used with crop_centered() to extract fixed-size regions.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no regions match the selection criteria, if both region_ids and regex are provided/omitted, or if n_patches is less than 1</p> </li> <li> <code>TypeError</code>             \u2013            <p>If region_ids contains invalid types</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Sample 10 patch centers from hippocampus\n&gt;&gt;&gt; centers = atlas.sample_region_patches(\n...     n_patches=10,\n...     region_ids=\"Hippocampus\",\n...     seed=42\n... )\n&gt;&gt;&gt; # Extract 256x256x256 voxel patches at each center\n&gt;&gt;&gt; patches = image.crop_centered(centers, patch_size=(256, 256, 256))\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Sample from multiple regions using list\n&gt;&gt;&gt; centers = atlas.sample_region_patches(\n...     n_patches=20,\n...     region_ids=[1, 2, 3],\n...     seed=42\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Sample using regex pattern\n&gt;&gt;&gt; centers = atlas.sample_region_patches(\n...     n_patches=5,\n...     regex=\".*cortex.*\",\n... )\n</code></pre> Notes <ul> <li>Coordinates are in physical space (mm), not voxel indices</li> <li>Centers are sampled uniformly from voxels within selected regions</li> <li>Use crop_centered() to extract fixed-size patches around these centers</li> <li>For ML training with fixed patch sizes (e.g., 256x256x256 voxels),   use a lower-resolution atlas to define masks, then crop at higher   resolution using physical coordinates</li> </ul> Source code in <code>zarrnii/atlas.py</code> <pre><code>def sample_region_patches(\n    self,\n    n_patches: int,\n    region_ids: Union[int, str, List[Union[int, str]]] = None,\n    regex: Optional[str] = None,\n    seed: Optional[int] = None,\n) -&gt; List[Tuple[float, float, float]]:\n    \"\"\"Sample random coordinates (centers) within atlas regions.\n\n    This method generates a list of center coordinates by randomly sampling\n    voxels within the selected atlas regions. The returned coordinates are\n    in physical/world space (mm) and can be used with crop_centered() to\n    extract fixed-size patches for machine learning training or other workflows.\n\n    Args:\n        n_patches: Number of patch centers to sample\n        region_ids: Region identifier(s) to sample from. Can be:\n            - Single int: label index\n            - Single str: region name or abbreviation\n            - List[int/str]: multiple regions by index, name, or abbreviation\n            - None: use regex parameter instead\n        regex: Regular expression to match region names. If provided,\n            region_ids must be None. Case-insensitive matching.\n        seed: Random seed for reproducibility. If None, patches are sampled\n            randomly each time.\n\n    Returns:\n        List of (x, y, z) coordinates in physical/world space (mm).\n        Each coordinate represents the center of a potential patch and\n        can be used with crop_centered() to extract fixed-size regions.\n\n    Raises:\n        ValueError: If no regions match the selection criteria, if both\n            region_ids and regex are provided/omitted, or if n_patches is\n            less than 1\n        TypeError: If region_ids contains invalid types\n\n    Examples:\n        &gt;&gt;&gt; # Sample 10 patch centers from hippocampus\n        &gt;&gt;&gt; centers = atlas.sample_region_patches(\n        ...     n_patches=10,\n        ...     region_ids=\"Hippocampus\",\n        ...     seed=42\n        ... )\n        &gt;&gt;&gt; # Extract 256x256x256 voxel patches at each center\n        &gt;&gt;&gt; patches = image.crop_centered(centers, patch_size=(256, 256, 256))\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Sample from multiple regions using list\n        &gt;&gt;&gt; centers = atlas.sample_region_patches(\n        ...     n_patches=20,\n        ...     region_ids=[1, 2, 3],\n        ...     seed=42\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Sample using regex pattern\n        &gt;&gt;&gt; centers = atlas.sample_region_patches(\n        ...     n_patches=5,\n        ...     regex=\".*cortex.*\",\n        ... )\n\n    Notes:\n        - Coordinates are in physical space (mm), not voxel indices\n        - Centers are sampled uniformly from voxels within selected regions\n        - Use crop_centered() to extract fixed-size patches around these centers\n        - For ML training with fixed patch sizes (e.g., 256x256x256 voxels),\n          use a lower-resolution atlas to define masks, then crop at higher\n          resolution using physical coordinates\n    \"\"\"\n    import re\n\n    import dask.array as da\n\n    # Validate input\n    if n_patches &lt; 1:\n        raise ValueError(f\"n_patches must be at least 1, got {n_patches}\")\n\n    if region_ids is None and regex is None:\n        raise ValueError(\"Must provide either region_ids or regex parameter\")\n    if region_ids is not None and regex is not None:\n        raise ValueError(\"Cannot provide both region_ids and regex parameters\")\n\n    # Set random seed if provided\n    if seed is not None:\n        np.random.seed(seed)\n\n    # Determine which labels to include (reuse logic from get_region_bounding_box)\n    selected_labels = []\n\n    if regex is not None:\n        # Match regions using regex\n        pattern = re.compile(regex, re.IGNORECASE)\n        for _, row in self.labels_df.iterrows():\n            region_name = str(row[self.name_column])\n            if pattern.search(region_name):\n                selected_labels.append(int(row[self.label_column]))\n\n        if not selected_labels:\n            raise ValueError(f\"No regions matched regex pattern: {regex}\")\n    else:\n        # Convert region_ids to list if single value\n        if not isinstance(region_ids, list):\n            region_ids = [region_ids]\n\n        # Resolve each region identifier to label\n        for region_id in region_ids:\n            label = self._resolve_region_identifier(region_id)\n            selected_labels.append(label)\n\n    # Create union mask of all selected regions\n    dseg_data = self.dseg.data\n    mask = None\n    for label in selected_labels:\n        region_mask = dseg_data == label\n        if mask is None:\n            mask = region_mask\n        else:\n            mask = mask | region_mask\n\n    # Find voxel coordinates where mask is True\n    indices = da.where(mask)\n\n    # Compute the indices to get actual coordinates\n    indices_computed = [idx.compute() for idx in indices]\n\n    # Check if any voxels were found\n    if any(idx.size == 0 for idx in indices_computed):\n        raise ValueError(f\"No voxels found for selected regions: {selected_labels}\")\n\n    # Get number of valid voxels\n    n_voxels = indices_computed[0].size\n\n    # Sample random voxels\n    # If n_patches &gt; n_voxels, sample with replacement\n    replace = n_patches &gt; n_voxels\n    sampled_indices = np.random.choice(n_voxels, size=n_patches, replace=replace)\n\n    # Get spatial dimensions (skip non-spatial like 'c', 't')\n    spatial_dims_lower = [d.lower() for d in [\"x\", \"y\", \"z\"]]\n    spatial_indices = []\n    for i, dim in enumerate(self.dseg.dims):\n        if dim.lower() in spatial_dims_lower:\n            spatial_indices.append(i)\n\n    # Build voxel coordinates for sampled centers\n    sampled_coords = []\n    for spatial_idx in spatial_indices:\n        sampled_coords.append(indices_computed[spatial_idx][sampled_indices])\n\n    # Map to x, y, z order\n    dim_to_coords = {}\n    for i, spatial_idx in enumerate(spatial_indices):\n        dim_name = self.dseg.dims[spatial_idx].lower()\n        dim_to_coords[dim_name] = sampled_coords[i]\n\n    # Get affine matrix\n    affine_matrix = self.dseg.affine.matrix\n\n    # Generate center coordinates in physical space\n    centers = []\n    for i in range(n_patches):\n        # Get center voxel coordinates in (x, y, z) order\n        center_voxel_xyz = np.array(\n            [\n                dim_to_coords[\"x\"][i],\n                dim_to_coords[\"y\"][i],\n                dim_to_coords[\"z\"][i],\n                1.0,\n            ]\n        )\n\n        # Transform to physical coordinates\n        center_physical = affine_matrix @ center_voxel_xyz\n        center_xyz = center_physical[:3]\n\n        # Convert to tuple\n        center = tuple(center_xyz.tolist())\n        centers.append(center)\n\n    return centers\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNiiAtlas.label_centroids","title":"<code>zarrnii.ZarrNiiAtlas.label_centroids(centroids, include_names=True)</code>","text":"<p>Map centroids to atlas labels using nearest neighbor interpolation.</p> <p>This method takes a set of centroids (typically from compute_centroids) and determines which atlas region each centroid falls into. It uses nearest neighbor interpolation to assign labels, making it robust to small coordinate mismatches.</p> <p>Parameters:</p> <ul> <li> <code>centroids</code>               (<code>ndarray</code>)           \u2013            <p>Nx3 numpy array of centroid coordinates in physical space (typically output from compute_centroids). Each row is [x, y, z] in physical/world coordinates (mm). Can also be an empty array (0, 3).</p> </li> <li> <code>include_names</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, includes region names from the labels dataframe in the output (default: True).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[DataFrame, DataFrame]</code>           \u2013            <p>tuple of two pandas DataFrames: 1. centroids DataFrame with columns:     - x, y, z: Physical coordinates (in mm) of each centroid     - index: Integer label index from the atlas     - name (optional): Region name if include_names=True 2. counts DataFrame with columns:     - index: Integer label index from the atlas     - name (optional): Region name if include_names=True     - count: Number of centroids in each region</p> </li> </ul> Notes <ul> <li>Input centroids must be in the same physical space as the atlas</li> <li>Points outside the atlas bounds receive index=0 (background)</li> <li>Uses scipy.interpolate.interpn with method='nearest' for label lookup</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Compute centroids from a segmentation\n&gt;&gt;&gt; centroids = binary_seg.compute_centroids()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Map centroids to atlas labels\n&gt;&gt;&gt; df_centroids, df_counts = atlas.label_centroids(centroids)\n&gt;&gt;&gt; print(df_centroids)\n&gt;&gt;&gt; print(df_counts)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Filter to specific regions\n&gt;&gt;&gt; hippocampus_points = df_centroids[\n...     df_centroids['name'] == 'Hippocampus'\n... ]\n</code></pre> Source code in <code>zarrnii/atlas.py</code> <pre><code>def label_centroids(\n    self,\n    centroids: np.ndarray,\n    include_names: bool = True,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Map centroids to atlas labels using nearest neighbor interpolation.\n\n    This method takes a set of centroids (typically from compute_centroids)\n    and determines which atlas region each centroid falls into. It uses\n    nearest neighbor interpolation to assign labels, making it robust to\n    small coordinate mismatches.\n\n    Args:\n        centroids: Nx3 numpy array of centroid coordinates in physical space\n            (typically output from compute_centroids). Each row is [x, y, z]\n            in physical/world coordinates (mm). Can also be an empty array (0, 3).\n        include_names: If True, includes region names from the labels dataframe\n            in the output (default: True).\n\n    Returns:\n        tuple of two pandas DataFrames:\n            1. centroids DataFrame with columns:\n                - x, y, z: Physical coordinates (in mm) of each centroid\n                - index: Integer label index from the atlas\n                - name (optional): Region name if include_names=True\n            2. counts DataFrame with columns:\n                - index: Integer label index from the atlas\n                - name (optional): Region name if include_names=True\n                - count: Number of centroids in each region\n\n    Notes:\n        - Input centroids must be in the same physical space as the atlas\n        - Points outside the atlas bounds receive index=0 (background)\n        - Uses scipy.interpolate.interpn with method='nearest' for label lookup\n\n    Examples:\n        &gt;&gt;&gt; # Compute centroids from a segmentation\n        &gt;&gt;&gt; centroids = binary_seg.compute_centroids()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Map centroids to atlas labels\n        &gt;&gt;&gt; df_centroids, df_counts = atlas.label_centroids(centroids)\n        &gt;&gt;&gt; print(df_centroids)\n        &gt;&gt;&gt; print(df_counts)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Filter to specific regions\n        &gt;&gt;&gt; hippocampus_points = df_centroids[\n        ...     df_centroids['name'] == 'Hippocampus'\n        ... ]\n    \"\"\"\n    # Handle empty centroids array\n    if centroids.shape[0] == 0:\n        columns_centroids = [\"x\", \"y\", \"z\", \"index\"]\n        columns_counts = [\"index\"]\n        if include_names:\n            columns_centroids.append(\"name\")\n            columns_counts.append(\"name\")\n        columns_counts.append(\"count\")\n        return (\n            pd.DataFrame(columns=columns_centroids),\n            pd.DataFrame(columns=columns_counts),\n        )\n\n    # Validate input shape\n    if centroids.ndim != 2 or centroids.shape[1] != 3:\n        raise ValueError(\n            f\"centroids must be Nx3 array, got shape {centroids.shape}\"\n        )\n\n    # Get atlas data and affine\n    dseg_data = self.dseg.data\n    if hasattr(dseg_data, \"compute\"):\n        dseg_data = dseg_data.compute()\n\n    affine_matrix = self.dseg.affine.matrix\n\n    # Convert physical coordinates to voxel coordinates\n    # centroids are in (x, y, z) order\n    # Create homogeneous coordinates\n    n_points = centroids.shape[0]\n    centroids_homogeneous = np.column_stack(\n        [centroids, np.ones((n_points, 1), dtype=np.float64)]\n    )\n\n    # Apply inverse affine transform\n    affine_inv = np.linalg.inv(affine_matrix)\n    voxel_coords_homogeneous = centroids_homogeneous @ affine_inv.T\n    voxel_coords = voxel_coords_homogeneous[:, :3]\n\n    # Create grid for interpn\n    # Grid should be in the order of the data array dimensions\n    # For ZarrNii, this is typically (z, y, x) or (c, z, y, x)\n    # Remove channel dimension if present\n    if dseg_data.ndim == 4:\n        dseg_data = dseg_data[0]  # Remove channel dimension\n\n    # Create coordinate grids for each dimension\n    # interpn expects a tuple of 1D arrays representing the grid coordinates\n    shape = dseg_data.shape\n    grid = tuple(np.arange(s) for s in shape)\n\n    # The inverse affine transform already converted physical (x, y, z) coordinates\n    # to voxel coordinates in the order matching the data array axes_order.\n    # So voxel_coords is already in the correct order for interpn!\n\n    # Use interpn to get labels at the centroid locations\n    label_at_points = interpn(\n        grid,\n        dseg_data,\n        voxel_coords,\n        method=\"nearest\",\n        bounds_error=False,\n        fill_value=0,\n    )\n\n    # Convert to integer labels\n    label_at_points = label_at_points.astype(int)\n\n    # Create DataFrame with x, y, z columns\n    # centroids are in (x, y, z) order, so we use that order for output\n    df_data = {\n        \"x\": centroids[:, 0],  # x from centroids\n        \"y\": centroids[:, 1],  # y from centroids\n        \"z\": centroids[:, 2],  # z from centroids\n        \"index\": label_at_points,\n    }\n\n    # Add region names if requested\n    if include_names and self.labels_df is not None:\n        # Create a lookup from index to name\n        label_to_name = dict(\n            zip(\n                self.labels_df[self.label_column],\n                self.labels_df[self.name_column],\n            )\n        )\n        # Map labels to names, use 'Unknown' for labels not in lookup table\n        df_data[\"name\"] = [\n            label_to_name.get(label, f\"Unknown_Label_{label}\")\n            for label in label_at_points\n        ]\n\n    df_centroids = pd.DataFrame(df_data)\n\n    # Create counts DataFrame - group by index and optionally name\n    if include_names and self.labels_df is not None:\n        df_counts = (\n            df_centroids.groupby([\"index\", \"name\"]).size().reset_index(name=\"count\")\n        )\n    else:\n        df_counts = df_centroids.groupby([\"index\"]).size().reset_index(name=\"count\")\n\n    return (df_centroids, df_counts)\n</code></pre>"},{"location":"reference/#zarrnii.transform.AffineTransform-functions","title":"Functions","text":""},{"location":"reference/#zarrnii.transform.AffineTransform.from_txt","title":"<code>zarrnii.transform.AffineTransform.from_txt(path, invert=False)</code>  <code>classmethod</code>","text":"<p>Create AffineTransform from text file containing matrix.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Union[str, bytes]</code>)           \u2013            <p>Path to text file containing 4x4 affine matrix</p> </li> <li> <code>invert</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to invert the matrix after loading</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>'AffineTransform'</code>           \u2013            <p>AffineTransform instance with loaded matrix</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>OSError</code>             \u2013            <p>If file cannot be read</p> </li> <li> <code>ValueError</code>             \u2013            <p>If file does not contain valid 4x4 matrix</p> </li> </ul> Source code in <code>zarrnii/transform.py</code> <pre><code>@classmethod\ndef from_txt(\n    cls, path: Union[str, bytes], invert: bool = False\n) -&gt; \"AffineTransform\":\n    \"\"\"Create AffineTransform from text file containing matrix.\n\n    Args:\n        path: Path to text file containing 4x4 affine matrix\n        invert: Whether to invert the matrix after loading\n\n    Returns:\n        AffineTransform instance with loaded matrix\n\n    Raises:\n        OSError: If file cannot be read\n        ValueError: If file does not contain valid 4x4 matrix\n    \"\"\"\n    matrix = np.loadtxt(path)\n    if invert:\n        matrix = np.linalg.inv(matrix)\n    return cls(matrix=matrix)\n</code></pre>"},{"location":"reference/#zarrnii.transform.AffineTransform.from_array","title":"<code>zarrnii.transform.AffineTransform.from_array(matrix, invert=False)</code>  <code>classmethod</code>","text":"<p>Create AffineTransform from numpy array.</p> <p>Parameters:</p> <ul> <li> <code>matrix</code>               (<code>ndarray</code>)           \u2013            <p>4x4 numpy array representing affine transformation</p> </li> <li> <code>invert</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to invert the matrix</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>'AffineTransform'</code>           \u2013            <p>AffineTransform instance with the matrix</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If matrix is not 4x4</p> </li> </ul> Source code in <code>zarrnii/transform.py</code> <pre><code>@classmethod\ndef from_array(cls, matrix: np.ndarray, invert: bool = False) -&gt; \"AffineTransform\":\n    \"\"\"Create AffineTransform from numpy array.\n\n    Args:\n        matrix: 4x4 numpy array representing affine transformation\n        invert: Whether to invert the matrix\n\n    Returns:\n        AffineTransform instance with the matrix\n\n    Raises:\n        ValueError: If matrix is not 4x4\n    \"\"\"\n    if matrix.shape != (4, 4):\n        raise ValueError(f\"Matrix must be 4x4, got shape {matrix.shape}\")\n\n    if invert:\n        matrix = np.linalg.inv(matrix)\n    return cls(matrix=matrix)\n</code></pre>"},{"location":"reference/#zarrnii.transform.AffineTransform.identity","title":"<code>zarrnii.transform.AffineTransform.identity()</code>  <code>classmethod</code>","text":"<p>Create identity transformation.</p> <p>Returns:</p> <ul> <li> <code>'AffineTransform'</code>           \u2013            <p>AffineTransform representing identity transformation (no change)</p> </li> </ul> Source code in <code>zarrnii/transform.py</code> <pre><code>@classmethod\ndef identity(cls) -&gt; \"AffineTransform\":\n    \"\"\"Create identity transformation.\n\n    Returns:\n        AffineTransform representing identity transformation (no change)\n    \"\"\"\n    return cls(matrix=np.eye(4, 4))\n</code></pre>"},{"location":"reference/#zarrnii.transform.AffineTransform.apply_transform","title":"<code>zarrnii.transform.AffineTransform.apply_transform(vecs)</code>","text":"<p>Apply transformation to coordinate vectors.</p> <p>Parameters:</p> <ul> <li> <code>vecs</code>               (<code>ndarray</code>)           \u2013            <p>Input coordinates to transform</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>Transformed coordinates</p> </li> </ul> Source code in <code>zarrnii/transform.py</code> <pre><code>def apply_transform(self, vecs: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Apply transformation to coordinate vectors.\n\n    Args:\n        vecs: Input coordinates to transform\n\n    Returns:\n        Transformed coordinates\n    \"\"\"\n    return self @ vecs\n</code></pre>"},{"location":"reference/#zarrnii.transform.AffineTransform.invert","title":"<code>zarrnii.transform.AffineTransform.invert()</code>","text":"<p>Return the inverse of the matrix transformation.</p> <p>Returns:</p> <ul> <li> <code>'AffineTransform'</code>           \u2013            <p>New AffineTransform with inverted matrix</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>LinAlgError</code>             \u2013            <p>If matrix is singular and cannot be inverted</p> </li> </ul> Source code in <code>zarrnii/transform.py</code> <pre><code>def invert(self) -&gt; \"AffineTransform\":\n    \"\"\"Return the inverse of the matrix transformation.\n\n    Returns:\n        New AffineTransform with inverted matrix\n\n    Raises:\n        np.linalg.LinAlgError: If matrix is singular and cannot be inverted\n    \"\"\"\n    return AffineTransform.from_array(np.linalg.inv(self.matrix))\n</code></pre>"},{"location":"reference/#zarrnii.transform.AffineTransform.update_for_orientation","title":"<code>zarrnii.transform.AffineTransform.update_for_orientation(input_orientation, output_orientation)</code>","text":"<p>Update the matrix to map from input orientation to output orientation.</p> <p>Parameters:</p> <ul> <li> <code>input_orientation</code>               (<code>str</code>)           \u2013            <p>Current anatomical orientation (e.g., 'RPI')</p> </li> <li> <code>output_orientation</code>               (<code>str</code>)           \u2013            <p>Target anatomical orientation (e.g., 'RAS')</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>'AffineTransform'</code>           \u2013            <p>New AffineTransform updated for orientation mapping</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If orientations are invalid or cannot be matched</p> </li> </ul> Source code in <code>zarrnii/transform.py</code> <pre><code>def update_for_orientation(\n    self, input_orientation: str, output_orientation: str\n) -&gt; \"AffineTransform\":\n    \"\"\"Update the matrix to map from input orientation to output orientation.\n\n    Args:\n        input_orientation: Current anatomical orientation (e.g., 'RPI')\n        output_orientation: Target anatomical orientation (e.g., 'RAS')\n\n    Returns:\n        New AffineTransform updated for orientation mapping\n\n    Raises:\n        ValueError: If orientations are invalid or cannot be matched\n    \"\"\"\n\n    # Define a mapping of anatomical directions to axis indices and flips\n    axis_map = {\n        \"R\": (0, 1),\n        \"L\": (0, -1),\n        \"A\": (1, 1),\n        \"P\": (1, -1),\n        \"S\": (2, 1),\n        \"I\": (2, -1),\n    }\n\n    # Parse the input and output orientations\n    input_axes = [axis_map[ax] for ax in input_orientation]\n    output_axes = [axis_map[ax] for ax in output_orientation]\n\n    # Create a mapping from input to output\n    reorder_indices = [None] * 3\n    flip_signs = [1] * 3\n\n    for out_idx, (out_axis, out_sign) in enumerate(output_axes):\n        for in_idx, (in_axis, in_sign) in enumerate(input_axes):\n            if out_axis == in_axis:  # Match axis\n                reorder_indices[out_idx] = in_idx\n                flip_signs[out_idx] = out_sign * in_sign\n                break\n\n    # Reorder and flip the affine matrix\n    reordered_matrix = np.zeros_like(self.matrix)\n    for i, (reorder_idx, flip_sign) in enumerate(zip(reorder_indices, flip_signs)):\n        if reorder_idx is None:\n            raise ValueError(\n                f\"Cannot match all axes from {input_orientation} to {output_orientation}.\"\n            )\n        reordered_matrix[i, :3] = flip_sign * self.matrix[reorder_idx, :3]\n        reordered_matrix[i, 3] = flip_sign * self.matrix[reorder_idx, 3]\n    reordered_matrix[3, :] = self.matrix[3, :]  # Preserve the homogeneous row\n\n    return AffineTransform.from_array(reordered_matrix)\n</code></pre>"},{"location":"reference/#zarrnii.transform.DisplacementTransform-functions","title":"Functions","text":""},{"location":"reference/#zarrnii.transform.DisplacementTransform.from_nifti","title":"<code>zarrnii.transform.DisplacementTransform.from_nifti(path)</code>  <code>classmethod</code>","text":"<p>Create DisplacementTransform from NIfTI file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Union[str, bytes]</code>)           \u2013            <p>Path to NIfTI displacement field file</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>'DisplacementTransform'</code>           \u2013            <p>DisplacementTransform instance loaded from file</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>OSError</code>             \u2013            <p>If file cannot be read</p> </li> <li> <code>ValueError</code>             \u2013            <p>If file format is invalid</p> </li> </ul> Source code in <code>zarrnii/transform.py</code> <pre><code>@classmethod\ndef from_nifti(cls, path: Union[str, bytes]) -&gt; \"DisplacementTransform\":\n    \"\"\"Create DisplacementTransform from NIfTI file.\n\n    Args:\n        path: Path to NIfTI displacement field file\n\n    Returns:\n        DisplacementTransform instance loaded from file\n\n    Raises:\n        OSError: If file cannot be read\n        ValueError: If file format is invalid\n    \"\"\"\n    disp_nib = nib.load(path)\n    disp_xyz = disp_nib.get_fdata().squeeze()\n    disp_affine = AffineTransform.from_array(disp_nib.affine)\n\n    # Convert from ITK transform convention\n    # ITK uses opposite sign convention for x and y displacements\n    disp_xyz[:, :, :, 0] = -disp_xyz[:, :, :, 0]\n    disp_xyz[:, :, :, 1] = -disp_xyz[:, :, :, 1]\n\n    # Create grid coordinates\n    disp_grid = (\n        np.arange(disp_xyz.shape[0]),\n        np.arange(disp_xyz.shape[1]),\n        np.arange(disp_xyz.shape[2]),\n    )\n\n    return cls(\n        disp_xyz=disp_xyz,\n        disp_grid=disp_grid,\n        disp_affine=disp_affine,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.transform.DisplacementTransform.apply_transform","title":"<code>zarrnii.transform.DisplacementTransform.apply_transform(vecs)</code>","text":"<p>Apply displacement transformation to coordinate vectors.</p> <p>Transforms input coordinates by interpolating displacement vectors from the displacement field and adding them to the input coordinates.</p> <p>Parameters:</p> <ul> <li> <code>vecs</code>               (<code>ndarray</code>)           \u2013            <p>Input coordinates as numpy array. Shape should be (3, N) for N points or (3,) for single point</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>Transformed coordinates with same shape as input</p> </li> </ul> Notes <p>Points outside the displacement field domain are filled with zero displacement (no transformation).</p> Source code in <code>zarrnii/transform.py</code> <pre><code>def apply_transform(self, vecs: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Apply displacement transformation to coordinate vectors.\n\n    Transforms input coordinates by interpolating displacement vectors\n    from the displacement field and adding them to the input coordinates.\n\n    Args:\n        vecs: Input coordinates as numpy array. Shape should be (3, N) for\n            N points or (3,) for single point\n\n    Returns:\n        Transformed coordinates with same shape as input\n\n    Notes:\n        Points outside the displacement field domain are filled with\n        zero displacement (no transformation).\n    \"\"\"\n    # Transform points to voxel space of the displacement field\n    vox_vecs = self.disp_affine.invert() @ vecs\n\n    # Initialize displacement vectors\n    disp_vecs = np.zeros(vox_vecs.shape)\n\n    # Interpolate displacement for each spatial dimension (x, y, z)\n    for ax in range(3):\n        disp_vecs[ax, :] = interpn(\n            self.disp_grid,\n            self.disp_xyz[:, :, :, ax].squeeze(),\n            vox_vecs[:3, :].T,\n            method=\"linear\",\n            bounds_error=False,\n            fill_value=0,\n        )\n\n    # Add displacement to original coordinates\n    return vecs + disp_vecs\n</code></pre>"},{"location":"examples/atlas_example/","title":"Atlas Module Example","text":"<p>This example demonstrates how to use the atlas functionality in ZarrNii for working with brain atlases and performing region-of-interest (ROI) analysis.</p>"},{"location":"examples/atlas_example/#atlas-focused-functionality","title":"Atlas-Focused Functionality","text":"<p>ZarrNii functionality is solely for atlases (dseg.nii.gz + dseg.tsv files), providing: - Loading atlases from local files or TemplateFlow - Region analysis by index, name, or abbreviation - ROI aggregation with multiple statistical functions - Feature mapping to assign values back to regions - Format conversion utilities (CSV, ITK-SNAP to TSV)</p>"},{"location":"examples/atlas_example/#loading-atlases-from-files","title":"Loading Atlases from Files","text":"<pre><code>from zarrnii import ZarrNiiAtlas\n\n# Load atlas from BIDS-format files\natlas = ZarrNiiAtlas.from_files(\n    dseg_path=\"atlas_dseg.nii.gz\",\n    labels_path=\"atlas_dseg.tsv\"\n)\n\n# Get basic atlas information\nprint(f\"Atlas shape: {atlas.dseg.shape}\")\nprint(f\"Number of regions: {len(atlas.labels_df)}\")\nprint(f\"Region labels: {atlas.labels_df[atlas.label_column].values}\")\n</code></pre>"},{"location":"examples/atlas_example/#loading-atlases-from-templateflow","title":"Loading Atlases from TemplateFlow","text":"<pre><code>from zarrnii import get_atlas, get_template\n\n# Load atlas directly from TemplateFlow (if available)\n# atlas = get_atlas(\"MNI152NLin2009cAsym\", \"DKT\", resolution=1)\n\n# Or load template first\n# template = get_template(\"MNI152NLin2009cAsym\", \"T1w\", resolution=1)\n# Note: These functions require templateflow to be installed\n</code></pre>"},{"location":"examples/atlas_example/#saving-atlases-to-templateflow","title":"Saving Atlases to TemplateFlow","text":"<pre><code>from zarrnii import save_atlas_to_templateflow\n\n# Save atlas to TemplateFlow directory as BIDS-compliant files\n# Requires templateflow to be installed\n# template_dir = save_atlas_to_templateflow(atlas, \"MyTemplate\", \"MyAtlas\")\nprint(f\"Atlas saved to: {template_dir}\")\n# Creates: tpl-MyTemplate_atlas-MyAtlas_dseg.nii.gz and .tsv files\n</code></pre> <ul> <li>Unified API: Access built-in and remote templates through same TemplateFlow interface</li> </ul> <pre><code># Install templateflow extra for lazy loading\n# pip install zarrnii[templateflow]\n\n# Lazy loading - templates copy to TEMPLATEFLOW_HOME on first access\nfrom zarrnii import get_builtin_template\ntemplate = get_builtin_template(\"placeholder\")  # Copies to TemplateFlow on first call\n\n# After lazy loading, both APIs work identically:\nimport templateflow.api as tflow\nzarrnii_template = tflow.get(\"placeholder\", suffix=\"SPIM\")        # zarrnii built-in\nremote_template = tflow.get(\"MNI152NLin2009cAsym\", suffix=\"T1w\")  # remote template\n\n# Manual installation (if preferred over lazy loading)\nfrom zarrnii import install_zarrnii_templates\nresults = install_zarrnii_templates()  # {'placeholder': True}\n</code></pre>"},{"location":"examples/atlas_example/#template-installation-behavior","title":"Template Installation Behavior","text":"<p>With <code>templateflow</code> extra installed: - \u2705 Lazy loading: Templates copy to <code>$TEMPLATEFLOW_HOME</code> on first <code>get_builtin_template()</code> call - \u2705 Proper setup: Uses TemplateFlow's <code>@requires_layout</code> decorator to ensure directory structure - \u2705 Unified API: Use TemplateFlow API for both zarrnii and remote templates - \u2705 Automatic: No extra steps needed</p> <p>Without <code>templateflow</code> extra: - \u2705 Direct access: <code>get_builtin_template()</code> works normally from zarrnii package - \u274c No lazy loading: Templates stay in zarrnii package only - \u274c No unified API: TemplateFlow functions unavailable</p> <pre><code># Check installation status\ntry:\n    results = install_zarrnii_templates()\n    print(\"TemplateFlow integration:\", results)\nexcept ImportError:\n    print(\"TemplateFlow extra not installed - using direct access only\")\n</code></pre>"},{"location":"examples/atlas_example/#backward-compatibility","title":"Backward Compatibility","text":"<p>The old atlas functions still work for convenience:</p> <pre><code>from zarrnii import get_builtin_atlas, list_builtin_atlases\n\n# These still work (equivalent to template system)\natlases = list_builtin_atlases() \natlas = get_builtin_atlas(\"placeholder\")  # Gets default atlas from default template\n</code></pre>"},{"location":"examples/atlas_example/#loading-an-atlas-from-files","title":"Loading an Atlas from Files","text":"<pre><code>from zarrnii import ZarrNiiAtlas\nimport numpy as np\n\n# Load atlas from BIDS format files\natlas = ZarrNiiAtlas.from_files(\n    dseg_path=\"path/to/atlas_dseg.nii.gz\",\n    labels_path=\"path/to/atlas_dseg.tsv\"\n)\n\nprint(f\"Atlas loaded: {atlas}\")\nprint(f\"Number of regions: {len(atlas.labels_df)}\")\nprint(f\"Region names: {atlas.labels_df[atlas.name_column].tolist()[:5]}...\")  # First 5 regions\n</code></pre>"},{"location":"examples/atlas_example/#creating-an-atlas-from-existing-data","title":"Creating an Atlas from Existing Data","text":"<pre><code>from zarrnii import ZarrNii, ZarrNiiAtlas, AffineTransform\nimport pandas as pd\nimport numpy as np\n\n# Create a simple segmentation image\nshape = (64, 64, 64)\ndseg_data = np.random.randint(0, 5, shape, dtype=np.int32)\ndseg = ZarrNii.from_darr(dseg_data, affine=AffineTransform.identity())\n\n# Create lookup table\nlabels_df = pd.DataFrame({\n    'index': [0, 1, 2, 3, 4],\n    'name': ['Background', 'Cortex', 'Hippocampus', 'Thalamus', 'Cerebellum'],\n    'abbreviation': ['BG', 'CTX', 'HIP', 'THA', 'CB']\n})\n\n# Create Atlas\natlas = ZarrNiiAtlas.create_from_dseg(dseg, labels_df)\n</code></pre>"},{"location":"examples/atlas_example/#region-analysis","title":"Region Analysis","text":""},{"location":"examples/atlas_example/#getting-region-information","title":"Getting Region Information","text":"<pre><code># Get information about a specific region by index (traditional way)\nregion_info = atlas.get_region_info(2)  # Hippocampus\nprint(f\"Region: {region_info['name']}\")\nprint(f\"Abbreviation: {region_info['abbreviation']}\")\n\n# NEW: Get information by region name\nregion_info = atlas.get_region_info(\"Hippocampus\")\nprint(f\"Hippocampus index: {region_info['index']}\")\n\n# NEW: Get information by abbreviation\nregion_info = atlas.get_region_info(\"HIP\")\nprint(f\"HIP full name: {region_info['name']}\")\n\n# Get all region names and labels\nfor label, name in zip(atlas.region_labels, atlas.region_names):\n    print(f\"Label {label}: {name}\")\n</code></pre>"},{"location":"examples/atlas_example/#creating-region-masks","title":"Creating Region Masks","text":"<pre><code># Get binary mask for hippocampus by index\nhippocampus_mask = atlas.get_region_mask(2)\nprint(f\"Hippocampus mask shape: {hippocampus_mask.shape}\")\n\n# NEW: Get mask by region name\nhippocampus_mask = atlas.get_region_mask(\"Hippocampus\")\nprint(f\"Hippocampus mask shape: {hippocampus_mask.shape}\")\n\n# NEW: Get mask by abbreviation\nhippocampus_mask = atlas.get_region_mask(\"HIP\")\nprint(f\"Hippocampus mask shape: {hippocampus_mask.shape}\")\n\n# Save mask as NIfTI\nhippocampus_mask.to_nifti(\"hippocampus_mask.nii.gz\")\n</code></pre>"},{"location":"examples/atlas_example/#calculating-region-volumes","title":"Calculating Region Volumes","text":"<pre><code># Calculate volume for all regions\nvolumes = {}\nfor _, row in atlas.labels_df.iterrows():\n    label = row[atlas.label_column]\n    if label == 0:  # Skip background\n        continue\n    volume = atlas.get_region_volume(label)\n    name = row[atlas.name_column]\n    volumes[name] = volume\n    print(f\"{name}: {volume:.2f} mm\u00b3\")\n\n# Calculate volume by name or abbreviation\ncortex_volume = atlas.get_region_volume(\"Cortex\")\nhippocampus_volume = atlas.get_region_volume(\"HIP\")  # By abbreviation\nprint(f\"Cortex volume: {cortex_volume:.2f} mm\u00b3\")\nprint(f\"Hippocampus volume: {hippocampus_volume:.2f} mm\u00b3\")\n</code></pre>"},{"location":"examples/atlas_example/#region-based-cropping","title":"Region-Based Cropping","text":""},{"location":"examples/atlas_example/#getting-bounding-boxes-for-regions","title":"Getting Bounding Boxes for Regions","text":"<p>The <code>get_region_bounding_box</code> method allows you to extract the spatial extents of one or more atlas regions in physical coordinates, which can then be used to crop images to focus on specific anatomical structures.</p> <pre><code>from zarrnii import ZarrNiiAtlas\n\n# Load an atlas\natlas = ZarrNiiAtlas.from_files(\"atlas_dseg.nii.gz\", \"atlas_dseg.tsv\")\n\n# Get bounding box for a single region by name\nbbox_min, bbox_max = atlas.get_region_bounding_box(\"Hippocampus\")\nprint(f\"Hippocampus extends from {bbox_min} to {bbox_max} mm in physical space\")\n\n# Get bounding box for multiple regions\nbbox_min, bbox_max = atlas.get_region_bounding_box([\"Hippocampus\", \"Amygdala\"])\nprint(f\"Combined bounding box: {bbox_min} to {bbox_max}\")\n\n# Use regex to select regions\nbbox_min, bbox_max = atlas.get_region_bounding_box(regex=\"Hippocampus.*\")\nprint(f\"All hippocampal subfields: {bbox_min} to {bbox_max}\")\n</code></pre>"},{"location":"examples/atlas_example/#cropping-images-to-regions","title":"Cropping Images to Regions","text":"<p>The bounding boxes returned by <code>get_region_bounding_box</code> are in physical coordinates and can be used directly with the <code>crop</code> method:</p> <pre><code>from zarrnii import ZarrNii\n\n# Load a brain image\nimage = ZarrNii.from_nifti(\"brain_image.nii.gz\")\n\n# Get bounding box for hippocampus\nbbox_min, bbox_max = atlas.get_region_bounding_box(\"Hippocampus\")\n\n# Crop the image to the hippocampus region\ncropped_image = image.crop(bbox_min, bbox_max, physical_coords=True)\n\n# Save the cropped region\ncropped_image.to_nifti(\"hippocampus_cropped.nii.gz\")\n</code></pre>"},{"location":"examples/atlas_example/#example-cropping-and-saving-as-tiff-stack","title":"Example: Cropping and Saving as TIFF Stack","text":"<p>Here's a complete example showing how to crop around the hippocampus and save as a z-stack of TIFF images:</p> <pre><code>from zarrnii import ZarrNii, ZarrNiiAtlas\nimport numpy as np\nfrom PIL import Image\nfrom pathlib import Path\n\n# Load atlas and image\natlas = ZarrNiiAtlas.from_files(\"atlas_dseg.nii.gz\", \"atlas_dseg.tsv\")\nbrain_image = ZarrNii.from_nifti(\"brain_mri.nii.gz\")\n\n# Get bounding box for hippocampus (or use regex for bilateral hippocampi)\nbbox_min, bbox_max = atlas.get_region_bounding_box(regex=\"[Hh]ippocampus.*\")\n\n# Crop both the atlas and the image\ncropped_atlas = atlas.crop(bbox_min, bbox_max, physical_coords=True)\ncropped_image = brain_image.crop(bbox_min, bbox_max, physical_coords=True)\n\n# Get the data (compute if dask array)\nimage_data = cropped_image.data\nif hasattr(image_data, \"compute\"):\n    image_data = image_data.compute()\n\n# Create output directory\noutput_dir = Path(\"hippocampus_slices\")\noutput_dir.mkdir(exist_ok=True)\n\n# Save each z-slice as a TIFF\n# Handle different axes orders (CZYX or CXYZ)\nif cropped_image.axes_order == \"ZYX\":\n    # Data is (C, Z, Y, X)\n    channel_idx = 0 if image_data.ndim == 4 else None\n    for z_idx in range(image_data.shape[1] if channel_idx is not None else image_data.shape[0]):\n        if channel_idx is not None:\n            slice_data = image_data[channel_idx, z_idx, :, :]\n        else:\n            slice_data = image_data[z_idx, :, :]\n\n        # Normalize to 0-255 for TIFF\n        slice_normalized = ((slice_data - slice_data.min()) / \n                           (slice_data.max() - slice_data.min()) * 255).astype(np.uint8)\n\n        # Save as TIFF\n        img = Image.fromarray(slice_normalized)\n        img.save(output_dir / f\"hippocampus_z{z_idx:03d}.tif\")\n\nprint(f\"Saved {image_data.shape[1 if channel_idx is not None else 0]} slices to {output_dir}\")\n</code></pre>"},{"location":"examples/atlas_example/#cropping-multiple-regions","title":"Cropping Multiple Regions","text":"<p>You can easily create crops for multiple regions of interest:</p> <pre><code># Define regions of interest\nregions_of_interest = {\n    \"hippocampus\": \"Hippocampus\",\n    \"amygdala\": \"Amygdala\",\n    \"cortical\": \"cortex.*\",  # regex pattern\n}\n\n# Crop and save each region\nfor region_name, region_pattern in regions_of_interest.items():\n    # Get bounding box (use regex if pattern contains special characters)\n    if any(char in region_pattern for char in [\"*\", \".\", \"[\", \"]\"]):\n        bbox_min, bbox_max = atlas.get_region_bounding_box(regex=region_pattern)\n    else:\n        bbox_min, bbox_max = atlas.get_region_bounding_box(region_pattern)\n\n    # Crop image\n    cropped = brain_image.crop(bbox_min, bbox_max, physical_coords=True)\n\n    # Save\n    cropped.to_nifti(f\"{region_name}_cropped.nii.gz\")\n    print(f\"Saved {region_name} crop: shape={cropped.shape}\")\n</code></pre>"},{"location":"examples/atlas_example/#image-analysis-with-atlas","title":"Image Analysis with Atlas","text":""},{"location":"examples/atlas_example/#aggregating-image-values-by-regions","title":"Aggregating Image Values by Regions","text":"<pre><code># Load an image for analysis (e.g., a functional or structural image)\nimage = ZarrNii.from_nifti(\"functional_image.nii.gz\")\n\n# Make sure image and atlas have the same shape and alignment\n# (In practice, you might need to resample/register the image to atlas space)\n\n# Aggregate image values by atlas regions\nresults = atlas.aggregate_image_by_regions(\n    image, \n    aggregation_func=\"mean\"  # Can be \"mean\", \"sum\", \"std\", \"median\", \"min\", \"max\"\n)\n\nprint(\"Mean signal by region:\")\nfor _, row in results.iterrows():\n    print(f\"{row['name']}: {row['mean_value']:.3f}\")\n\n# Save results to CSV\nresults.to_csv(\"roi_analysis_results.csv\", index=False)\n</code></pre>"},{"location":"examples/atlas_example/#different-aggregation-functions","title":"Different Aggregation Functions","text":"<pre><code># Calculate multiple statistics\nstats = {}\nfor func in [\"mean\", \"std\", \"min\", \"max\"]:\n    stats[func] = atlas.aggregate_image_by_regions(image, aggregation_func=func)\n\n# Combine results (note: results have 'label' and 'name' columns by default)\ncombined_results = stats[\"mean\"][[\"label\", \"name\"]].copy()\nfor func in [\"mean\", \"std\", \"min\", \"max\"]:\n    combined_results[f\"{func}_value\"] = stats[func][f\"{func}_value\"]\n\nprint(combined_results.head())\n</code></pre>"},{"location":"examples/atlas_example/#analyzing-specific-regions-only","title":"Analyzing Specific Regions Only","text":"<pre><code># Get results for all regions, then filter\nall_results = atlas.aggregate_image_by_regions(image, aggregation_func=\"mean\")\n\n# Filter to specific regions (e.g., cortical regions with labels 1-3)\ncortical_results = all_results[all_results[\"label\"].isin([1, 2, 3])]\n\nprint(\"Cortical region analysis:\")\nprint(cortical_results)\n</code></pre>"},{"location":"examples/atlas_example/#creating-feature-maps","title":"Creating Feature Maps","text":""},{"location":"examples/atlas_example/#mapping-statistical-values-back-to-image-space","title":"Mapping Statistical Values Back to Image Space","text":"<pre><code># Create a feature DataFrame (e.g., from previous analysis)\nfeature_data = pd.DataFrame({\n    'index': [1, 2, 3, 4],\n    'activation_score': [2.5, 3.1, 1.8, 2.9],\n    'p_value': [0.001, 0.0001, 0.05, 0.002]\n})\n\n# Create feature maps\nactivation_map = atlas.create_feature_map(\n    feature_data, \n    feature_column='activation_score',\n    background_value=0.0\n)\n\np_value_map = atlas.create_feature_map(\n    feature_data, \n    feature_column='p_value',\n    background_value=1.0  # Non-significant p-value for background\n)\n\n# Save feature maps\nactivation_map.to_nifti(\"activation_map.nii.gz\")\np_value_map.to_nifti(\"p_value_map.nii.gz\")\n</code></pre>"},{"location":"examples/atlas_example/#atlas-summary-information","title":"Atlas Summary Information","text":""},{"location":"examples/atlas_example/#getting-atlas-overview","title":"Getting Atlas Overview","text":"<pre><code># Get information about all regions\nprint(\"Atlas Summary:\")\nprint(f\"Total regions: {len(atlas.labels_df)}\")\nprint(f\"\\nRegion list:\")\nfor _, row in atlas.labels_df.iterrows():\n    label = row[atlas.label_column]\n    name = row[atlas.name_column]\n    if label != 0:  # Skip background\n        volume = atlas.get_region_volume(label)\n        print(f\"  {label}: {name} - {volume:.2f} mm\u00b3\")\n\n# Calculate total brain volume (excluding background)\ntotal_volume = sum(\n    atlas.get_region_volume(row[atlas.label_column])\n    for _, row in atlas.labels_df.iterrows()\n    if row[atlas.label_column] != 0\n)\nprint(f\"\\nTotal brain volume: {total_volume:.2f} mm\u00b3\")\n</code></pre>"},{"location":"examples/atlas_example/#lookup-table-conversion-utilities","title":"Lookup Table Conversion Utilities","text":""},{"location":"examples/atlas_example/#convert-csv-to-bids-tsv-format","title":"Convert CSV to BIDS TSV Format","text":"<pre><code>from zarrnii import import_lut_csv_as_tsv\n\n# Convert a CSV lookup table to BIDS-compatible TSV\nimport_lut_csv_as_tsv(\n    csv_path=\"atlas_lookup.csv\",\n    tsv_path=\"atlas_dseg.tsv\",\n    csv_columns=[\"abbreviation\", \"name\", \"index\"]  # Specify column order\n)\n</code></pre>"},{"location":"examples/atlas_example/#convert-itk-snap-label-file-to-tsv","title":"Convert ITK-SNAP Label File to TSV","text":"<pre><code>from zarrnii import import_lut_itksnap_as_tsv\n\n# Convert ITK-SNAP format to BIDS TSV\nimport_lut_itksnap_as_tsv(\n    itksnap_path=\"atlas.txt\",\n    tsv_path=\"atlas_dseg.tsv\"\n)\n</code></pre>"},{"location":"examples/atlas_example/#advanced-usage","title":"Advanced Usage","text":""},{"location":"examples/atlas_example/#working-with-multi-resolution-data","title":"Working with Multi-Resolution Data","text":"<pre><code># Load atlas at different resolution levels (if using OME-Zarr format)\natlas = ZarrNiiAtlas.from_files(\n    dseg_path=\"atlas.ome.zarr\",\n    labels_path=\"atlas_dseg.tsv\",\n    level=2  # Use downsampled level\n)\n\n# This allows processing at different scales for efficiency\n</code></pre>"},{"location":"examples/atlas_example/#custom-column-names","title":"Custom Column Names","text":"<pre><code># Create atlas with custom column naming conventions\n# First load the data\ndseg = ZarrNii.from_nifti(\"custom_atlas.nii.gz\")\nlabels_df = pd.read_csv(\"custom_labels.tsv\", sep=\"\\t\")\n\n# Create atlas with custom column names\natlas = ZarrNiiAtlas.create_from_dseg(\n    dseg, \n    labels_df,\n    label_column=\"region_id\",\n    name_column=\"region_name\",\n    abbrev_column=\"short_name\"\n)\n</code></pre>"},{"location":"examples/atlas_example/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    atlas = ZarrNiiAtlas.from_files(\"nonexistent.nii.gz\", \"nonexistent.tsv\")\nexcept FileNotFoundError as e:\n    print(f\"Atlas files not found: {e}\")\n\ntry:\n    results = atlas.aggregate_image_by_regions(wrong_sized_image)\nexcept ValueError as e:\n    print(f\"Shape mismatch: {e}\")\n</code></pre>"},{"location":"examples/atlas_example/#integration-with-existing-zarrnii-workflows","title":"Integration with Existing ZarrNii Workflows","text":"<pre><code># Complete workflow: load, transform, analyze\nfrom zarrnii import ZarrNii, ZarrNiiAtlas, AffineTransform\n\n# Load atlas and image\natlas = ZarrNiiAtlas.from_files(\"atlas_dseg.nii.gz\", \"atlas_dseg.tsv\")\nimage = ZarrNii.from_ome_zarr(\"microscopy_data.ome.zarr\", level=1)\n\n# Apply spatial transformation to align image to atlas\ntransform = AffineTransform.from_txt(\"subject_to_atlas.txt\")\naligned_image = image.apply_transform(transform, ref_znimg=atlas.dseg)\n\n# Perform ROI analysis\nresults = atlas.aggregate_image_by_regions(aligned_image, aggregation_func=\"mean\")\n\n# Save results\nresults.to_csv(\"roi_quantification.csv\", index=False)\n\n# Create and save feature map\nfeature_map = atlas.create_feature_map(results, \"mean_value\")\nfeature_map.to_ome_zarr(\"quantification_map.ome.zarr\")\n</code></pre> <p>This atlas functionality provides a complete toolkit for working with brain atlases in the ZarrNii ecosystem, enabling sophisticated region-based analysis workflows that can scale from microscopy data to MRI volumes.</p>"},{"location":"examples/downsampling/","title":"Downsampling and Upsampling","text":"<p>This section covers resolution changes in ZarrNii, including downsampling for efficient processing and upsampling for analysis.</p>"},{"location":"examples/downsampling/#overview","title":"Overview","text":"<p>ZarrNii provides methods for changing image resolution through downsampling and upsampling operations. These are essential for creating multi-resolution datasets and efficient processing workflows.</p>"},{"location":"examples/downsampling/#downsampling","title":"Downsampling","text":""},{"location":"examples/downsampling/#basic-downsampling","title":"Basic Downsampling","text":"<pre><code>from zarrnii import ZarrNii\n\n# Load a high-resolution dataset\nznimg = ZarrNii.from_nifti(\"path/to/highres.nii\")\nprint(\"Original shape:\", znimg.darr.shape)\n\n# Downsample by level (2^level reduction)\ndownsampled = znimg.downsample(level=2)\nprint(\"Downsampled shape:\", downsampled.darr.shape)\n</code></pre>"},{"location":"examples/downsampling/#custom-downsampling-factors","title":"Custom Downsampling Factors","text":"<pre><code># Downsample with specific factors for each axis\ndownsampled_custom = znimg.downsample(\n    along_x=2, \n    along_y=2, \n    along_z=1  # 2x in X,Y; no change in Z\n)\n</code></pre>"},{"location":"examples/downsampling/#upsampling","title":"Upsampling","text":""},{"location":"examples/downsampling/#basic-upsampling","title":"Basic Upsampling","text":"<pre><code># Upsample by factors\nupsampled = znimg.upsample(\n    along_x=2, \n    along_y=2, \n    along_z=1  # 2x in X,Y; no change in Z\n)\n\n# Upsample to specific target shape\ntarget_shape = (100, 200, 300)\nupsampled_to_shape = znimg.upsample(to_shape=target_shape)\n</code></pre>"},{"location":"examples/downsampling/#multi-resolution-workflows","title":"Multi-Resolution Workflows","text":""},{"location":"examples/downsampling/#creating-image-pyramids","title":"Creating Image Pyramids","text":"<pre><code># Create multiple resolution levels\npyramid_levels = []\ncurrent = znimg\n\nfor level in range(4):\n    pyramid_levels.append(current)\n    current = current.downsample(level=1)\n    print(f\"Level {level}: {current.darr.shape}\")\n</code></pre>"},{"location":"examples/downsampling/#working-with-ome-zarr-multi-resolution","title":"Working with OME-Zarr Multi-Resolution","text":"<pre><code># Load multi-resolution OME-Zarr at specific level\nznimg_level0 = ZarrNii.from_ome_zarr(\"path/to/multires.zarr\", level=0)\nznimg_level2 = ZarrNii.from_ome_zarr(\"path/to/multires.zarr\", level=2)\n\n# Create new multi-resolution OME-Zarr\nznimg.to_ome_zarr(\n    \"output_multires.zarr\",\n    max_layer=4\n)\n</code></pre>"},{"location":"examples/downsampling/#automatic-near-isotropic-downsampling","title":"Automatic Near-Isotropic Downsampling","text":"<p>For datasets with anisotropic voxels (e.g., lightsheet microscopy with fine Z resolution), you can automatically downsample to create more isotropic voxels:</p> <pre><code># Load with automatic near-isotropic downsampling\nznimg_isotropic = ZarrNii.from_ome_zarr(\n    \"path/to/anisotropic_data.ome.zarr\", \n    downsample_near_isotropic=True\n)\n\n# Example: if original scales are z=0.25\u03bcm, y=1.0\u03bcm, x=1.0\u03bcm\n# Z dimension gets downsampled by 4x to make scales isotropic:\n# Result: z=1.0\u03bcm, y=1.0\u03bcm, x=1.0\u03bcm (isotropic voxels)\nprint(\"Original anisotropic scales:\", znimg_normal.scale)\nprint(\"Near-isotropic scales:\", znimg_isotropic.scale)\n</code></pre> <p>How it works: - Identifies dimensions with finer resolution (smaller scale values) - Downsamples by powers of 2 to match the coarsest resolution - Works for any spatial dimension (X, Y, or Z) - Only applies when significant anisotropy is detected</p>"},{"location":"examples/downsampling/#memory-efficient-processing","title":"Memory-Efficient Processing","text":"<pre><code># Work with large images efficiently using Dask\n# Downsampling is lazy and computed only when needed\ndownsampled = znimg.downsample(level=2)\n\n# Compute result when needed\nresult = downsampled.darr.compute()\n</code></pre>"},{"location":"examples/downsampling/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use appropriate chunk sizes: For Dask arrays, ensure chunks are well-sized for your operations</li> <li>Lazy evaluation: Downsampling operations are lazy and computed only when <code>.compute()</code> is called</li> <li>Memory management: Use <code>.rechunk()</code> if needed to optimize chunk sizes for your workflow</li> <li>Level-based downsampling: Use <code>level</code> parameter for consistent 2^level reductions</li> </ol>"},{"location":"examples/downsampling/#see-also","title":"See Also","text":"<ul> <li>Multiscale Processing for advanced multi-resolution workflows  </li> <li>Working with Zarr and NIfTI for basic format operations</li> <li>API Reference for detailed method documentation</li> </ul>"},{"location":"examples/imaris_usage/","title":"Working with Imaris Files","text":"<p>ZarrNii provides seamless support for reading and writing Imaris (.ims) files, enabling integration with microscopy workflows that use Imaris for image analysis and visualization.</p> <p>!!! note \"Optional Dependency\"     Imaris support requires the optional <code>imaris</code> dependency. Install it with:     <code>bash     pip install zarrnii[imaris]</code></p> <p>!!! info \"HDF5-Based Imaris Support\"     ZarrNii provides robust Imaris (.ims) file support through a carefully crafted HDF5 implementation. The implementation follows the exact structure of correctly-formed Imaris files to ensure maximum compatibility with Imaris software.</p> <pre><code>**Key Features:**\n- Reads all standard Imaris files with multiple channels, timepoints, and resolution levels\n- Creates Imaris-compatible files using the correct HDF5 structure and metadata\n- Handles both single and multi-channel data automatically\n- Preserves spatial metadata and supports histogram generation\n</code></pre>"},{"location":"examples/imaris_usage/#loading-imaris-files","title":"Loading Imaris Files","text":""},{"location":"examples/imaris_usage/#basic-loading","title":"Basic Loading","text":"<pre><code>from zarrnii import ZarrNii\n\n# Load an Imaris file\nznimg = ZarrNii.from_imaris(\"microscopy_data.ims\")\n\nprint(f\"Data shape: {znimg.darr.shape}\")\nprint(f\"Spacing: {znimg.get_zooms()}\")\n</code></pre>"},{"location":"examples/imaris_usage/#selecting-specific-data","title":"Selecting Specific Data","text":"<p>Imaris files can contain multiple resolution levels, timepoints, and channels. You can specify which to load:</p> <pre><code># Load specific resolution level, timepoint, and channel\nznimg = ZarrNii.from_imaris(\n    \"microscopy_data.ims\",\n    level=1,        # Resolution level (0 = full resolution)\n    timepoint=5,    # Time point\n    channel=0       # Channel index\n)\n\n# Specify axes order and orientation\nznimg = ZarrNii.from_imaris(\n    \"microscopy_data.ims\",\n    axes_order=\"ZYX\",     # Spatial axes order\n    orientation=\"RAS\"     # Coordinate system orientation\n)\n</code></pre>"},{"location":"examples/imaris_usage/#saving-to-imaris-format","title":"Saving to Imaris Format","text":""},{"location":"examples/imaris_usage/#basic-saving","title":"Basic Saving","text":"<pre><code>import numpy as np\nimport dask.array as da\n\n# Create or load data\ndata = np.random.rand(64, 128, 96).astype(np.float32)\ndarr = da.from_array(data[np.newaxis, ...], chunks=\"auto\")\nznimg = ZarrNii.from_darr(darr, spacing=[2.0, 1.0, 1.0])\n\n# Save to Imaris format\noutput_path = znimg.to_imaris(\"output_data.ims\")\nprint(f\"Saved to: {output_path}\")\n</code></pre>"},{"location":"examples/imaris_usage/#compression-options","title":"Compression Options","text":"<pre><code># Save with specific compression settings\nznimg.to_imaris(\n    \"compressed_data.ims\",\n    compression=\"gzip\",      # Compression method\n    compression_opts=6       # Compression level (0-9)\n)\n</code></pre>"},{"location":"examples/imaris_usage/#format-conversions","title":"Format Conversions","text":""},{"location":"examples/imaris_usage/#imaris-to-nifti","title":"Imaris to NIfTI","text":"<pre><code># Load from Imaris and save as NIfTI\nznimg = ZarrNii.from_imaris(\"microscopy_data.ims\")\nznimg.to_nifti(\"converted_data.nii.gz\")\n</code></pre>"},{"location":"examples/imaris_usage/#nifti-to-imaris","title":"NIfTI to Imaris","text":"<pre><code># Load from NIfTI and save as Imaris\nznimg = ZarrNii.from_nifti(\"brain_scan.nii.gz\")\nznimg.to_imaris(\"brain_scan.ims\")\n</code></pre>"},{"location":"examples/imaris_usage/#round-trip-processing","title":"Round-trip Processing","text":"<pre><code># Load, process, and save back to Imaris\nznimg = ZarrNii.from_imaris(\"original_data.ims\")\n\n# Apply transformations\ncropped = znimg.crop((10, 10, 10), (100, 100, 100))\ndownsampled = cropped.downsample(level=2)\n\n# Save processed result\ndownsampled.to_imaris(\"processed_data.ims\")\n</code></pre>"},{"location":"examples/imaris_usage/#integration-with-other-formats","title":"Integration with Other Formats","text":""},{"location":"examples/imaris_usage/#multi-format-workflow","title":"Multi-format Workflow","text":"<pre><code># Load from Imaris\nmicroscopy_data = ZarrNii.from_imaris(\"confocal_stack.ims\")\n\n# Convert to OME-Zarr for analysis\nmicroscopy_data.to_ome_zarr(\"analysis_data.ome.zarr\")\n\n# Load analysis results and convert back\nresults = ZarrNii.from_ome_zarr(\"analysis_results.ome.zarr\")\nresults.to_imaris(\"final_results.ims\")\n</code></pre>"},{"location":"examples/imaris_usage/#understanding-imaris-file-structure","title":"Understanding Imaris File Structure","text":"<p>Imaris files use HDF5 format with a specific hierarchy:</p> <pre><code>MyData.ims\n\u251c\u2500\u2500 DataSet/\n\u2502   \u2514\u2500\u2500 ResolutionLevel 0/\n\u2502       \u2514\u2500\u2500 TimePoint 0/\n\u2502           \u2514\u2500\u2500 Channel 0/\n\u2502               \u2514\u2500\u2500 Data          # The actual image data\n\u251c\u2500\u2500 DataSetInfo/\n\u2502   \u251c\u2500\u2500 Image/                   # Image metadata\n\u2502   \u2514\u2500\u2500 Channel 0/               # Channel information\n\u2514\u2500\u2500 DataSetTimes/\n    \u2514\u2500\u2500 Time                     # Temporal information\n</code></pre> <p>ZarrNii handles this structure automatically, extracting spatial metadata and presenting a unified interface consistent with other supported formats.</p>"},{"location":"examples/imaris_usage/#imaris-file-format-support","title":"Imaris File Format Support","text":"<p>ZarrNii provides comprehensive Imaris (.ims) file support through a robust HDF5-based implementation:</p>"},{"location":"examples/imaris_usage/#file-creation-and-compatibility","title":"File Creation and Compatibility","text":"<p>ZarrNii creates Imaris files that are compatible with Imaris software by following the exact structure found in correctly-formed Imaris files:</p> <pre><code># Create Imaris-compatible files\nznimg.to_imaris(\"output.ims\", compression=\"gzip\")\n</code></pre> <p>Key Features: - Correct HDF5 structure: Follows the exact directory hierarchy used by Imaris - Proper metadata: Includes all necessary attributes for Imaris compatibility - Multi-channel support: Automatically handles single and multi-channel data - Histogram generation: Creates proper histograms for each channel - Compression support: Supports various HDF5 compression options</p> <p>File Structure Created: - Top-level attributes matching Imaris format (ImarisVersion, DataSetDirectoryName, etc.) - <code>DataSet/ResolutionLevel 0/TimePoint 0/Channel X/Data</code> hierarchy - <code>DataSetInfo</code> group with channel metadata - Proper histogram data for each channel</p>"},{"location":"examples/imaris_usage/#best-practices","title":"Best Practices","text":""},{"location":"examples/imaris_usage/#memory-management","title":"Memory Management","text":"<p>For large Imaris files, consider using chunked processing:</p> <pre><code># Load with specific chunking strategy\nznimg = ZarrNii.from_imaris(\"large_dataset.ims\", chunks=(1, 64, 64, 64))\n\n# Process in chunks to avoid memory issues\ncropped = znimg.crop((0, 0, 0), (500, 500, 500))  # Crop first\ndownsampled = cropped.downsample(level=2)          # Then downsample\n</code></pre>"},{"location":"examples/imaris_usage/#metadata-preservation","title":"Metadata Preservation","text":"<p>ZarrNii automatically extracts and preserves spatial metadata from Imaris files:</p> <pre><code>znimg = ZarrNii.from_imaris(\"calibrated_data.ims\")\n\n# Access spatial information\nprint(f\"Voxel spacing: {znimg.get_zooms()}\")\nprint(f\"Origin: {znimg.get_origin()}\")\nprint(f\"Orientation: {znimg.orientation}\")\n</code></pre>"},{"location":"examples/imaris_usage/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    znimg = ZarrNii.from_imaris(\"data.ims\", level=5)\nexcept ValueError as e:\n    print(f\"Invalid parameters: {e}\")\nexcept ImportError as e:\n    print(f\"Missing dependency: {e}\")\n    print(\"Install with: pip install zarrnii[imaris]\")\n</code></pre>"},{"location":"examples/imaris_usage/#example-complete-microscopy-processing-pipeline","title":"Example: Complete Microscopy Processing Pipeline","text":"<pre><code>from zarrnii import ZarrNii\n\ndef process_microscopy_data(input_path, output_path):\n    \"\"\"Complete processing pipeline for microscopy data.\"\"\"\n\n    # Load original Imaris data\n    print(\"Loading Imaris data...\")\n    znimg = ZarrNii.from_imaris(input_path)\n\n    # Apply processing steps\n    print(\"Processing data...\")\n\n    # 1. Crop to region of interest\n    cropped = znimg.crop((50, 50, 50), (400, 400, 300))\n\n    # 2. Downsample for faster processing\n    downsampled = cropped.downsample(level=1)\n\n    # 3. Save intermediate result as OME-Zarr for analysis\n    downsampled.to_ome_zarr(\"intermediate_analysis.ome.zarr\")\n\n    # 4. Save final result as Imaris\n    print(\"Saving processed data...\")\n    downsampled.to_imaris(output_path)\n\n    print(f\"Processing complete. Output saved to: {output_path}\")\n    return downsampled\n\n# Run the pipeline\nresult = process_microscopy_data(\"raw_confocal.ims\", \"processed_confocal.ims\")\n</code></pre> <p>This example demonstrates ZarrNii's ability to seamlessly integrate Imaris files into broader image processing workflows while maintaining spatial accuracy and metadata consistency.</p>"},{"location":"examples/ml_patch_sampling/","title":"ML Training: Patch Sampling and Cropping","text":"<p>This example demonstrates how to use <code>sample_region_patches()</code> and <code>crop_centered()</code> for machine learning workflows where you need to extract fixed-size training patches from anatomical regions defined in an atlas.</p>"},{"location":"examples/ml_patch_sampling/#overview","title":"Overview","text":"<p>The typical ML training workflow involves: 1. Define regions using a lower-resolution atlas 2. Sample center coordinates from those regions 3. Extract fixed-size patches at those centers from high-resolution data</p> <p>This approach is particularly useful when: - Your atlas is at a different resolution than your training data - You need consistent patch dimensions for ML models (e.g., 256\u00d7256\u00d7256 voxels) - You want to sample from specific anatomical regions</p>"},{"location":"examples/ml_patch_sampling/#basic-workflow","title":"Basic Workflow","text":""},{"location":"examples/ml_patch_sampling/#step-1-load-atlas-and-sample-centers","title":"Step 1: Load Atlas and Sample Centers","text":"<pre><code>from zarrnii import ZarrNiiAtlas\n\n# Load atlas (e.g., at 1mm resolution)\natlas = ZarrNiiAtlas.from_files(\n    dseg_path=\"atlas_dseg.nii.gz\",\n    labels_path=\"atlas_dseg.tsv\"\n)\n\n# Sample 100 centers from cortical regions\ncenters = atlas.sample_region_patches(\n    n_patches=100,\n    region_ids=\"Cortex\",\n    seed=42  # For reproducibility\n)\n\nprint(f\"Sampled {len(centers)} centers\")\nprint(f\"First center: {centers[0]} mm (physical coordinates)\")\n</code></pre> <p>Output:</p> <pre><code>Sampled 100 centers\nFirst center: (45.2, 67.8, 123.4) mm (physical coordinates)\n</code></pre>"},{"location":"examples/ml_patch_sampling/#step-2-extract-fixed-size-patches","title":"Step 2: Extract Fixed-Size Patches","text":"<pre><code>from zarrnii import ZarrNii\n\n# Load high-resolution image (e.g., at 0.1mm resolution)\nhighres_image = ZarrNii.from_ome_zarr(\"microscopy_data.ome.zarr\", level=0)\n\n# Extract 256x256x256 voxel patches at each center\npatches = highres_image.crop_centered(\n    centers,\n    patch_size=(256, 256, 256)  # Fixed size in voxels\n)\n\nprint(f\"Created {len(patches)} patches\")\nprint(f\"First patch shape: {patches[0].shape}\")\nprint(f\"All patches have consistent spatial dims: {all(p.shape[1:] == patches[0].shape[1:] for p in patches)}\")\n</code></pre> <p>Output:</p> <pre><code>Created 100 patches\nFirst patch shape: (1, 256, 256, 256)\nAll patches have consistent spatial dims: True\n</code></pre>"},{"location":"examples/ml_patch_sampling/#advanced-examples","title":"Advanced Examples","text":""},{"location":"examples/ml_patch_sampling/#multi-region-sampling","title":"Multi-Region Sampling","text":"<p>Sample from multiple anatomical regions:</p> <pre><code># Sample from multiple regions using a list\ncenters = atlas.sample_region_patches(\n    n_patches=50,\n    region_ids=[\"Hippocampus\", \"Amygdala\", \"Cortex\"],\n    seed=42\n)\n\n# Or use regex patterns\ncortical_centers = atlas.sample_region_patches(\n    n_patches=100,\n    regex=\".*[Cc]ortex.*\",  # Match any cortical regions\n    seed=42\n)\n</code></pre>"},{"location":"examples/ml_patch_sampling/#different-patch-sizes-for-different-tasks","title":"Different Patch Sizes for Different Tasks","text":"<pre><code># Large patches for context (512\u00b3 voxels)\nlarge_patches = highres_image.crop_centered(centers[:10], patch_size=(512, 512, 512))\n\n# Medium patches for training (256\u00b3 voxels)\ntraining_patches = highres_image.crop_centered(centers, patch_size=(256, 256, 256))\n\n# Small patches for fine details (128\u00b3 voxels)\ndetail_patches = highres_image.crop_centered(centers, patch_size=(128, 128, 128))\n</code></pre>"},{"location":"examples/ml_patch_sampling/#single-center-cropping","title":"Single Center Cropping","text":"<p>When you need just one patch, <code>crop_centered()</code> returns a single <code>ZarrNii</code> object (not a list):</p> <pre><code># Get one specific center\ncenter = centers[0]\n\n# Extract single patch\npatch = highres_image.crop_centered(\n    center,  # Single tuple, not a list\n    patch_size=(256, 256, 256)\n)\n\nprint(f\"Single patch type: {type(patch)}\")  # &lt;class 'zarrnii.core.ZarrNii'&gt;\nprint(f\"Shape: {patch.shape}\")\n</code></pre>"},{"location":"examples/ml_patch_sampling/#complete-ml-training-pipeline","title":"Complete ML Training Pipeline","text":"<p>Here's a complete example showing data preparation for ML training:</p> <pre><code>import numpy as np\nfrom zarrnii import ZarrNii, ZarrNiiAtlas\nfrom pathlib import Path\n\n# Configuration\nPATCH_SIZE = (256, 256, 256)\nN_TRAIN_PATCHES = 1000\nN_VAL_PATCHES = 200\nOUTPUT_DIR = Path(\"ml_training_data\")\nOUTPUT_DIR.mkdir(exist_ok=True)\n\n# Step 1: Load atlas and define regions of interest\natlas = ZarrNiiAtlas.from_files(\n    dseg_path=\"atlas_1mm_dseg.nii.gz\",\n    labels_path=\"atlas_1mm_dseg.tsv\"\n)\n\n# Step 2: Sample centers for training set\ntrain_centers = atlas.sample_region_patches(\n    n_patches=N_TRAIN_PATCHES,\n    region_ids=[\"Gray-Matter\", \"Cortex\"],\n    seed=42\n)\n\n# Step 3: Sample centers for validation set\nval_centers = atlas.sample_region_patches(\n    n_patches=N_VAL_PATCHES,\n    region_ids=[\"Gray-Matter\", \"Cortex\"],\n    seed=123  # Different seed for validation\n)\n\n# Step 4: Load high-resolution data\nhighres_image = ZarrNii.from_ome_zarr(\n    \"specimen_highres.ome.zarr\",\n    level=0  # Highest resolution\n)\n\n# Step 5: Extract training patches\nprint(\"Extracting training patches...\")\ntrain_patches = highres_image.crop_centered(train_centers, patch_size=PATCH_SIZE)\n\n# Step 6: Extract validation patches\nprint(\"Extracting validation patches...\")\nval_patches = highres_image.crop_centered(val_centers, patch_size=PATCH_SIZE)\n\n# Step 7: Save patches (example with one format)\nprint(\"Saving patches...\")\nfor i, patch in enumerate(train_patches):\n    patch.to_nifti(OUTPUT_DIR / f\"train_patch_{i:04d}.nii.gz\")\n\nfor i, patch in enumerate(val_patches):\n    patch.to_nifti(OUTPUT_DIR / f\"val_patch_{i:04d}.nii.gz\")\n\nprint(f\"Training patches: {len(train_patches)}\")\nprint(f\"Validation patches: {len(val_patches)}\")\nprint(f\"Patch size: {PATCH_SIZE}\")\nprint(f\"Output directory: {OUTPUT_DIR}\")\n</code></pre>"},{"location":"examples/ml_patch_sampling/#integration-with-pytorch-dataloader","title":"Integration with PyTorch DataLoader","text":"<p>Convert patches to numpy arrays for direct use with PyTorch:</p> <pre><code>import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nclass PatchDataset(Dataset):\n    def __init__(self, patches, transform=None):\n        \"\"\"\n        Args:\n            patches: List of ZarrNii objects\n            transform: Optional transforms to apply\n        \"\"\"\n        self.patches = patches\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.patches)\n\n    def __getitem__(self, idx):\n        # Get patch data\n        patch = self.patches[idx]\n\n        # Convert to numpy array\n        data = patch.data\n        if hasattr(data, 'compute'):\n            data = data.compute()\n\n        # Convert to float32 and normalize\n        data = data.astype(np.float32)\n\n        # Remove channel dimension if present\n        if data.shape[0] == 1:\n            data = data[0]\n\n        # Apply transforms if any\n        if self.transform:\n            data = self.transform(data)\n\n        return torch.from_numpy(data)\n\n# Create datasets\ntrain_dataset = PatchDataset(train_patches)\nval_dataset = PatchDataset(val_patches)\n\n# Create dataloaders\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=4,\n    shuffle=True,\n    num_workers=4\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=4,\n    shuffle=False,\n    num_workers=4\n)\n\n# Use in training loop\nfor batch in train_loader:\n    # batch shape: (batch_size, z, y, x)\n    # Your training code here\n    pass\n</code></pre>"},{"location":"examples/ml_patch_sampling/#multi-resolution-training-strategy","title":"Multi-Resolution Training Strategy","text":"<p>Sample at different atlas resolutions for multi-scale analysis:</p> <pre><code># Load atlases at different resolutions\natlas_1mm = ZarrNiiAtlas.from_files(\"atlas_1mm_dseg.nii.gz\", \"atlas_dseg.tsv\")\natlas_2mm = ZarrNiiAtlas.from_files(\"atlas_2mm_dseg.nii.gz\", \"atlas_dseg.tsv\")\n\n# Sample centers from each resolution\ncenters_1mm = atlas_1mm.sample_region_patches(\n    n_patches=500,\n    region_ids=\"Cortex\",\n    seed=42\n)\n\ncenters_2mm = atlas_2mm.sample_region_patches(\n    n_patches=500,\n    region_ids=\"Cortex\",\n    seed=42\n)\n\n# Load image at multiple resolutions\nhighres_image = ZarrNii.from_ome_zarr(\"data.ome.zarr\", level=0)  # Highest resolution\nlowres_image = ZarrNii.from_ome_zarr(\"data.ome.zarr\", level=2)   # Lower resolution\n\n# Extract patches at different scales\n# All patches will be 256\u00b3 voxels, but represent different physical sizes\nhighres_patches = highres_image.crop_centered(centers_1mm, patch_size=(256, 256, 256))\nlowres_patches = lowres_image.crop_centered(centers_2mm, patch_size=(256, 256, 256))\n</code></pre>"},{"location":"examples/ml_patch_sampling/#balanced-sampling-from-multiple-regions","title":"Balanced Sampling from Multiple Regions","text":"<p>Ensure equal representation from different anatomical regions:</p> <pre><code># Define regions and number of patches per region\nregions_config = {\n    \"Cortex\": 300,\n    \"Hippocampus\": 200,\n    \"Thalamus\": 200,\n    \"Cerebellum\": 300\n}\n\n# Sample from each region\nall_centers = []\nfor region_name, n_patches in regions_config.items():\n    centers = atlas.sample_region_patches(\n        n_patches=n_patches,\n        region_ids=region_name,\n        seed=42\n    )\n    all_centers.extend(centers)\n\nprint(f\"Total centers sampled: {len(all_centers)}\")\n\n# Extract all patches\npatches = highres_image.crop_centered(all_centers, patch_size=(256, 256, 256))\n</code></pre>"},{"location":"examples/ml_patch_sampling/#quality-control-verify-patch-coverage","title":"Quality Control: Verify Patch Coverage","text":"<p>Check that patches cover the intended regions:</p> <pre><code># Sample centers\ncenters = atlas.sample_region_patches(\n    n_patches=50,\n    region_ids=\"Hippocampus\",\n    seed=42\n)\n\n# Extract patches from the atlas itself to verify regions\natlas_patches = atlas.crop_centered(centers, patch_size=(64, 64, 64))\n\n# Check that patches contain the target region label\nhippocampus_label = atlas.get_region_info(\"Hippocampus\")[\"index\"]\n\nfor i, patch in enumerate(atlas_patches):\n    data = patch.data\n    if hasattr(data, 'compute'):\n        data = data.compute()\n\n    # Check if hippocampus label is present\n    has_hippocampus = np.any(data == hippocampus_label)\n    print(f\"Patch {i}: Contains Hippocampus = {has_hippocampus}\")\n</code></pre>"},{"location":"examples/ml_patch_sampling/#memory-efficient-processing","title":"Memory-Efficient Processing","text":"<p>For large datasets, process patches in batches:</p> <pre><code>def process_in_batches(centers, image, patch_size, batch_size=10):\n    \"\"\"Process patches in batches to save memory.\"\"\"\n    n_patches = len(centers)\n\n    for i in range(0, n_patches, batch_size):\n        # Get batch of centers\n        batch_centers = centers[i:i+batch_size]\n\n        # Extract batch of patches\n        batch_patches = image.crop_centered(batch_centers, patch_size=patch_size)\n\n        # Process batch (e.g., save, train, etc.)\n        for j, patch in enumerate(batch_patches):\n            patch_idx = i + j\n            # Your processing code here\n            patch.to_nifti(f\"patch_{patch_idx:04d}.nii.gz\")\n\n        print(f\"Processed patches {i} to {i+len(batch_patches)-1}\")\n\n# Use the function\nprocess_in_batches(\n    centers=train_centers,\n    image=highres_image,\n    patch_size=(256, 256, 256),\n    batch_size=10\n)\n</code></pre>"},{"location":"examples/ml_patch_sampling/#tips-and-best-practices","title":"Tips and Best Practices","text":""},{"location":"examples/ml_patch_sampling/#1-coordinate-systems","title":"1. Coordinate Systems","text":"<ul> <li>Centers are always in physical space (mm) in (x, y, z) order</li> <li>Patch sizes are always in voxels in (x, y, z) order</li> <li>This allows sampling at one resolution and cropping at another</li> </ul>"},{"location":"examples/ml_patch_sampling/#2-reproducibility","title":"2. Reproducibility","text":"<ul> <li>Always set a <code>seed</code> for reproducible sampling</li> <li>Save the seed value with your training configuration</li> </ul> <pre><code># Save configuration\nconfig = {\n    'seed': 42,\n    'n_patches': 1000,\n    'patch_size': (256, 256, 256),\n    'regions': [\"Cortex\", \"Hippocampus\"],\n}\n</code></pre>"},{"location":"examples/ml_patch_sampling/#3-handling-edge-cases","title":"3. Handling Edge Cases","text":"<ul> <li>Patches near image boundaries may be smaller than requested</li> <li>Check patch dimensions before training:</li> </ul> <pre><code># Filter patches by size\nmin_size = (200, 200, 200)  # Minimum acceptable size\nvalid_patches = [\n    p for p in patches \n    if all(s &gt;= m for s, m in zip(p.shape[1:], min_size))\n]\n</code></pre>"},{"location":"examples/ml_patch_sampling/#4-data-augmentation","title":"4. Data Augmentation","text":"<ul> <li>Combine with standard augmentation techniques:</li> <li>Rotation (use <code>apply_transform()</code>)</li> <li>Flipping</li> <li>Intensity scaling</li> <li>Adding noise</li> </ul>"},{"location":"examples/ml_patch_sampling/#5-distributed-training","title":"5. Distributed Training","text":"<ul> <li>Sample centers once and share across workers:</li> </ul> <pre><code># In main process\ncenters = atlas.sample_region_patches(n_patches=10000, region_ids=\"Cortex\", seed=42)\n\n# Save centers for workers\nimport pickle\nwith open('centers.pkl', 'wb') as f:\n    pickle.dump(centers, f)\n\n# In worker processes, load and subset\nwith open('centers.pkl', 'rb') as f:\n    all_centers = pickle.load(f)\n\n# Each worker gets a subset\nworker_centers = all_centers[worker_id::n_workers]\npatches = image.crop_centered(worker_centers, patch_size=(256, 256, 256))\n</code></pre>"},{"location":"examples/ml_patch_sampling/#see-also","title":"See Also","text":"<ul> <li>Atlas Example - General atlas functionality</li> <li>Transformations - Spatial transformations and alignment</li> <li>Multiscale OME-Zarr - Working with multi-resolution data</li> </ul>"},{"location":"examples/multiscale/","title":"Multiscale OME-Zarr","text":"<p>This section covers working with multi-resolution OME-Zarr datasets, including creation, manipulation, and optimization strategies.</p>"},{"location":"examples/multiscale/#overview","title":"Overview","text":"<p>OME-Zarr supports multi-resolution image pyramids that enable efficient visualization and analysis at different scales. ZarrNii provides support for creating and reading multiscale datasets.</p>"},{"location":"examples/multiscale/#understanding-ome-zarr-multiscale","title":"Understanding OME-Zarr Multiscale","text":""},{"location":"examples/multiscale/#loading-multiscale-data","title":"Loading Multiscale Data","text":"<pre><code>from zarrnii import ZarrNii\nimport zarr\n\n# Load different resolution levels of a multiscale OME-Zarr dataset\nznimg_level0 = ZarrNii.from_ome_zarr(\"path/to/multiscale.zarr\", level=0)  # Full resolution\nznimg_level1 = ZarrNii.from_ome_zarr(\"path/to/multiscale.zarr\", level=1)  # Half resolution\nznimg_level2 = ZarrNii.from_ome_zarr(\"path/to/multiscale.zarr\", level=2)  # Quarter resolution\n\nprint(\"Level 0 shape:\", znimg_level0.darr.shape)\nprint(\"Level 1 shape:\", znimg_level1.darr.shape)\nprint(\"Level 2 shape:\", znimg_level2.darr.shape)\n</code></pre>"},{"location":"examples/multiscale/#inspecting-available-levels","title":"Inspecting Available Levels","text":"<pre><code># Use zarr directly to inspect the structure\nstore = zarr.open_group(\"path/to/multiscale.zarr\", mode='r')\nprint(\"Available arrays:\", list(store.keys()))\n\n# Check shapes at each level\nfor key in sorted(store.keys()):\n    if key.isdigit():\n        array = store[key]\n        print(f\"Level {key}: shape {array.shape}, chunks {array.chunks}\")\n</code></pre>"},{"location":"examples/multiscale/#creating-multiscale-ome-zarr","title":"Creating Multiscale OME-Zarr","text":""},{"location":"examples/multiscale/#basic-multiscale-creation","title":"Basic Multiscale Creation","text":"<pre><code># Load a single-resolution image\nznimg = ZarrNii.from_nifti(\"path/to/highres.nii\")\n\n# Create multiscale OME-Zarr with default parameters\nznimg.to_ome_zarr(\n    \"output_multiscale.zarr\",\n    max_layer=4  # Creates 4 downsampling levels\n)\n</code></pre>"},{"location":"examples/multiscale/#custom-multiscale-parameters","title":"Custom Multiscale Parameters","text":"<pre><code># Create multiscale with custom settings\nznimg.to_ome_zarr(\n    \"custom_multiscale.zarr\",\n    max_layer=6,  # More downsampling levels\n    scaling_method=None  # Use default downsampling\n)\n</code></pre>"},{"location":"examples/multiscale/#working-with-different-resolution-levels","title":"Working with Different Resolution Levels","text":""},{"location":"examples/multiscale/#processing-at-different-scales","title":"Processing at Different Scales","text":"<pre><code># Load and process at different resolutions for different tasks\n\n# Use low resolution for quick overview\nthumbnail = ZarrNii.from_ome_zarr(\"data.zarr\", level=3)\noverview_stats = compute_statistics(thumbnail)\n\n# Use medium resolution for analysis  \nanalysis_res = ZarrNii.from_ome_zarr(\"data.zarr\", level=1)\nfeature_map = extract_features(analysis_res)\n\n# Use full resolution for final processing\nfull_res = ZarrNii.from_ome_zarr(\"data.zarr\", level=0)\nfinal_result = apply_detailed_processing(full_res)\n</code></pre>"},{"location":"examples/multiscale/#multi-resolution-workflow","title":"Multi-Resolution Workflow","text":"<pre><code># Progressive processing workflow\ndef progressive_analysis(zarr_path):\n    # Start with thumbnail for parameter estimation\n    low_res = ZarrNii.from_ome_zarr(zarr_path, level=3)\n    parameters = estimate_parameters(low_res)\n\n    # Refine on medium resolution\n    med_res = ZarrNii.from_ome_zarr(zarr_path, level=1) \n    refined_params = refine_parameters(med_res, parameters)\n\n    # Apply to full resolution\n    full_res = ZarrNii.from_ome_zarr(zarr_path, level=0)\n    final_result = process_with_params(full_res, refined_params)\n\n    return final_result\n\nresult = progressive_analysis(\"multiscale_data.zarr\")\n</code></pre>"},{"location":"examples/multiscale/#channel-and-time-series-support","title":"Channel and Time Series Support","text":""},{"location":"examples/multiscale/#multi-channel-multiscale","title":"Multi-Channel Multiscale","text":"<pre><code># Load specific channels from multiscale data\n# Channel selection works with any resolution level\ndapi_full = ZarrNii.from_ome_zarr(\"multi_channel.zarr\", level=0, channels=[0])\ngfp_thumbnail = ZarrNii.from_ome_zarr(\"multi_channel.zarr\", level=3, channels=[1])\n\nprint(\"DAPI full resolution:\", dapi_full.darr.shape)\nprint(\"GFP thumbnail:\", gfp_thumbnail.darr.shape)\n</code></pre>"},{"location":"examples/multiscale/#channel-labels","title":"Channel labels","text":"<pre><code># If channel labels are present from OME metadata (e.g. for data from SPIMprep)\n# You can select channels based on label\nabeta_full = ZarrNii.from_ome_zarr(\"multi_channel.zarr\", level=3, channel_labels=[\"Abeta\"])\n</code></pre>"},{"location":"examples/multiscale/#memory-management","title":"Memory Management","text":""},{"location":"examples/multiscale/#efficient-loading","title":"Efficient Loading","text":"<pre><code># Load only what you need\n# Zarr and Dask handle lazy loading automatically\n\n# Load with appropriate chunking\nznimg = ZarrNii.from_ome_zarr(\"large_dataset.zarr\", level=1)\n\n# Process in blocks to manage memory\ndef process_large_dataset(znimg):\n    # Process data block by block\n    result = znimg.darr.map_blocks(\n        process_block,\n        dtype=np.float32,\n        drop_axis=None\n    )\n    return result\n\nprocessed = process_large_dataset(znimg)\n</code></pre>"},{"location":"examples/multiscale/#best-practices","title":"Best Practices","text":""},{"location":"examples/multiscale/#choosing-resolution-levels","title":"Choosing Resolution Levels","text":"<pre><code># Guidelines for choosing appropriate resolution levels\n\ndef choose_resolution_level(task_type, data_size):\n    \"\"\"Choose optimal resolution level based on task and data size\"\"\"\n    if task_type == \"thumbnail\":\n        return 4  # Very low resolution for quick preview\n    elif task_type == \"segmentation\":\n        return 1  # Medium resolution for segmentation\n    elif task_type == \"measurement\":\n        return 0  # Full resolution for accurate measurements\n    else:\n        # Default to medium resolution\n        return 2\n\n# Use the function\nlevel = choose_resolution_level(\"segmentation\", data.shape)\nznimg = ZarrNii.from_ome_zarr(\"data.zarr\", level=level)\n</code></pre>"},{"location":"examples/multiscale/#performance-tips","title":"Performance Tips","text":"<ol> <li>Choose appropriate levels: Use lower resolution levels for exploratory analysis</li> <li>Minimize data loading: Only load the resolution level you actually need</li> <li>Leverage lazy evaluation: Let Dask handle memory management automatically</li> </ol>"},{"location":"examples/multiscale/#see-also","title":"See Also","text":"<ul> <li>Downsampling and Upsampling for resolution change operations</li> <li>Working with Zarr and NIfTI for basic format operations  </li> <li>API Reference for detailed method documentation</li> </ul>"},{"location":"examples/near_isotropic/","title":"Near-Isotropic Downsampling","text":"<p>This example demonstrates the automatic near-isotropic downsampling feature, which is particularly useful for lightsheet microscopy data where Z resolution is often much finer than X/Y resolution.</p>"},{"location":"examples/near_isotropic/#problem-anisotropic-voxels","title":"Problem: Anisotropic Voxels","text":"<p>Many biomedical imaging modalities produce datasets with anisotropic voxels where one dimension has much finer resolution than others:</p> <pre><code>from zarrnii import ZarrNii\n\n# Load anisotropic lightsheet data\nznimg = ZarrNii.from_ome_zarr(\"lightsheet_data.ome.zarr\")\n\nprint(\"Original scales:\", znimg.scale)\n# Output might show: {'z': 0.25, 'y': 1.0, 'x': 1.0}\n# Z has 4x finer resolution than X/Y\n</code></pre>"},{"location":"examples/near_isotropic/#solution-automatic-downsampling","title":"Solution: Automatic Downsampling","text":"<p>The <code>downsample_near_isotropic</code> parameter automatically identifies and corrects anisotropic voxels:</p> <pre><code># Load with automatic near-isotropic downsampling\nznimg_isotropic = ZarrNii.from_ome_zarr(\n    \"lightsheet_data.ome.zarr\", \n    downsample_near_isotropic=True\n)\n\nprint(\"Isotropic scales:\", znimg_isotropic.scale)\n# Output: {'z': 1.0, 'y': 1.0, 'x': 1.0}\n# All dimensions now have the same resolution\n\nprint(\"Shape comparison:\")\nprint(f\"Original:  {znimg.darr.shape}\")\nprint(f\"Isotropic: {znimg_isotropic.darr.shape}\")\n# Z dimension is reduced by the downsampling factor\n</code></pre>"},{"location":"examples/near_isotropic/#how-it-works","title":"How It Works","text":"<p>The algorithm:</p> <ol> <li>Identifies the coarsest resolution (largest scale value) among spatial dimensions</li> <li>Calculates downsampling factors as powers of 2 for finer resolution dimensions  </li> <li>Applies selective downsampling using the existing downsample method</li> </ol> <pre><code># For scales z=0.25, y=1.0, x=1.0:\n# - Max scale = 1.0 (coarsest resolution)\n# - Z ratio = 1.0 / 0.25 = 4.0\n# - Z downsampling factor = 2^2 = 4\n# - Result: z=1.0, y=1.0, x=1.0 (isotropic)\n</code></pre>"},{"location":"examples/near_isotropic/#benefits","title":"Benefits","text":"<ul> <li>Improved processing efficiency: Isotropic voxels work better with many algorithms</li> <li>Consistent visualization: Equal sampling in all dimensions for 3D rendering</li> <li>Memory reduction: Removes unnecessary oversampling in fine dimensions</li> <li>Algorithm compatibility: Many image processing algorithms assume isotropic voxels</li> </ul>"},{"location":"examples/near_isotropic/#when-to-use","title":"When to Use","text":"<p>This feature is most beneficial for: - Lightsheet microscopy data with fine Z-sampling - High-resolution imaging with anisotropic acquisition - Preprocessing before analysis algorithms that assume isotropy - Visualization where consistent sampling is desired</p>"},{"location":"examples/near_isotropic/#manual-control","title":"Manual Control","text":"<p>For fine-grained control, you can still use manual downsampling:</p> <pre><code># Manual approach - downsample Z dimension by specific factor\nznimg_manual = znimg.downsample(along_z=4, along_y=1, along_x=1)\n\n# This gives the same result as downsample_near_isotropic=True\n# but allows custom control over factors\n</code></pre>"},{"location":"examples/near_isotropic/#performance-impact","title":"Performance Impact","text":"<p>The automatic downsampling: - Uses the same efficient downsampling algorithm as manual methods - Is applied lazily with Dask for memory efficiency - Reduces data size and subsequent processing time - Preserves all metadata and coordinate system information</p>"},{"location":"examples/near_isotropic/#see-also","title":"See Also","text":"<ul> <li>Downsampling and Upsampling for general resolution operations</li> <li>Basic Tasks for getting started with transformations</li> <li>API Reference for detailed parameter documentation</li> </ul>"},{"location":"examples/scaled_processing/","title":"Multi-Resolution Plugin Architecture","text":"<p>This section covers the multi-resolution plugin architecture that enables efficient processing where algorithms are computed at low resolution and applied to full resolution data.</p>"},{"location":"examples/scaled_processing/#overview","title":"Overview","text":"<p>The scaled processing plugin architecture in ZarrNii allows for efficient multi-resolution operations. This is particularly useful for algorithms that can be computed efficiently at lower resolution and then applied to the full-resolution data. Common use cases include:</p> <ul> <li>Bias field correction</li> <li>Background estimation</li> <li>Denoising operations</li> <li>Global intensity normalization</li> </ul> <p>ZarrNii uses the pluggy framework for its plugin system, providing a flexible and extensible architecture for creating custom processing plugins.</p>"},{"location":"examples/scaled_processing/#plugin-interface","title":"Plugin Interface","text":"<p>All scaled processing plugins must inherit from <code>ScaledProcessingPlugin</code> and implement the required hook methods decorated with <code>@hookimpl</code>:</p>"},{"location":"examples/scaled_processing/#lowres_funclowres_array-npndarray-npndarray","title":"<code>lowres_func(lowres_array: np.ndarray) -&gt; np.ndarray</code>","text":"<p>This function processes the downsampled data and returns a result that will be upsampled and applied to the full-resolution data.</p>"},{"location":"examples/scaled_processing/#highres_funcfullres_array-daskarray-upsampled_output-daskarray-daskarray","title":"<code>highres_func(fullres_array: dask.array, upsampled_output: dask.array) -&gt; dask.array</code>","text":"<p>This function receives the full-resolution dask array and the upsampled output (already upsampled to match the full-resolution shape), and applies the operation blockwise. The upsampling is handled internally by the <code>apply_scaled_processing</code> method.</p>"},{"location":"examples/scaled_processing/#scaled_processing_plugin_name-str","title":"<code>scaled_processing_plugin_name() -&gt; str</code>","text":"<p>Returns the name of the plugin.</p>"},{"location":"examples/scaled_processing/#scaled_processing_plugin_description-str","title":"<code>scaled_processing_plugin_description() -&gt; str</code>","text":"<p>Returns a description of what the plugin does.</p>"},{"location":"examples/scaled_processing/#basic-usage","title":"Basic Usage","text":"<pre><code>from zarrnii import ZarrNii, GaussianBiasFieldCorrection\n\n# Load your data\nznimg = ZarrNii.from_nifti(\"path/to/image.nii\")\n\n# Apply bias field correction\ncorrected = znimg.apply_scaled_processing(\n    GaussianBiasFieldCorrection(sigma=5.0),\n    downsample_factor=4\n)\n\n# Save result\ncorrected.to_nifti(\"corrected_image.nii\")\n</code></pre>"},{"location":"examples/scaled_processing/#built-in-plugins","title":"Built-in Plugins","text":""},{"location":"examples/scaled_processing/#gaussianbiasfieldcorrection","title":"GaussianBiasFieldCorrection","text":"<p>A simple bias field correction plugin that estimates smooth bias fields using Gaussian smoothing at low resolution and applies correction by division.</p> <pre><code># Basic usage with default parameters\ncorrected = znimg.apply_scaled_processing(GaussianBiasFieldCorrection())\n\n# Custom parameters\ncorrected = znimg.apply_scaled_processing(\n    GaussianBiasFieldCorrection(sigma=3.0, mode='constant'),\n    downsample_factor=8\n)\n</code></pre> <p>Parameters: - <code>sigma</code>: Standard deviation for Gaussian smoothing (default: 5.0) - <code>mode</code>: Boundary condition for smoothing (default: 'reflect')</p>"},{"location":"examples/scaled_processing/#n4biasfieldcorrection","title":"N4BiasFieldCorrection","text":"<p>A more sophisticated bias field correction plugin that uses the N4 algorithm from ANTsPy for superior bias field estimation at low resolution, then applies correction by division.</p> <p>Installation: Requires <code>antspyx</code> to be installed:</p> <pre><code># Install with N4 support\npip install 'zarrnii[n4]'\n# or install antspyx directly\npip install antspyx\n</code></pre> <pre><code>from zarrnii import N4BiasFieldCorrection\n\n# Basic usage with default parameters\ncorrected = znimg.apply_scaled_processing(N4BiasFieldCorrection())\n\n# Custom parameters for more control\ncorrected = znimg.apply_scaled_processing(\n    N4BiasFieldCorrection(\n        spline_spacing=150.0,\n        convergence={'iters': [25, 25], 'tol': 0.001},\n        shrink_factor=2\n    ),\n    downsample_factor=4\n)\n</code></pre> <p>Parameters: - <code>spline_spacing</code>: Spacing between knots for spline fitting (default: 200.0) - <code>convergence</code>: Dictionary with 'iters' (list) and 'tol' (float) for convergence criteria (default: {'iters': [50], 'tol': 0.001}) - <code>shrink_factor</code>: Shrink factor for processing (default: 1)</p>"},{"location":"examples/scaled_processing/#creating-custom-plugins","title":"Creating Custom Plugins","text":"<p>You can create custom plugins by inheriting from <code>ScaledProcessingPlugin</code> and implementing the required hook methods:</p> <pre><code>from zarrnii.plugins import ScaledProcessingPlugin\nfrom zarrnii.plugins.scaled_processing.base import hookimpl\nimport numpy as np\nimport dask.array as da\nfrom scipy import ndimage\n\nclass CustomPlugin(ScaledProcessingPlugin):\n    \"\"\"Custom scaled processing plugin example.\"\"\"\n\n    def __init__(self, param1=1.0, **kwargs):\n        super().__init__(param1=param1, **kwargs)\n        self.param1 = param1\n\n    @hookimpl\n    def lowres_func(self, lowres_array: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Process low-resolution data.\"\"\"\n        # Your low-resolution algorithm here\n        # Example: compute some correction map\n        correction_map = np.ones_like(lowres_array) * self.param1\n        return correction_map\n\n    @hookimpl\n    def highres_func(self, fullres_array: da.Array, upsampled_output: da.Array) -&gt; da.Array:\n        \"\"\"Apply correction to full-resolution data.\"\"\"\n        # The upsampling is handled internally by apply_scaled_processing\n        # This example shows a simple multiplication operation\n        # Apply correction directly (both arrays are same size)\n        result = fullres_array * upsampled_output\n        return result\n\n    @hookimpl\n    def scaled_processing_plugin_name(self) -&gt; str:\n        \"\"\"Return the name of the plugin.\"\"\"\n        return \"Custom Plugin\"\n\n    @hookimpl\n    def scaled_processing_plugin_description(self) -&gt; str:\n        \"\"\"Return a description of the plugin.\"\"\"\n        return \"A custom multi-resolution processing plugin\"\n\n# Method 1: Direct usage with ZarrNii (recommended for simple cases)\nresult = znimg.apply_scaled_processing(CustomPlugin(param1=2.0))\n\n# Method 2: Using the plugin manager (recommended for external plugins)\nfrom zarrnii.plugins import get_plugin_manager\n\npm = get_plugin_manager()\nplugin = CustomPlugin(param1=2.0)\npm.register(plugin)\n\n# Now the plugin is available through the plugin manager\n# and can be discovered by other tools\n</code></pre>"},{"location":"examples/scaled_processing/#external-plugin-development","title":"External Plugin Development","text":"<p>External plugins allow you to package and distribute your custom plugins as separate Python packages that can be discovered and used by ZarrNii. Here's a complete example of how to create an external plugin:</p>"},{"location":"examples/scaled_processing/#step-1-create-your-plugin-package","title":"Step 1: Create Your Plugin Package","text":"<p>Create a new Python package with the following structure:</p> <pre><code>my_zarrnii_plugin/\n\u251c\u2500\u2500 setup.py or pyproject.toml\n\u2514\u2500\u2500 my_zarrnii_plugin/\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 my_plugin.py\n</code></pre>"},{"location":"examples/scaled_processing/#step-2-implement-your-plugin","title":"Step 2: Implement Your Plugin","text":"<p>In <code>my_plugin.py</code>:</p> <pre><code>\"\"\"Custom external plugin for ZarrNii.\"\"\"\nfrom zarrnii.plugins import ScaledProcessingPlugin\nfrom zarrnii.plugins.scaled_processing.base import hookimpl\nimport numpy as np\nimport dask.array as da\nfrom scipy import ndimage\n\nclass MyExternalPlugin(ScaledProcessingPlugin):\n    \"\"\"An example external plugin for demonstration.\"\"\"\n\n    def __init__(self, smoothing_sigma=2.0, scale_factor=1.5, **kwargs):\n        \"\"\"Initialize the plugin with custom parameters.\n\n        Args:\n            smoothing_sigma: Sigma for Gaussian smoothing\n            scale_factor: Scaling factor for the correction\n        \"\"\"\n        super().__init__(\n            smoothing_sigma=smoothing_sigma,\n            scale_factor=scale_factor,\n            **kwargs\n        )\n        self.smoothing_sigma = smoothing_sigma\n        self.scale_factor = scale_factor\n\n    @hookimpl\n    def lowres_func(self, lowres_array: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Compute correction map at low resolution.\"\"\"\n        if lowres_array.size == 0:\n            raise ValueError(\"Input array is empty\")\n\n        # Apply smoothing to estimate low-frequency components\n        smoothed = ndimage.gaussian_filter(\n            lowres_array.astype(np.float32),\n            sigma=self.smoothing_sigma\n        )\n\n        # Scale the correction map\n        correction_map = smoothed * self.scale_factor\n\n        return correction_map\n\n    @hookimpl\n    def highres_func(self, fullres_array: da.Array, upsampled_output: da.Array) -&gt; da.Array:\n        \"\"\"Apply the correction to full-resolution data.\"\"\"\n        # Apply the correction by division\n        epsilon = np.finfo(np.float32).eps\n        corrected = fullres_array / da.maximum(upsampled_output, epsilon)\n        return corrected\n\n    @hookimpl\n    def scaled_processing_plugin_name(self) -&gt; str:\n        \"\"\"Return plugin name.\"\"\"\n        return \"My External Plugin\"\n\n    @hookimpl\n    def scaled_processing_plugin_description(self) -&gt; str:\n        \"\"\"Return plugin description.\"\"\"\n        return (\n            f\"External plugin with smoothing (sigma={self.smoothing_sigma}) \"\n            f\"and scaling (factor={self.scale_factor})\"\n        )\n</code></pre>"},{"location":"examples/scaled_processing/#step-3-make-your-plugin-discoverable","title":"Step 3: Make Your Plugin Discoverable","text":"<p>In <code>__init__.py</code>:</p> <pre><code>\"\"\"My ZarrNii Plugin Package.\"\"\"\nfrom .my_plugin import MyExternalPlugin\n\n__all__ = [\"MyExternalPlugin\"]\n</code></pre>"},{"location":"examples/scaled_processing/#step-4-configure-your-package","title":"Step 4: Configure Your Package","text":"<p>In <code>pyproject.toml</code>:</p> <pre><code>[project]\nname = \"my-zarrnii-plugin\"\nversion = \"0.1.0\"\ndescription = \"My custom ZarrNii processing plugin\"\ndependencies = [\n    \"zarrnii&gt;=0.1.0\",\n    \"scipy&gt;=1.11.0\",\n]\n\n[project.entry-points.\"zarrnii.plugins\"]\nmy_external_plugin = \"my_zarrnii_plugin:MyExternalPlugin\"\n</code></pre>"},{"location":"examples/scaled_processing/#step-5-use-your-external-plugin","title":"Step 5: Use Your External Plugin","text":"<p>After installing your plugin package (<code>pip install my-zarrnii-plugin</code>):</p> <pre><code>from zarrnii import ZarrNii\nfrom my_zarrnii_plugin import MyExternalPlugin\n\n# Load your data\nznimg = ZarrNii.from_nifti(\"input.nii\")\n\n# Use the external plugin directly\nresult = znimg.apply_scaled_processing(\n    MyExternalPlugin(smoothing_sigma=3.0, scale_factor=2.0),\n    downsample_factor=4\n)\n\n# Or register it with the plugin manager\nfrom zarrnii.plugins import get_plugin_manager\n\npm = get_plugin_manager()\nplugin = MyExternalPlugin(smoothing_sigma=3.0, scale_factor=2.0)\npm.register(plugin)\n\n# Query registered plugins\nfor p in pm.get_plugins():\n    names = pm.hook.scaled_processing_plugin_name()\n    descriptions = pm.hook.scaled_processing_plugin_description()\n    print(f\"Plugin: {names[0]} - {descriptions[0]}\")\n</code></pre>"},{"location":"examples/scaled_processing/#advanced-usage","title":"Advanced Usage","text":""},{"location":"examples/scaled_processing/#custom-downsampling-factors","title":"Custom Downsampling Factors","text":"<pre><code># Use different downsampling factors\nresult = znimg.apply_scaled_processing(\n    GaussianBiasFieldCorrection(),\n    downsample_factor=8  # 8x downsampling\n)\n</code></pre>"},{"location":"examples/scaled_processing/#custom-chunk-sizes","title":"Custom Chunk Sizes","text":"<pre><code># Specify custom chunk sizes for low-resolution processing\n# The chunk_size parameter controls the chunking of low-resolution intermediate results\nresult = znimg.apply_scaled_processing(\n    GaussianBiasFieldCorrection(),\n    chunk_size=(1, 32, 32, 32)  # Used for low-res processing chunks\n)\n</code></pre>"},{"location":"examples/scaled_processing/#temporary-file-options","title":"Temporary File Options","text":"<p>The framework uses temporary OME-Zarr files to break up the dask computation graph for better performance. You can control this behavior:</p> <pre><code># Disable temporary file usage (may impact performance on large datasets)\nresult = znimg.apply_scaled_processing(\n    GaussianBiasFieldCorrection(),\n    use_temp_zarr=False\n)\n\n# Use custom temporary file location\nresult = znimg.apply_scaled_processing(\n    GaussianBiasFieldCorrection(),\n    temp_zarr_path=\"/custom/path/temp_processing.ome.zarr\"\n)\n</code></pre>"},{"location":"examples/scaled_processing/#plugin-class-vs-instance","title":"Plugin Class vs Instance","text":"<pre><code># Using plugin class (parameters passed as kwargs)\nresult1 = znimg.apply_scaled_processing(GaussianBiasFieldCorrection, sigma=3.0)\n\n# Using plugin instance (parameters set during initialization)\nplugin = GaussianBiasFieldCorrection(sigma=3.0)\nresult2 = znimg.apply_scaled_processing(plugin)\n</code></pre>"},{"location":"examples/scaled_processing/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Downsampling Factor: Higher factors reduce computation time but may reduce accuracy</li> <li>Chunk Sizes: Optimize for your memory constraints and processing requirements</li> <li>Algorithm Complexity: The <code>lowres_func</code> runs on small numpy arrays, while <code>highres_func</code> uses dask for scalability</li> <li>Temporary Files: The default temporary OME-Zarr approach breaks up dask computation graphs for better performance on large datasets. Disable only if you have specific memory/disk constraints</li> <li>Dask-based Upsampling: Uses ZarrNii's <code>.upsample()</code> method which leverages dask for efficient parallel upsampling</li> </ol>"},{"location":"examples/scaled_processing/#integration-with-other-operations","title":"Integration with Other Operations","text":"<p>The scaled processing plugins integrate seamlessly with other ZarrNii operations:</p> <pre><code># Chain operations\nresult = (znimg\n    .apply_scaled_processing(GaussianBiasFieldCorrection())\n    .downsample(level=1)\n    .segment_otsu())\n\n# Save to different formats\nresult.to_nifti(\"processed.nii\")\nresult.to_ome_zarr(\"processed.ome.zarr\")\n</code></pre>"},{"location":"examples/scaled_processing/#see-also","title":"See Also","text":"<ul> <li>Segmentation Plugins for other plugin architectures</li> <li>Downsampling and Upsampling for resolution operations</li> <li>API Reference for detailed method documentation</li> </ul>"},{"location":"examples/segmentation_example/","title":"Segmentation Plugin Example","text":"<p>This example demonstrates how to use the segmentation plugin system in ZarrNii. The plugin system is built on the pluggy framework, providing a flexible and extensible architecture for implementing custom segmentation algorithms.</p>"},{"location":"examples/segmentation_example/#plugin-architecture-overview","title":"Plugin Architecture Overview","text":"<p>ZarrNii's segmentation plugins use pluggy hook specifications and implementations: - Hook specifications define the interface that all plugins must implement - Hook implementations (using <code>@hookimpl</code> decorator) provide the actual functionality - Plugins can be used directly or registered with the plugin manager for discovery</p>"},{"location":"examples/segmentation_example/#basic-otsu-segmentation","title":"Basic Otsu Segmentation","text":"<pre><code>import numpy as np\nimport dask.array as da\nfrom zarrnii import ZarrNii, OtsuSegmentation\n\n# Load or create your image data\n# For this example, we'll create synthetic bimodal data\nnp.random.seed(42)\nimage_data = np.random.normal(0.2, 0.05, (1, 50, 100, 100))  # Background\nimage_data[0, 20:30, 40:60, 40:60] = np.random.normal(0.8, 0.05, (10, 20, 20))  # Foreground\n\n# Create ZarrNii instance\ndarr = da.from_array(image_data, chunks=(1, 25, 50, 50))\nznimg = ZarrNii.from_darr(darr, axes_order=\"ZYX\", orientation=\"RAS\")\n\n# Method 1: Using the convenience method\nsegmented = znimg.segment_otsu(nbins=256)\n\n# Method 2: Using the plugin directly\nplugin = OtsuSegmentation(nbins=256)\nsegmented = znimg.segment(plugin)\n\n# Method 3: Using plugin class with parameters\nsegmented = znimg.segment(OtsuSegmentation, nbins=128)\n\n# The result is a new ZarrNii instance with binary segmentation\nprint(f\"Original shape: {znimg.shape}\")\nprint(f\"Segmented shape: {segmented.shape}\")\nprint(f\"Segmented dtype: {segmented.data.dtype}\")\nprint(f\"Unique values: {np.unique(segmented.data.compute())}\")\n\n# Save segmented result as OME-Zarr\nsegmented.to_ome_zarr(\"segmented_image.ome.zarr\")\n</code></pre>"},{"location":"examples/segmentation_example/#custom-chunk-processing","title":"Custom Chunk Processing","text":"<p>For large datasets, you can control the chunk size for blockwise processing:</p> <pre><code># Segment with custom chunk size for memory efficiency\ncustom_chunks = (1, 10, 25, 25)\nsegmented = znimg.segment_otsu(chunk_size=custom_chunks)\n\n# The segmentation will be applied block-wise using dask\nresult_data = segmented.data.compute()\n</code></pre>"},{"location":"examples/segmentation_example/#creating-custom-segmentation-plugins","title":"Creating Custom Segmentation Plugins","text":"<p>You can create your own segmentation plugins by inheriting from <code>SegmentationPlugin</code> and implementing the required hook methods with the <code>@hookimpl</code> decorator:</p> <pre><code>from zarrnii.plugins import SegmentationPlugin\nfrom zarrnii.plugins.segmentation.base import hookimpl\nimport numpy as np\n\nclass ThresholdSegmentation(SegmentationPlugin):\n    \"\"\"Simple threshold-based segmentation plugin.\"\"\"\n\n    def __init__(self, threshold: float = 0.5, **kwargs):\n        \"\"\"Initialize the plugin.\n\n        Args:\n            threshold: Threshold value for segmentation\n        \"\"\"\n        super().__init__(threshold=threshold, **kwargs)\n        self.threshold = threshold\n\n    @hookimpl\n    def segment(self, image: np.ndarray, metadata=None) -&gt; np.ndarray:\n        \"\"\"Apply threshold segmentation.\n\n        Args:\n            image: Input image as numpy array\n            metadata: Optional metadata dictionary\n\n        Returns:\n            Binary segmentation mask\n        \"\"\"\n        binary_mask = image &gt; self.threshold\n        return binary_mask.astype(np.uint8)\n\n    @hookimpl\n    def segmentation_plugin_name(self) -&gt; str:\n        \"\"\"Return the name of the plugin.\"\"\"\n        return \"Threshold Segmentation\"\n\n    @hookimpl\n    def segmentation_plugin_description(self) -&gt; str:\n        \"\"\"Return a description of the plugin.\"\"\"\n        return f\"Simple thresholding at value {self.threshold}\"\n\n# Method 1: Direct usage with ZarrNii (recommended for simple cases)\ncustom_plugin = ThresholdSegmentation(threshold=0.3)\nsegmented = znimg.segment(custom_plugin)\n\n# Method 2: Using the plugin manager (recommended for external plugins)\nfrom zarrnii.plugins import get_plugin_manager\n\npm = get_plugin_manager()\nplugin = ThresholdSegmentation(threshold=0.3)\npm.register(plugin)\n\n# Now the plugin is available through the plugin manager\n# and can be discovered by other tools\nregistered_plugins = pm.get_plugins()\nprint(f\"Registered {len(registered_plugins)} plugins\")\n</code></pre>"},{"location":"examples/segmentation_example/#external-segmentation-plugin-development","title":"External Segmentation Plugin Development","text":"<p>External plugins allow you to package and distribute your custom segmentation algorithms as separate Python packages. Here's a complete example:</p>"},{"location":"examples/segmentation_example/#step-1-create-your-plugin-package-structure","title":"Step 1: Create Your Plugin Package Structure","text":"<pre><code>my_segmentation_plugin/\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 my_segmentation_plugin/\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 adaptive_threshold.py\n</code></pre>"},{"location":"examples/segmentation_example/#step-2-implement-your-segmentation-plugin","title":"Step 2: Implement Your Segmentation Plugin","text":"<p>In <code>adaptive_threshold.py</code>:</p> <pre><code>\"\"\"Adaptive threshold segmentation plugin.\"\"\"\nfrom zarrnii.plugins import SegmentationPlugin\nfrom zarrnii.plugins.segmentation.base import hookimpl\nimport numpy as np\nfrom skimage.filters import threshold_local\n\nclass AdaptiveThresholdSegmentation(SegmentationPlugin):\n    \"\"\"Adaptive thresholding segmentation for images with varying illumination.\"\"\"\n\n    def __init__(self, block_size=35, offset=10, method='gaussian', **kwargs):\n        \"\"\"Initialize adaptive threshold segmentation.\n\n        Args:\n            block_size: Size of pixel neighborhood for threshold calculation\n            offset: Constant subtracted from weighted mean\n            method: Method for computing threshold ('gaussian' or 'mean')\n        \"\"\"\n        super().__init__(\n            block_size=block_size,\n            offset=offset,\n            method=method,\n            **kwargs\n        )\n        self.block_size = block_size\n        self.offset = offset\n        self.method = method\n\n    @hookimpl\n    def segment(self, image: np.ndarray, metadata=None) -&gt; np.ndarray:\n        \"\"\"Segment image using adaptive thresholding.\n\n        Args:\n            image: Input image as numpy array\n            metadata: Optional metadata (unused)\n\n        Returns:\n            Binary segmentation mask\n        \"\"\"\n        if image.size == 0:\n            raise ValueError(\"Input image is empty\")\n\n        if image.ndim &lt; 2:\n            raise ValueError(\"Input image must be at least 2D\")\n\n        # Work with 2D slice for threshold computation\n        work_image = image\n        if work_image.ndim &gt; 2:\n            # Use first channel/slice if multi-dimensional\n            if work_image.ndim == 3 and work_image.shape[0] &lt;= 4:\n                work_image = work_image[0]\n            elif work_image.ndim &gt; 3:\n                work_image = work_image.reshape(-1, *work_image.shape[-2:])[0]\n\n        # Compute adaptive threshold\n        threshold = threshold_local(\n            work_image,\n            block_size=self.block_size,\n            offset=self.offset,\n            method=self.method\n        )\n\n        # Apply threshold to original image\n        binary_mask = image &gt; threshold\n\n        return binary_mask.astype(np.uint8)\n\n    @hookimpl\n    def segmentation_plugin_name(self) -&gt; str:\n        \"\"\"Return the name of the plugin.\"\"\"\n        return \"Adaptive Threshold Segmentation\"\n\n    @hookimpl\n    def segmentation_plugin_description(self) -&gt; str:\n        \"\"\"Return a description of the plugin.\"\"\"\n        return (\n            f\"Adaptive thresholding using {self.method} method \"\n            f\"(block_size={self.block_size}, offset={self.offset})\"\n        )\n</code></pre>"},{"location":"examples/segmentation_example/#step-3-configure-package-discovery","title":"Step 3: Configure Package Discovery","text":"<p>In <code>__init__.py</code>:</p> <pre><code>\"\"\"My Segmentation Plugin Package.\"\"\"\nfrom .adaptive_threshold import AdaptiveThresholdSegmentation\n\n__all__ = [\"AdaptiveThresholdSegmentation\"]\n</code></pre> <p>In <code>pyproject.toml</code>:</p> <pre><code>[project]\nname = \"my-segmentation-plugin\"\nversion = \"0.1.0\"\ndescription = \"Adaptive threshold segmentation plugin for ZarrNii\"\ndependencies = [\n    \"zarrnii&gt;=0.1.0\",\n    \"scikit-image&gt;=0.21.0\",\n]\n\n[project.entry-points.\"zarrnii.plugins\"]\nadaptive_threshold = \"my_segmentation_plugin:AdaptiveThresholdSegmentation\"\n</code></pre>"},{"location":"examples/segmentation_example/#step-4-use-your-external-plugin","title":"Step 4: Use Your External Plugin","text":"<p>After installing (<code>pip install my-segmentation-plugin</code>):</p> <pre><code>from zarrnii import ZarrNii\nfrom my_segmentation_plugin import AdaptiveThresholdSegmentation\n\n# Load your data\nznimg = ZarrNii.from_ome_zarr(\"input.ome.zarr\")\n\n# Use the plugin directly\nsegmented = znimg.segment(\n    AdaptiveThresholdSegmentation(block_size=51, offset=5)\n)\n\n# Or register with the plugin manager for discovery\nfrom zarrnii.plugins import get_plugin_manager\n\npm = get_plugin_manager()\nplugin = AdaptiveThresholdSegmentation(block_size=51, offset=5)\npm.register(plugin)\n\n# Call hooks through the plugin manager\ntest_image = znimg.data.compute()\nresults = pm.hook.segment(image=test_image)\n\n# Get plugin information\nnames = pm.hook.segmentation_plugin_name()\ndescriptions = pm.hook.segmentation_plugin_description()\nprint(f\"Using: {names[0]} - {descriptions[0]}\")\n</code></pre>"},{"location":"examples/segmentation_example/#working-with-multi-channel-images","title":"Working with Multi-channel Images","text":"<p>The segmentation plugins automatically handle multi-channel images:</p> <pre><code># Create multi-channel test data\nmultichannel_data = np.random.rand(3, 50, 100, 100)  # 3 channels\nmultichannel_data[0, 20:30, 40:60, 40:60] += 0.5  # Add signal to first channel\n\ndarr = da.from_array(multichannel_data, chunks=(1, 25, 50, 50))\nznimg = ZarrNii.from_darr(darr, axes_order=\"ZYX\", orientation=\"RAS\")\n\n# Segment - will use first channel for threshold calculation\n# but preserve all channel dimensions in output\nsegmented = znimg.segment_otsu()\nprint(f\"Input shape: {znimg.shape}\")       # (3, 50, 100, 100)\nprint(f\"Output shape: {segmented.shape}\")   # (3, 50, 100, 100)\n</code></pre>"},{"location":"examples/segmentation_example/#integration-with-existing-workflows","title":"Integration with Existing Workflows","text":"<p>The segmentation plugins integrate seamlessly with other ZarrNii operations:</p> <pre><code># Complete workflow: load, downsample, segment, save\nznimg = ZarrNii.from_ome_zarr(\"input_image.ome.zarr\", level=1)\n\n# Downsample for faster processing\ndownsampled = znimg.downsample(factors=2, spatial_dims=[\"z\", \"y\", \"x\"])\n\n# Apply segmentation\nsegmented = downsampled.segment_otsu(nbins=128)\n\n# Crop to region of interest\nbbox_min = (10, 20, 20)\nbbox_max = (40, 80, 80)\ncropped = segmented.crop_with_bounding_box(bbox_min, bbox_max)\n\n# Save final result\ncropped.to_ome_zarr(\"processed_segmentation.ome.zarr\")\n</code></pre>"},{"location":"examples/transformations/","title":"Transformations","text":"<p>This section covers spatial transformations in ZarrNii, including affine transforms and displacement fields.</p>"},{"location":"examples/transformations/#overview","title":"Overview","text":"<p>ZarrNii provides support for spatial transformations through the <code>AffineTransform</code> and <code>DisplacementTransform</code> classes. These can be used to apply transformations derived from performing ANTS registration on downsampled (e.g. level &gt; 3)  images, then applied to higher resolution (e.g. level &lt; 3) images. ZarrNii performs these computations in a block-wise manner using Dask, upsampling the displacement field for a block before applying it. You can also provide a sequence of transforms to perform composition of transformations to resample in a single step.</p>"},{"location":"examples/transformations/#affine-transformations","title":"Affine Transformations","text":""},{"location":"examples/transformations/#creating-affine-transformations","title":"Creating Affine Transformations","text":"<pre><code>from zarrnii import ZarrNii\nfrom zarrnii.transform import AffineTransform\nimport numpy as np\n\n# Load a dataset\nznimg = ZarrNii.from_nifti(\"path/to/image.nii\")\n\n# Create identity transformation\nidentity_transform = AffineTransform.identity()\n\n# Create transformation from a matrix\nmatrix = np.array([\n    [2.0, 0.0, 0.0, 10.0],  # Scale X by 2, translate by 10\n    [0.0, 2.0, 0.0, -5.0],  # Scale Y by 2, translate by -5\n    [0.0, 0.0, 1.0, 0.0],   # No change in Z\n    [0.0, 0.0, 0.0, 1.0]    # Homogeneous coordinates\n])\ntransform = AffineTransform.from_array(matrix)\n\n# Load transformation from text file\ntransform_from_file = AffineTransform.from_txt(\"transform.txt\")\n</code></pre>"},{"location":"examples/transformations/#applying-transformations","title":"Applying Transformations","text":"<pre><code># Apply transformation (requires reference image)\nref_znimg = ZarrNii.from_nifti(\"path/to/reference.nii\")\ntransformed = znimg.apply_transform(transform, ref_znimg=ref_znimg)\n</code></pre>"},{"location":"examples/transformations/#working-with-displacement-transformations","title":"Working with Displacement Transformations","text":"<pre><code>from zarrnii.transform import DisplacementTransform\n\n# Load displacement field from NIfTI file\ndisp_transform = DisplacementTransform.from_nifti(\"displacement_field.nii\")\n\n# Apply displacement transformation\ndeformed = znimg.apply_transform(disp_transform, ref_znimg=ref_znimg)\n</code></pre>"},{"location":"examples/transformations/#multiple-transformations","title":"Multiple Transformations","text":"<pre><code># Apply multiple transformations in sequence\n# Each transformation is applied sequentially\nresult = znimg.apply_transform(transform1, transform2, ref_znimg=ref_znimg)\n</code></pre>"},{"location":"examples/transformations/#coordinate-transformations","title":"Coordinate Transformations","text":"<pre><code># Transform coordinates using the matrix multiplication operator\nvoxel_coords = np.array([50, 60, 30])\nras_coords = transform @ voxel_coords\n\n# Transform multiple points\npoints = np.array([[50, 60, 30], [100, 120, 60]]).T  # 3xN array\ntransformed_points = transform @ points\n</code></pre>"},{"location":"examples/transformations/#inverting-transformations","title":"Inverting Transformations","text":"<pre><code># Get the inverse of a transformation\ninverse_transform = transform.invert()\n\n# Apply inverse transformation\nrestored = transformed_znimg.apply_transform(inverse_transform, ref_znimg=znimg)\n</code></pre>"},{"location":"examples/transformations/#coordinate-system-handling","title":"Coordinate System Handling","text":"<pre><code># Update transformation for different orientations\nupdated_transform = transform.update_for_orientation(\"RPI\", \"RAS\")\n</code></pre>"},{"location":"examples/transformations/#best-practices","title":"Best Practices","text":"<ol> <li>Use ref_znimg parameter: Always provide a reference image when applying transformations</li> <li>Consider coordinate systems: Be aware of voxel vs RAS coordinate conventions  </li> <li>Memory efficiency: Use lazy evaluation with Dask arrays for large datasets</li> <li>Transformation order: Remember that multiple transforms are applied sequentially</li> </ol>"},{"location":"examples/transformations/#see-also","title":"See Also","text":"<ul> <li>API Reference for detailed method documentation</li> <li>Downsampling and Upsampling for resolution change operations</li> <li>Working with Zarr and NIfTI for basic format operations</li> </ul>"},{"location":"examples/zarr_nifti/","title":"Examples: Working with Zarr and NIfTI","text":"<p>This section provides practical workflows for using ZarrNii with OME-Zarr and NIfTI datasets.</p>"},{"location":"examples/zarr_nifti/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Loading Datasets<ul> <li>From OME-Zarr</li> <li>From NIfTI</li> </ul> </li> <li>Performing Transformations<ul> <li>Downsampling</li> <li>Cropping</li> <li>Combining Affine Transformations</li> </ul> </li> <li>Saving Results<ul> <li>To OME-Zarr</li> <li>To NIfTI</li> </ul> </li> <li>Advanced Example: Full Workflow</li> </ol>"},{"location":"examples/zarr_nifti/#loading-datasets","title":"Loading Datasets","text":""},{"location":"examples/zarr_nifti/#from-ome-zarr","title":"From OME-Zarr","text":"<p>Load a dataset from an OME-Zarr file and inspect its metadata:</p> <pre><code>from zarrnii import ZarrNii\n\n# Load OME-Zarr dataset\nznimg = ZarrNii.from_ome_zarr(\"path/to/dataset.zarr\")\n\n# Inspect data\nprint(\"Shape:\", znimg.darr.shape)\nprint(\"Affine matrix:\\n\", znimg.affine.matrix)\n</code></pre>"},{"location":"examples/zarr_nifti/#from-nifti","title":"From NIfTI","text":"<p>Load a NIfTI dataset and inspect its attributes:</p> <pre><code># Load NIfTI dataset\nznimg = ZarrNii.from_nifti(\"path/to/dataset.nii\")\n\n# Inspect data\nprint(\"Shape:\", znimg.darr.shape)\nprint(\"Affine matrix:\\n\", znimg.affine.matrix)\n</code></pre>"},{"location":"examples/zarr_nifti/#performing-transformations","title":"Performing Transformations","text":""},{"location":"examples/zarr_nifti/#downsampling","title":"Downsampling","text":"<p>Reduce the resolution of the dataset using the <code>downsample</code> method:</p> <pre><code># Downsample by level\ndownsampled = znimg.downsample(level=2)\nprint(\"Downsampled shape:\", downsampled.darr.shape)\n</code></pre>"},{"location":"examples/zarr_nifti/#cropping","title":"Cropping","text":"<p>Extract a specific region from the dataset using bounding boxes:</p>"},{"location":"examples/zarr_nifti/#voxel-space","title":"Voxel Space:","text":"<pre><code>cropped = znimg.crop((10, 10, 10), (50, 50, 50))\nprint(\"Cropped shape:\", cropped.darr.shape)\n</code></pre>"},{"location":"examples/zarr_nifti/#with-ras-coordinates","title":"With RAS Coordinates:","text":"<pre><code># Note: crop_with_bounding_box is a legacy method that still supports RAS coords\ncropped_ras = znimg.crop_with_bounding_box(\n    (-20, -20, -20), (20, 20, 20), ras_coords=True\n)\nprint(\"Cropped shape:\", cropped_ras.darr.shape)\n</code></pre>"},{"location":"examples/zarr_nifti/#combining-affine-transformations","title":"Combining Affine Transformations","text":"<p>Apply multiple transformations to the dataset in sequence:</p> <pre><code>from zarrnii.transform import AffineTransform\nimport numpy as np\n\n# Define transformations using matrices\nscale_matrix = np.array([\n    [2.0, 0.0, 0.0, 0.0],\n    [0.0, 2.0, 0.0, 0.0], \n    [0.0, 0.0, 1.0, 0.0],\n    [0.0, 0.0, 0.0, 1.0]\n])\nscale = AffineTransform.from_array(scale_matrix)\n\ntranslate_matrix = np.array([\n    [1.0, 0.0, 0.0, 10.0],\n    [0.0, 1.0, 0.0, -5.0],\n    [0.0, 0.0, 1.0, 0.0],\n    [0.0, 0.0, 0.0, 1.0]\n])\ntranslate = AffineTransform.from_array(translate_matrix)\n\n# Apply transformations\ntransformed = znimg.apply_transform(scale, translate, ref_znimg=znimg)\nprint(\"Transformed affine matrix:\\n\", transformed.affine.matrix)\n</code></pre>"},{"location":"examples/zarr_nifti/#saving-results","title":"Saving Results","text":""},{"location":"examples/zarr_nifti/#to-ome-zarr","title":"To OME-Zarr","text":"<p>Save the dataset to OME-Zarr format:</p> <pre><code>znimg.to_ome_zarr(\"output.zarr\", max_layer=3, scaling_method=\"local_mean\")\n</code></pre>"},{"location":"examples/zarr_nifti/#to-nifti","title":"To NIfTI","text":"<p>Save the dataset to NIfTI format:</p> <pre><code>znimg.to_nifti(\"output.nii\")\n</code></pre>"},{"location":"examples/zarr_nifti/#advanced-example-full-workflow","title":"Advanced Example: Full Workflow","text":"<p>Combine multiple operations in a single workflow:</p> <pre><code>from zarrnii import ZarrNii\nfrom zarrnii.transform import AffineTransform\nimport numpy as np\n\n# Load an OME-Zarr dataset\nznimg = ZarrNii.from_ome_zarr(\"path/to/dataset.zarr\")\n\n# Crop the dataset\ncropped = znimg.crop((10, 10, 10), (100, 100, 100))\n\n# Downsample the dataset\ndownsampled = cropped.downsample(level=2)\n\n# Apply an affine transformation\nscale_matrix = np.array([\n    [1.5, 0.0, 0.0, 0.0],\n    [0.0, 1.5, 0.0, 0.0],\n    [0.0, 0.0, 1.0, 0.0],\n    [0.0, 0.0, 0.0, 1.0]\n])\nscale = AffineTransform.from_array(scale_matrix)\ntransformed = downsampled.apply_transform(scale, ref_znimg=downsampled)\n\n# Save the result as a NIfTI file\ntransformed.to_nifti(\"final_output.nii\")\n</code></pre>"},{"location":"examples/zarr_nifti/#summary","title":"Summary","text":"<p>In this section, you learned how to: - Load datasets from OME-Zarr and NIfTI formats. - Perform transformations like downsampling, cropping, and affine transformations. - Save results back to OME-Zarr or NIfTI.</p> <p>Next: - Explore the API Reference for in-depth details about ZarrNii's classes and methods. - Check the FAQ for answers to common questions.</p>"},{"location":"walkthrough/advanced_use_cases/","title":"Walkthrough: Advanced Use Cases","text":"<p>This guide explores advanced workflows with ZarrNii, including metadata preservation, handling multiscale OME-Zarr pyramids, and combining multiple transformations.</p>"},{"location":"walkthrough/advanced_use_cases/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Preserving Metadata</li> <li>Working with Multiscale Pyramids</li> <li>Combining Transformations</li> <li>Handling Large Datasets</li> </ol>"},{"location":"walkthrough/advanced_use_cases/#preserving-metadata","title":"Preserving Metadata","text":"<p>ZarrNii is designed to handle and preserve metadata when converting between formats or applying transformations.</p>"},{"location":"walkthrough/advanced_use_cases/#accessing-metadata","title":"Accessing Metadata","text":"<p>OME-Zarr metadata is automatically extracted and stored in the <code>axes</code>, <code>coordinate_transformations</code>, and <code>omero</code> attributes of a <code>ZarrNii</code> instance.</p> <pre><code>znimg = ZarrNii.from_ome_zarr(\"path/to/dataset.zarr\")\n\n# Access axes metadata\nprint(\"Axes metadata:\", znimg.axes)\n\n# Access coordinate transformations\nprint(\"Coordinate transformations:\", znimg.coordinate_transformations)\n\n# Access Omero metadata\nprint(\"Omero metadata:\", znimg.omero)\n</code></pre>"},{"location":"walkthrough/advanced_use_cases/#preserving-metadata-during-transformations","title":"Preserving Metadata During Transformations","text":"<p>When you perform transformations like cropping or downsampling, ZarrNii ensures metadata remains consistent.</p> <pre><code>cropped = znimg.crop((10, 10, 10), (50, 50, 50))\nprint(\"Updated metadata:\", cropped.coordinate_transformations)\n</code></pre>"},{"location":"walkthrough/advanced_use_cases/#working-with-multiscale-pyramids","title":"Working with Multiscale Pyramids","text":"<p>OME-Zarr datasets often include multiscale pyramids, where each level represents a progressively downsampled version of the data.</p>"},{"location":"walkthrough/advanced_use_cases/#loading-a-specific-level","title":"Loading a Specific Level","text":"<p>You can load a specific pyramid level using the <code>level</code> argument in <code>from_ome_zarr</code>:</p> <pre><code>znimg = ZarrNii.from_ome_zarr(\"path/to/dataset.zarr\", level=2)\nprint(\"Loaded shape:\", znimg.darr.shape)\n</code></pre>"},{"location":"walkthrough/advanced_use_cases/#handling-custom-downsampling","title":"Handling Custom Downsampling","text":"<p>If the desired level isn't available in the pyramid, ZarrNii computes additional downsampling lazily:</p> <pre><code>level, do_downsample, ds_kwargs = ZarrNii.get_level_and_downsampling_kwargs(\n    \"path/to/dataset.zarr\", level=5\n)\nif do_downsample:\n    znimg = znimg.downsample(**ds_kwargs)\n</code></pre>"},{"location":"walkthrough/advanced_use_cases/#combining-transformations","title":"Combining Transformations","text":"<p>ZarrNii allows you to chain multiple transformations into a single workflow. This is useful when applying affine transformations, interpolations, or warping.</p>"},{"location":"walkthrough/advanced_use_cases/#chaining-affine-transformations","title":"Chaining Affine Transformations","text":"<pre><code>from zarrnii.transform import AffineTransform\nimport numpy as np\n\n# Create transformations using matrices\nscaling_matrix = np.array([\n    [2.0, 0.0, 0.0, 0.0],\n    [0.0, 2.0, 0.0, 0.0], \n    [0.0, 0.0, 1.0, 0.0],\n    [0.0, 0.0, 0.0, 1.0]\n])\nscaling = AffineTransform.from_array(scaling_matrix)\n\ntranslation_matrix = np.array([\n    [1.0, 0.0, 0.0, 10.0],\n    [0.0, 1.0, 0.0, -5.0],\n    [0.0, 0.0, 1.0, 0.0],\n    [0.0, 0.0, 0.0, 1.0]\n])\ntranslation = AffineTransform.from_array(translation_matrix)\n\n# Apply multiple transformations sequentially\ncombined = znimg.apply_transform(scaling, translation, ref_znimg=znimg)\nprint(\"New affine matrix:\\n\", combined.affine.matrix)\n</code></pre>"},{"location":"walkthrough/advanced_use_cases/#handling-large-datasets","title":"Handling Large Datasets","text":"<p>ZarrNii leverages Dask to handle datasets that don't fit into memory.</p>"},{"location":"walkthrough/advanced_use_cases/#optimizing-chunking","title":"Optimizing Chunking","text":"<p>Ensure the dataset is chunked appropriately for operations like downsampling or interpolation:</p> <pre><code># Rechunk for efficient processing\nrechunked = znimg.darr.rechunk((1, 64, 64, 64))\nprint(\"Rechunked shape:\", rechunked.shape)\n</code></pre>"},{"location":"walkthrough/advanced_use_cases/#lazy-evaluation","title":"Lazy Evaluation","text":"<p>Most transformations in ZarrNii are lazy, meaning computations are only triggered when necessary. Use <code>.compute()</code> to materialize results.</p> <pre><code># Trigger computation\ncropped = znimg.crop((10, 10, 10), (50, 50, 50))\ncropped.darr.compute()\n</code></pre>"},{"location":"walkthrough/advanced_use_cases/#summary","title":"Summary","text":"<p>This guide covered: - Preserving metadata across transformations and format conversions. - Working with multiscale pyramids in OME-Zarr. - Combining transformations for complex workflows. - Handling large datasets efficiently with Dask.</p> <p>Next, explore: - Examples: Detailed workflows and practical use cases. - API Reference: Technical details for ZarrNii classes and methods.</p>"},{"location":"walkthrough/basic_tasks/","title":"Walkthrough: Basic Tasks","text":"<p>This guide covers the most common tasks you'll perform with ZarrNii, including reading data, performing transformations, and saving results.</p>"},{"location":"walkthrough/basic_tasks/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Reading Data<ul> <li>From OME-Zarr</li> <li>From NIfTI</li> <li>Working with 5D Data</li> </ul> </li> <li>Transforming Data<ul> <li>Cropping</li> <li>Downsampling</li> <li>Upsampling</li> <li>Applying Affine Transformations</li> </ul> </li> <li>Saving Data<ul> <li>To NIfTI</li> <li>To OME-Zarr</li> </ul> </li> </ol>"},{"location":"walkthrough/basic_tasks/#reading-data","title":"Reading Data","text":""},{"location":"walkthrough/basic_tasks/#from-ome-zarr","title":"From OME-Zarr","text":"<p>Load a dataset from an OME-Zarr file using <code>from_ome_zarr</code>:</p> <pre><code>from zarrnii import ZarrNii\n\n# Load the dataset\nznimg = ZarrNii.from_ome_zarr(\"path/to/dataset.ome.zarr\")\n\n# Inspect the data\nprint(\"Data shape:\", znimg.darr.shape)\nprint(\"Affine matrix:\\n\", znimg.affine.matrix)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#automatic-anisotropy-correction","title":"Automatic Anisotropy Correction","text":"<p>For datasets with anisotropic voxels (common in lightsheet microscopy), you can automatically downsample to create more isotropic voxels:</p> <pre><code># Load with near-isotropic downsampling\nznimg_isotropic = ZarrNii.from_ome_zarr(\n    \"path/to/anisotropic_data.ome.zarr\", \n    downsample_near_isotropic=True\n)\n\nprint(\"Isotropic data shape:\", znimg_isotropic.darr.shape)\nprint(\"Isotropic scales:\", znimg_isotropic.scale)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#from-nifti","title":"From NIfTI","text":"<p>Load a dataset from a NIfTI file using <code>from_nifti</code>:</p> <pre><code># Load the dataset\nznimg = ZarrNii.from_nifti(\"path/to/dataset.nii\")\n\n# Inspect the data\nprint(\"Data shape:\", znimg.darr.shape)\nprint(\"Affine matrix:\\n\", znimg.affine.matrix)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#working-with-5d-data","title":"Working with 5D Data","text":"<p>ZarrNii supports 5D images with time and channel dimensions (T,C,Z,Y,X). You can select specific timepoints and channels during loading or after loading.</p>"},{"location":"walkthrough/basic_tasks/#loading-with-timepoint-selection","title":"Loading with Timepoint Selection:","text":"<pre><code># Load specific timepoints\nznimg_time = ZarrNii.from_ome_zarr(\"timeseries.zarr\", timepoints=[0, 2, 4])\nprint(\"Timepoint subset shape:\", znimg_time.darr.shape)\n\n# Load specific channels by index\nznimg_channels = ZarrNii.from_ome_zarr(\"multichannel.zarr\", channels=[0, 2])\n\n# Load specific channels by label\nznimg_labels = ZarrNii.from_ome_zarr(\"labeled.zarr\", channel_labels=[\"DAPI\", \"GFP\"])\n\n# Combine timepoint and channel selection\nznimg_subset = ZarrNii.from_ome_zarr(\"data.zarr\", timepoints=[1, 3], channels=[0])\n</code></pre>"},{"location":"walkthrough/basic_tasks/#post-loading-selection","title":"Post-loading Selection:","text":"<pre><code># Load full dataset first\nznimg = ZarrNii.from_ome_zarr(\"timeseries.zarr\")\n\n# Select timepoints after loading\nselected_time = znimg.select_timepoints([0, 2])\n\n# Select channels after loading\nselected_channels = znimg.select_channels([1, 2])\n\n# Chain selections\nsubset = znimg.select_timepoints([0, 1]).select_channels([0])\n</code></pre>"},{"location":"walkthrough/basic_tasks/#transforming-data","title":"Transforming Data","text":""},{"location":"walkthrough/basic_tasks/#cropping","title":"Cropping","text":"<p>Crop the dataset to a specific bounding box. You can define the bounding box in either voxel space or RAS (real-world) coordinates. For 5D data, cropping operates only on spatial dimensions, preserving time and channel dimensions.</p>"},{"location":"walkthrough/basic_tasks/#voxel-space-cropping","title":"Voxel Space Cropping:","text":"<pre><code># Preferred method using crop()\ncropped = znimg.crop((10, 10, 10), (50, 50, 50))\nprint(\"Cropped shape:\", cropped.darr.shape)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#ras-space-cropping","title":"RAS Space Cropping:","text":"<pre><code># Use legacy method for RAS coordinates\ncropped_ras = znimg.crop_with_bounding_box(\n    (-20, -20, -20), (20, 20, 20), ras_coords=True\n)\nprint(\"Cropped shape:\", cropped_ras.darr.shape)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#downsampling","title":"Downsampling","text":"<p>Downsample the dataset to reduce its resolution. You can specify either a downsampling level or individual scaling factors for each axis. For 5D data, downsampling operates only on spatial dimensions, preserving time and channel dimensions.</p>"},{"location":"walkthrough/basic_tasks/#by-level","title":"By Level:","text":"<pre><code>downsampled = znimg.downsample(level=2)\nprint(\"Downsampled shape:\", downsampled.darr.shape)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#by-scaling-factors","title":"By Scaling Factors:","text":"<pre><code>downsampled_manual = znimg.downsample(along_x=2, along_y=2, along_z=1)\nprint(\"Downsampled shape:\", downsampled_manual.darr.shape)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#upsampling","title":"Upsampling","text":"<p>Increase the resolution of the dataset by upsampling.</p>"},{"location":"walkthrough/basic_tasks/#by-scaling-factors_1","title":"By Scaling Factors:","text":"<pre><code>upsampled = znimg.upsample(along_x=2, along_y=2, along_z=2)\nprint(\"Upsampled shape:\", upsampled.darr.shape)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#to-target-shape","title":"To Target Shape:","text":"<pre><code>upsampled_target = znimg.upsample(to_shape=(1, 256, 256, 256))\nprint(\"Upsampled shape:\", upsampled_target.darr.shape)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#applying-affine-transformations","title":"Applying Affine Transformations","text":"<p>Apply a custom affine transformation to the dataset.</p> <pre><code>from zarrnii.transform import AffineTransform\nimport numpy as np\n\n# Define a scaling transformation using a matrix\nscaling_matrix = np.array([\n    [2.0, 0.0, 0.0, 0.0],\n    [0.0, 2.0, 0.0, 0.0], \n    [0.0, 0.0, 1.0, 0.0],\n    [0.0, 0.0, 0.0, 1.0]\n])\nscaling_transform = AffineTransform.from_array(scaling_matrix)\n\n# Apply the transformation  \ntransformed = znimg.apply_transform(scaling_transform, ref_znimg=znimg)\nprint(\"Transformed affine matrix:\\n\", transformed.affine.matrix)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#saving-data","title":"Saving Data","text":""},{"location":"walkthrough/basic_tasks/#to-nifti","title":"To NIfTI","text":"<p>Save the dataset as a NIfTI file using <code>to_nifti</code>:</p> <pre><code>znimg.to_nifti(\"output_dataset.nii\")\n</code></pre>"},{"location":"walkthrough/basic_tasks/#to-ome-zarr","title":"To OME-Zarr","text":"<p>Save the dataset as an OME-Zarr file using <code>to_ome_zarr</code>:</p> <pre><code>znimg.to_ome_zarr(\"output_dataset.ome.zarr\")\n</code></pre> <p>You can also save additional metadata during the process:</p> <pre><code>znimg.to_ome_zarr(\n    \"output_dataset.ome.zarr\",\n    max_layer=3,\n    scaling_method=\"local_mean\"\n)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#summary","title":"Summary","text":"<p>This guide covered the essential operations you can perform with ZarrNii: - Reading datasets from OME-Zarr and NIfTI formats. - Transforming datasets through cropping, downsampling, upsampling, and affine transformations. - Saving datasets back to either format.</p> <p>Next, explore Advanced Use Cases or dive into the API Reference for detailed technical documentation.</p>"},{"location":"walkthrough/getting_started/","title":"Getting Started","text":"<p>This guide helps you set up ZarrNii and get started with its basic functionality. By the end of this guide, you'll be able to read OME-Zarr and NIfTI datasets, perform basic transformations, and save your results.</p>"},{"location":"walkthrough/getting_started/#installation","title":"Installation","text":"<p>ZarrNii requires Python 3.11 or later. Install it using uv, a modern, fast Python package installer and project manager.</p>"},{"location":"walkthrough/getting_started/#1-clone-the-repository","title":"1. Clone the Repository","text":"<p>If you're using the source code, clone the ZarrNii repository:</p> <pre><code>git clone https://github.com/khanlab/zarrnii.git\ncd zarrnii\n</code></pre>"},{"location":"walkthrough/getting_started/#2-install-with-uv","title":"2. Install with uv","text":"<p>Run the following command to install the library and its dependencies:</p> <pre><code>uv sync --dev\n</code></pre> <p>If you don't use uv, install ZarrNii and its dependencies using <code>pip</code>:</p> <pre><code>pip install zarrnii\n</code></pre>"},{"location":"walkthrough/getting_started/#3-optional-dependencies","title":"3. Optional Dependencies","text":"<p>For additional format support, install optional dependencies:</p> <pre><code># For Imaris (.ims) file support\npip install zarrnii[imaris]\n</code></pre>"},{"location":"walkthrough/getting_started/#prerequisites","title":"Prerequisites","text":"<p>Before using ZarrNii, ensure you have: - OME-Zarr datasets: Multidimensional images in Zarr format. - NIfTI datasets: Neuroimaging data in <code>.nii</code> or <code>.nii.gz</code> format. - Imaris datasets (optional): Microscopy data in <code>.ims</code> format (requires <code>zarrnii[imaris]</code>).</p>"},{"location":"walkthrough/getting_started/#basic-usage","title":"Basic Usage","text":""},{"location":"walkthrough/getting_started/#1-reading-data","title":"1. Reading Data","text":"<p>You can load an OME-Zarr or NIfTI dataset into a <code>ZarrNii</code> object.</p>"},{"location":"walkthrough/getting_started/#from-ome-zarr","title":"From OME-Zarr:","text":"<pre><code>from zarrnii import ZarrNii\n\n# Load OME-Zarr\nznimg = ZarrNii.from_ome_zarr(\"path/to/dataset.ome.zarr\")\n\nprint(\"Data shape:\", znimg.darr.shape)\nprint(\"Affine matrix:\\n\", znimg.affine.matrix)\n\n# For anisotropic data, automatically create more isotropic voxels\nznimg_isotropic = ZarrNii.from_ome_zarr(\n    \"path/to/dataset.ome.zarr\", \n    downsample_near_isotropic=True\n)\n# Load specific timepoints and channels from 5D data\nznimg_5d = ZarrNii.from_ome_zarr(\"timeseries.zarr\", timepoints=[0, 2], channels=[1])\nprint(\"5D subset shape:\", znimg_5d.darr.shape)\n</code></pre>"},{"location":"walkthrough/getting_started/#from-nifti","title":"From NIfTI:","text":"<pre><code># Load NIfTI (supports 3D, 4D, and 5D data)\nznimg = ZarrNii.from_nifti(\"path/to/dataset.nii\")\n\nprint(\"Data shape:\", znimg.darr.shape)\nprint(\"Affine matrix:\\n\", znimg.affine.matrix)\n</code></pre>"},{"location":"walkthrough/getting_started/#from-imaris-requires-zarrniiimaris","title":"From Imaris (requires <code>zarrnii[imaris]</code>):","text":"<pre><code># Load Imaris\nznimg = ZarrNii.from_imaris(\"path/to/dataset.ims\")\n\nprint(\"Data shape:\", znimg.darr.shape)\nprint(\"Affine matrix:\\n\", znimg.affine.matrix)\n</code></pre>"},{"location":"walkthrough/getting_started/#2-working-with-5d-data","title":"2. Working with 5D Data","text":"<p>ZarrNii supports 5D images with time and channel dimensions (T,C,Z,Y,X format). You can select specific timepoints and channels either during loading or after loading.</p>"},{"location":"walkthrough/getting_started/#loading-with-selection","title":"Loading with Selection:","text":"<pre><code># Load specific timepoints\nznimg_time = ZarrNii.from_ome_zarr(\"timeseries.zarr\", timepoints=[0, 2, 4])\n\n# Load specific channels  \nznimg_channels = ZarrNii.from_ome_zarr(\"multichannel.zarr\", channels=[0, 2])\n\n# Load specific channels by label\nznimg_labels = ZarrNii.from_ome_zarr(\"labeled.zarr\", channel_labels=[\"DAPI\", \"GFP\"])\n\n# Combine timepoint and channel selection\nznimg_subset = ZarrNii.from_ome_zarr(\"data.zarr\", timepoints=[1, 3], channels=[0])\n</code></pre>"},{"location":"walkthrough/getting_started/#post-loading-selection","title":"Post-loading Selection:","text":"<pre><code># Load full dataset first\nznimg = ZarrNii.from_ome_zarr(\"timeseries.zarr\")\n\n# Select timepoints after loading\nselected_time = znimg.select_timepoints([0, 2])\n\n# Select channels after loading\nselected_channels = znimg.select_channels([1, 2])\n\n# Chain selections\nsubset = znimg.select_timepoints([0, 1]).select_channels([0])\n</code></pre>"},{"location":"walkthrough/getting_started/#3-performing-transformations","title":"3. Performing Transformations","text":"<p>ZarrNii supports various transformations, such as cropping, downsampling, and upsampling. When working with 5D data, spatial transformations preserve the time and channel dimensions.</p>"},{"location":"walkthrough/getting_started/#cropping","title":"Cropping:","text":"<p>Crop a region from the dataset using voxel coordinates:</p> <pre><code>cropped = znimg.crop((10, 10, 10), (50, 50, 50))\nprint(\"Cropped shape:\", cropped.darr.shape)\n</code></pre>"},{"location":"walkthrough/getting_started/#downsampling","title":"Downsampling:","text":"<p>Reduce the resolution of your dataset:</p> <pre><code>downsampled = znimg.downsample(level=2)\nprint(\"Downsampled shape:\", downsampled.darr.shape)\n\n# For 5D data: (3, 2, 16, 32, 32) -&gt; (3, 2, 8, 16, 16)\n# Time and channel dimensions are preserved\n</code></pre>"},{"location":"walkthrough/getting_started/#upsampling","title":"Upsampling:","text":"<p>Increase the resolution of your dataset:</p> <pre><code>upsampled = znimg.upsample(along_x=2, along_y=2, along_z=2)\nprint(\"Upsampled shape:\", upsampled.darr.shape)\n</code></pre>"},{"location":"walkthrough/getting_started/#3-image-segmentation","title":"3. Image Segmentation","text":"<p>ZarrNii includes a plugin architecture for image segmentation algorithms. You can apply segmentation to identify regions of interest in your data.</p>"},{"location":"walkthrough/getting_started/#otsu-thresholding","title":"Otsu Thresholding:","text":"<p>Apply automatic Otsu thresholding for binary segmentation:</p> <pre><code>segmented = znimg.segment_otsu(nbins=256)\nprint(\"Segmented shape:\", segmented.darr.shape)\nprint(\"Unique values:\", segmented.darr.compute().unique())  # Should show [0, 1]\n</code></pre>"},{"location":"walkthrough/getting_started/#using-plugin-interface","title":"Using Plugin Interface:","text":"<p>You can also use the generic plugin interface:</p> <pre><code>from zarrnii import OtsuSegmentation\n\nplugin = OtsuSegmentation(nbins=128)\nsegmented = znimg.segment(plugin)\n</code></pre>"},{"location":"walkthrough/getting_started/#custom-chunk-processing","title":"Custom Chunk Processing:","text":"<p>For large datasets, you can control memory usage with custom chunk sizes:</p> <pre><code>segmented = znimg.segment_otsu(chunk_size=(1, 10, 50, 50))\n</code></pre>"},{"location":"walkthrough/getting_started/#4-saving-data","title":"4. Saving Data","text":"<p>ZarrNii makes it easy to save your datasets in both OME-Zarr and NIfTI formats.</p>"},{"location":"walkthrough/getting_started/#to-nifti","title":"To NIfTI:","text":"<p>Save the dataset as a <code>.nii</code> file:</p> <pre><code>znimg.to_nifti(\"output_dataset.nii\")\n</code></pre>"},{"location":"walkthrough/getting_started/#to-ome-zarr","title":"To OME-Zarr:","text":"<p>Save the dataset back to OME-Zarr format:</p> <pre><code>znimg.to_ome_zarr(\"output_dataset.ome.zarr\")\n</code></pre>"},{"location":"walkthrough/getting_started/#example-workflow","title":"Example Workflow","text":"<p>Here\u2019s a full workflow from loading an OME-Zarr dataset to saving a downsampled version as NIfTI:</p> <pre><code>from zarrnii import ZarrNii\n\n# Load a 5D OME-Zarr dataset with specific timepoints and channels\nznimg = ZarrNii.from_ome_zarr(\"path/to/timeseries.zarr\", \n                              timepoints=[0, 2, 4], \n                              channels=[0, 1])\n\n# Perform transformations\ncropped = znimg.crop((10, 10, 10), (100, 100, 100))\ndownsampled = cropped.downsample(level=2)\n\n# Save the result as a NIfTI file\n\ndownsampled.to_nifti(\"processed_timeseries.nii\")\n\n# Or save as OME-Zarr with metadata preservation\ndownsampled.to_ome_zarr(\"processed_timeseries.ome.zarr\")\n\n# Save as TIFF stack for compatibility with napari plugins like cellseg3d\n# (Crop first for better performance with large datasets)\ncropped_small = znimg.crop((10, 10, 10), (50, 50, 50))\ncropped_small.to_tiff_stack(\"tiff_stack/slice_{z:03d}.tif\")\n\n\n# Apply segmentation to the original image\nsegmented = znimg.segment_otsu(nbins=256)\nsegmented.to_nifti(\"segmented_output.nii\")\n\n</code></pre>"},{"location":"walkthrough/getting_started/#whats-next","title":"What\u2019s Next?","text":"<ul> <li>Walkthrough: Basic Tasks: Learn more about common workflows like cropping, interpolation, and combining transformations.</li> <li>Segmentation Plugin Examples: Learn how to use and create segmentation plugins.</li> <li>API Reference: Explore the detailed API for ZarrNii.</li> </ul>"},{"location":"walkthrough/overview/","title":"Walkthrough: Overview","text":"<p>This page provides an overview of the core concepts behind ZarrNii. It\u2019s the starting point for understanding how to work with OME-Zarr, NIfTI, and ZarrNii\u2019s transformation tools.</p>"},{"location":"walkthrough/overview/#core-concepts","title":"Core Concepts","text":""},{"location":"walkthrough/overview/#1-zarr-and-ome-zarr","title":"1. Zarr and OME-Zarr","text":"<ul> <li>Zarr is a format for chunked, compressed N-dimensional arrays.</li> <li>OME-Zarr extends Zarr with metadata for multidimensional microscopy images, supporting axes definitions and multiscale pyramids.</li> </ul>"},{"location":"walkthrough/overview/#key-features-of-ome-zarr","title":"Key Features of OME-Zarr:","text":"<ul> <li>Axes Metadata: Defines spatial dimensions (e.g., <code>x</code>, <code>y</code>, <code>z</code>).</li> <li>Multiscale Pyramids: Stores image resolutions at multiple scales.</li> <li>Annotations: Includes OME metadata for visualization and analysis.</li> </ul>"},{"location":"walkthrough/overview/#2-nifti","title":"2. NIfTI","text":"<ul> <li>NIfTI is a neuroimaging file format, commonly used for MRI and fMRI data.</li> <li>It supports spatial metadata, such as voxel sizes and affine transformations, for anatomical alignment.</li> </ul>"},{"location":"walkthrough/overview/#3-zarrnii","title":"3. ZarrNii","text":"<ul> <li>ZarrNii provides tools to bridge these formats while preserving spatial metadata and enabling transformations.</li> </ul>"},{"location":"walkthrough/overview/#main-features","title":"Main Features:","text":"<ul> <li>Read and write OME-Zarr and NIfTI formats.</li> <li>Apply transformations like cropping, downsampling, and interpolation.</li> <li>Apply segmentation algorithms through an extensible plugin system.</li> <li>Convert between ZYX (OME-Zarr) and XYZ (NIfTI) axes orders.</li> </ul>"},{"location":"walkthrough/overview/#data-model","title":"Data Model","text":"<p>ZarrNii wraps datasets using the <code>ZarrNii</code> class, which has the following attributes:</p> <ul> <li><code>darr</code>: The dask array containing image data.</li> <li><code>affine</code>: An affine transformation matrix for spatial alignment.</li> <li><code>axes_order</code>: Specifies the data layout (<code>ZYX</code> or <code>XYZ</code>).</li> <li>OME-Zarr Metadata:</li> <li><code>axes</code>: Defines the dimensions and units.</li> <li><code>coordinate_transformations</code>: Lists scaling and translation transformations.</li> <li><code>omero</code>: Contains channel and visualization metadata.</li> </ul>"},{"location":"walkthrough/overview/#example-workflow","title":"Example Workflow","text":"<p>Here\u2019s a high-level example workflow using ZarrNii:</p> <ol> <li> <p>Read Data:    <code>python    from zarrnii import ZarrNii    znimg = ZarrNii.from_ome_zarr(\"path/to/dataset.ome.zarr\")</code></p> </li> <li> <p>Apply Transformations:    <code>python    znimg_downsampled = znimg.downsample(level=2)    znimg_cropped = znimg_downsampled.crop((0, 0, 0), (100, 100, 100))</code></p> </li> <li> <p>Apply Segmentation:    <code>python    znimg_segmented = znimg.segment_otsu(nbins=256)</code></p> </li> <li> <p>Convert Formats:    <code>python    znimg_cropped.to_nifti(\"output.nii\")    znimg_segmented.to_nifti(\"segmented.nii\")</code></p> </li> </ol>"},{"location":"walkthrough/overview/#whats-next","title":"What\u2019s Next?","text":"<ul> <li>Getting Started: Step-by-step guide to installing and using ZarrNii.</li> <li>Basic Tasks: Learn how to read, write, and transform data.</li> </ul>"}]}