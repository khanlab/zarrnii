{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to the documentation for ZarrNii, a Python library for working with OME-Zarr, NIfTI, and Imaris formats. ZarrNii bridges the gap between these popular formats, enabling seamless data transformation, metadata preservation, and efficient processing of large biomedical images.</p>"},{"location":"#what-is-zarrnii","title":"What is ZarrNii?","text":"<p>ZarrNii is designed for researchers and engineers working with:</p> <ul> <li>OME-Zarr: A format for storing multidimensional image data, commonly used in microscopy.</li> <li>NIfTI: A standard format for neuroimaging data.</li> <li>Imaris: A microscopy file format (.ims) using HDF5 structure for 3D/4D image analysis.</li> </ul> <p>ZarrNii allows you to:</p> <ul> <li>Read and write OME-Zarr, NIfTI, and Imaris datasets.</li> <li>Work with 4D and 5D images, including time-series data (T,C,Z,Y,X).</li> <li>Perform transformations like cropping, downsampling, and interpolation.</li> <li>Select specific channels and timepoints from multidimensional datasets.</li> <li>Preserve and manipulate metadata from OME-Zarr (e.g., axes, coordinate transformations, OME annotations).</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Seamless Format Conversion: Easily convert between OME-Zarr, NIfTI, and Imaris while preserving spatial metadata.</li> <li>5D Image Support: Work with time-series data in (T,C,Z,Y,X) format with timepoint and channel selection.</li> <li>Transformations: Apply common operations like affine transformations, downsampling, and upsampling.</li> <li>Multiscale Support: Work with multiscale OME-Zarr pyramids.</li> <li>Metadata Handling: Access and modify OME-Zarr metadata like axes and transformations.</li> <li>Lazy Loading: Leverage Dask arrays for efficient processing of large datasets.</li> <li>Segmentation Plugins: Extensible plugin architecture for image segmentation algorithms.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from zarrnii import ZarrNii\n\n# Load an OME-Zarr dataset\nznimg = ZarrNii.from_ome_zarr(\"path/to/zarr_dataset.ome.zarr\")\n\n# Or load from Imaris (requires optional dependency)\n# znimg = ZarrNii.from_imaris(\"path/to/microscopy_data.ims\")\n\n# Load with specific timepoints and channels (5D support)\nznimg_subset = ZarrNii.from_ome_zarr(\"timeseries.zarr\", timepoints=[0, 2], channels=[1])\n\n# Perform a transformation (e.g., downsample)\ndownsampled_znimg = znimg.downsample(level=2)\n\n# Apply segmentation using Otsu thresholding\nsegmented_znimg = znimg.segment_otsu(nbins=256)\n\n# Save as NIfTI\ndownsampled_znimg.to_nifti(\"output_dataset.nii\")\nsegmented_znimg.to_nifti(\"segmented_dataset.nii\")\n</code></pre>"},{"location":"#learn-more","title":"Learn More","text":"<p>Explore the documentation to get started:</p> <ul> <li>Walkthrough: Overview: Understand the core concepts.</li> <li>API Reference: Dive into the technical details.</li> <li>Examples: Learn through practical examples.</li> <li>Segmentation Plugin Examples: Learn how to use and create segmentation plugins.</li> <li>FAQ: Find answers to common questions.</li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to ZarrNii will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Near-isotropic downsampling: New <code>downsample_near_isotropic</code> parameter in <code>from_ome_zarr()</code> automatically downsamples dimensions with higher resolution to create more isotropic voxels</li> <li>Comprehensive documentation with examples and API reference</li> <li>Multi-resolution OME-Zarr support with pyramid creation</li> <li>Enhanced transformation pipeline with composite operations</li> <li>Memory-efficient processing with Dask integration</li> <li>Support for multi-channel and time-series data</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Migration from Poetry to uv: Faster dependency management and builds with modern Python packaging</li> <li>Automated versioning: Version now derived from git tags using setuptools-scm</li> <li>Enhanced CI/CD: Updated workflows with trusted publishing to PyPI</li> <li>Improved performance for large dataset operations</li> <li>Enhanced metadata preservation across format conversions</li> <li>Optimized chunk sizing for better I/O performance</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>NumPy 2.0 Compatibility: Fixed deprecated np.product usage</li> <li>Documentation build issues with missing files</li> <li>Improved error handling for malformed input files</li> <li>Better memory management for large transformations</li> </ul>"},{"location":"changelog/#010-initial-development","title":"[0.1.0] - Initial Development","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Core ZarrNii class for unified OME-Zarr and NIfTI handling</li> <li>Affine and displacement transformation support</li> <li>Basic downsampling and upsampling operations</li> <li>Format conversion between OME-Zarr and NIfTI</li> <li>Spatial coordinate system management</li> <li>Integration with nibabel and zarr libraries</li> </ul>"},{"location":"changelog/#features","title":"Features","text":"<ul> <li>Lazy loading with Dask arrays</li> <li>Metadata preservation during transformations</li> <li>Multi-scale image pyramid support</li> <li>Flexible resampling and interpolation methods</li> <li>Comprehensive test suite</li> </ul>"},{"location":"changelog/#documentation","title":"Documentation","text":"<ul> <li>Getting started guide</li> <li>API reference</li> <li>Example workflows</li> <li>Installation instructions</li> </ul>"},{"location":"changelog/#development-notes","title":"Development Notes","text":"<p>This project is under active development. The API may change between versions as we refine the interface based on user feedback and use cases.</p>"},{"location":"changelog/#contributing","title":"Contributing","text":"<p>See Contributing for information about contributing to ZarrNii development.</p>"},{"location":"cli/","title":"Command Line Interface (CLI)","text":"<p>ZarrNii provides convenient command-line tools for converting between OME-Zarr and NIfTI formats. These console scripts are simple wrappers around the main ZarrNii API, making it easy to perform conversions without writing Python code.</p>"},{"location":"cli/#installation","title":"Installation","text":"<p>The CLI scripts are automatically installed when you install ZarrNii:</p> <pre><code>pip install zarrnii\n</code></pre> <p>After installation, you'll have access to two console commands: - <code>z2n</code> - Convert OME-Zarr to NIfTI or TIFF stack format - <code>n2z</code> - Convert NIfTI to OME-Zarr</p>"},{"location":"cli/#z2n-ome-zarr-to-niftitiff-conversion","title":"z2n: OME-Zarr to NIfTI/TIFF Conversion","text":"<p>The <code>z2n</code> script converts OME-Zarr datasets to NIfTI format or TIFF stack format. TIFF stack export is particularly useful for compatibility with napari plugins like cellseg3d that don't support OME-Zarr multiscale data.</p>"},{"location":"cli/#basic-usage","title":"Basic Usage","text":"<pre><code># NIfTI output\nz2n input.ome.zarr output.nii.gz\n\n# TIFF stack output (auto-detected by pattern)\nz2n input.ome.zarr output_z{z:04d}.tif\n</code></pre>"},{"location":"cli/#output-format-selection","title":"Output Format Selection","text":"<p>The output format is automatically detected based on the output filename: - <code>.nii</code> or <code>.nii.gz</code> extensions \u2192 NIfTI format - <code>.tif</code> or <code>.tiff</code> extensions with <code>{z}</code> pattern \u2192 TIFF stack format - Other extensions default to NIfTI format</p> <p>TIFF Stack Format: Each Z-slice is saved as a separate 2D TIFF file. This format is useful for: - Napari plugins that don't support OME-Zarr (e.g., cellseg3d) - Tools that expect individual image files - Visual inspection of individual slices</p> <p>\u26a0\ufe0f Performance Note: TIFF stack export loads all data into memory. Consider cropping or downsampling large datasets first.</p>"},{"location":"cli/#options","title":"Options","text":"<pre><code>z2n --help\n</code></pre> <pre><code>usage: z2n [-h] [--level LEVEL] [--channels CHANNELS] \n           [--channel-labels CHANNEL_LABELS [CHANNEL_LABELS ...]]\n           [--timepoints TIMEPOINTS] [--axes-order {ZYX,XYZ}] \n           [--orientation ORIENTATION] [--downsample-near-isotropic] \n           [--chunks CHUNKS] [--rechunk]\n           input output\n\nConvert OME-Zarr to NIfTI format\n\npositional arguments:\n  input                 Input OME-Zarr path or store\n  output                Output NIfTI file (.nii or .nii.gz)\n\noptions:\n  -h, --help            show this help message and exit\n  --level LEVEL         Pyramid level to load (0 = highest resolution) (default: 0)\n  --channels CHANNELS   Comma-separated channel indices to load (e.g., '0,2,3')\n  --channel-labels CHANNEL_LABELS [CHANNEL_LABELS ...]\n                        Channel names to load by label\n  --timepoints TIMEPOINTS\n                        Comma-separated timepoint indices to load (e.g., '0,1,2')\n  --axes-order {ZYX,XYZ}\n                        Spatial axes order for processing (default: ZYX)\n  --orientation ORIENTATION\n                        Anatomical orientation string (default: RAS)\n  --downsample-near-isotropic\n                        Apply near-isotropic downsampling\n  --chunks CHUNKS       Chunk specification for dask arrays (default: auto)\n  --rechunk             Rechunk data arrays\n</code></pre>"},{"location":"cli/#examples","title":"Examples","text":"<pre><code># Basic conversion\nz2n input.ome.zarr output.nii.gz\n\n# Convert from ZIP store with specific pyramid level\nz2n input.ome.zarr.zip output.nii.gz --level 1\n\n# Select specific channels and reorder axes\nz2n input.ome.zarr output.nii.gz --channels 0,2 --axes-order ZYX\n\n# Change orientation and apply near-isotropic downsampling\nz2n input.ome.zarr output.nii.gz --orientation LPI --downsample-near-isotropic\n\n# TIFF Stack Export\n# Save each Z-slice as a separate TIFF file for compatibility with napari plugins\nz2n input.ome.zarr output_z{z:04d}.tif --tiff-channel 0\n\n# Export to TIFF stack with custom pattern and settings\nz2n input.ome.zarr slices/brain_{z:03d}.tiff --tiff-timepoint 0 --tiff-no-compress\n\n# Export multi-channel data as multi-channel TIFFs (no channel selection)\nz2n input.ome.zarr multichannel_z{z:04d}.tif\n\n# Load specific timepoints\nz2n input.ome.zarr output.nii.gz --timepoints 0,5,10\n\n# Use channel labels instead of indices\nz2n input.ome.zarr output.nii.gz --channel-labels DAPI GFP\n</code></pre>"},{"location":"cli/#n2z-nifti-to-ome-zarr-conversion","title":"n2z: NIfTI to OME-Zarr Conversion","text":"<p>The <code>n2z</code> script converts NIfTI files to OME-Zarr format with multiscale pyramid generation. It's a wrapper around <code>ZarrNii.from_nifti().to_ome_zarr()</code>.</p>"},{"location":"cli/#basic-usage_1","title":"Basic Usage","text":"<pre><code>n2z input.nii.gz output.ome.zarr\n</code></pre>"},{"location":"cli/#options_1","title":"Options","text":"<pre><code>n2z --help\n</code></pre> <pre><code>usage: n2z [-h] [--chunks CHUNKS] [--axes-order {XYZ,ZYX}] [--name NAME] \n           [--as-ref] [--zooms ZOOMS] [--max-layer MAX_LAYER] \n           [--scale-factors SCALE_FACTORS]\n           input output\n\nConvert NIfTI to OME-Zarr format\n\npositional arguments:\n  input                 Input NIfTI file (.nii or .nii.gz)\n  output                Output OME-Zarr path or store\n\noptions:\n  -h, --help            show this help message and exit\n  --chunks CHUNKS       Chunk specification for dask arrays (default: auto)\n  --axes-order {XYZ,ZYX}\n                        Spatial axes order for processing (default: XYZ)\n  --name NAME           Name for the dataset\n  --as-ref              Create as reference without loading data\n  --zooms ZOOMS         Custom voxel sizes as comma-separated floats (e.g., '2.0,2.0,2.0')\n  --max-layer MAX_LAYER\n                        Maximum number of pyramid levels (default: 4)\n  --scale-factors SCALE_FACTORS\n                        Custom scale factors as comma-separated integers (e.g., '2,4,8')\n</code></pre>"},{"location":"cli/#examples_1","title":"Examples","text":"<pre><code># Basic conversion\nn2z input.nii.gz output.ome.zarr\n\n# Create compressed ZIP store with more pyramid levels\nn2z input.nii.gz output.ome.zarr.zip --max-layer 6\n\n# Use custom voxel sizes and create as reference\nn2z input.nii.gz output.ome.zarr --as-ref --zooms 1.5,1.5,2.5\n\n# Specify custom scale factors for pyramid\nn2z input.nii.gz output.ome.zarr --scale-factors 2,4,8,16\n\n# Change axes order and add dataset name\nn2z input.nii.gz output.ome.zarr --axes-order ZYX --name \"brain_scan\"\n\n# Custom chunking specification\nn2z input.nii.gz output.ome.zarr --chunks 64,64,32\n</code></pre>"},{"location":"cli/#common-workflows","title":"Common Workflows","text":""},{"location":"cli/#round-trip-conversion","title":"Round-trip Conversion","text":"<p>Convert from NIfTI to OME-Zarr and back to verify data integrity:</p> <pre><code># Original NIfTI to OME-Zarr\nn2z original.nii.gz intermediate.ome.zarr\n\n# OME-Zarr back to NIfTI\nz2n intermediate.ome.zarr final.nii.gz\n</code></pre>"},{"location":"cli/#working-with-zip-stores","title":"Working with ZIP Stores","text":"<p>OME-Zarr supports compressed ZIP format for efficient storage and sharing:</p> <pre><code># Create compressed OME-Zarr\nn2z input.nii.gz output.ome.zarr.zip\n\n# Convert from compressed OME-Zarr\nz2n input.ome.zarr.zip output.nii.gz\n</code></pre>"},{"location":"cli/#multiscale-processing","title":"Multiscale Processing","text":"<p>Create OME-Zarr with multiple resolution levels for efficient visualization:</p> <pre><code># Create 6 pyramid levels\nn2z input.nii.gz output.ome.zarr --max-layer 6\n\n# Extract a lower resolution level\nz2n output.ome.zarr downsampled.nii.gz --level 2\n</code></pre>"},{"location":"cli/#channel-and-timepoint-selection","title":"Channel and Timepoint Selection","text":"<p>For multi-dimensional datasets:</p> <pre><code># Select specific channels during conversion\nz2n multichannel.ome.zarr selected_channels.nii.gz --channels 0,2,4\n\n# Select specific timepoints\nz2n timeseries.ome.zarr timepoint_subset.nii.gz --timepoints 0,10,20\n</code></pre>"},{"location":"cli/#error-handling","title":"Error Handling","text":"<p>The CLI scripts provide informative error messages for common issues:</p> <ul> <li>File not found: Clear error message if input file doesn't exist</li> <li>Invalid arguments: Helpful guidance for malformed arguments</li> <li>Conversion errors: Detailed error information for debugging</li> </ul> <p>Example error handling:</p> <pre><code># Missing input file\n$ z2n missing.ome.zarr output.nii.gz\nError: Input path 'missing.ome.zarr' does not exist\n\n# Invalid zoom specification\n$ n2z input.nii.gz output.ome.zarr --zooms 1.0,2.0\nError during conversion: Expected exactly 3 comma-separated floats\n</code></pre>"},{"location":"cli/#integration-with-python-api","title":"Integration with Python API","text":"<p>The CLI scripts use the same underlying API as the Python interface, so you can easily switch between command-line and programmatic usage:</p> <pre><code># Equivalent Python code for: n2z input.nii.gz output.ome.zarr\nfrom zarrnii import ZarrNii\n\nznimg = ZarrNii.from_nifti(\"input.nii.gz\")\nznimg.to_ome_zarr(\"output.ome.zarr\")\n</code></pre> <pre><code># Equivalent Python code for: z2n input.ome.zarr output.nii.gz --level 1\nfrom zarrnii import ZarrNii\n\nznimg = ZarrNii.from_ome_zarr(\"input.ome.zarr\", level=1)\nznimg.to_nifti(\"output.nii.gz\")\n</code></pre> <p>This consistency makes it easy to prototype with the CLI and then integrate the same functionality into your Python workflows.</p>"},{"location":"contributing/","title":"Contributing to ZarrNii","text":"<p>Thank you for your interest in contributing to ZarrNii! This document provides guidelines for contributing to the project.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11 or higher</li> <li>Git</li> <li>uv package manager (recommended) or pip</li> </ul>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Clone the repository:    <code>bash    git clone https://github.com/khanlab/zarrnii.git    cd zarrnii</code></p> </li> <li> <p>Install dependencies:    ```bash    # Using uv (recommended)    uv sync --dev</p> </li> </ol> <p># Or using pip    pip install -e \".[dev]\"    ```</p> <ol> <li>Set up pre-commit hooks:    <code>bash    uv run pre-commit install</code></li> </ol>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#code-style-and-quality","title":"Code Style and Quality","text":"<p>ZarrNii follows Python best practices and uses several tools to maintain code quality:</p> <ul> <li>Black: Code formatting (line-length: 88)</li> <li>isort: Import sorting (profile: black, line-length: 88)  </li> <li>flake8: Linting and style checking (max-line-length: 88, extend-ignore: E203,W503)</li> <li>pre-commit: Automated checks before commits</li> </ul> <p>Run quality checks:</p> <pre><code># Format code  \nuv run black .\n\n# Sort imports\nuv run isort .\n\n# Lint code\nuv run flake8 .\n\n# Run all quality checks (matches CI exactly)\n./scripts/quality-check.sh\n\n# Or use justfile for convenience (if just is installed)\njust format\njust lint  \njust quality\n</code></pre> <p>Important: All tools are configured with consistent settings: - Line length: 88 characters (matches Black's default) - Import profile: black (ensures compatibility) - Flake8 ignores: E203, W503 (compatibility with Black)</p>"},{"location":"contributing/#testing","title":"Testing","text":"<p>ZarrNii uses pytest for testing. Tests are located in the <code>tests/</code> directory.</p> <pre><code># Run all tests\nuv run pytest -v\n\n# Run with coverage\nuv run pytest --cov=zarrnii\n\n# Run with detailed coverage report\nuv run pytest --cov=zarrnii --cov-report=term-missing\n\n# Generate HTML coverage report\nuv run pytest --cov=zarrnii --cov-report=html\n\n# Run specific test file\nuv run pytest tests/test_io.py\n\n# Or use justfile\njust test\n</code></pre>"},{"location":"contributing/#coverage-requirements","title":"Coverage Requirements","text":"<p>ZarrNii maintains code coverage standards: - Minimum coverage: 70% (CI requirement) - Target coverage: 85%+ - Current coverage: 86%+ - Coverage reports exclude <code>_version.py</code> and test files - New features must include comprehensive tests - Pull requests should not decrease overall coverage significantly</p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>Documentation is built with MkDocs and hosted on GitHub Pages.</p> <pre><code># Serve documentation locally\nuv run mkdocs serve\n\n# Build documentation\nuv run mkdocs build\n\n# Deploy to GitHub Pages (maintainers only)\nuv run mkdocs gh-deploy\n\n# Or use justfile\njust serve-docs\njust build-docs\n</code></pre>"},{"location":"contributing/#contribution-types","title":"Contribution Types","text":""},{"location":"contributing/#bug-reports","title":"Bug Reports","text":"<p>When reporting bugs, please include:</p> <ul> <li>Python version and platform</li> <li>ZarrNii version</li> <li>Minimal code example that reproduces the issue</li> <li>Expected vs. actual behavior</li> <li>Full error traceback if applicable</li> </ul>"},{"location":"contributing/#feature-requests","title":"Feature Requests","text":"<p>For feature requests, please provide:</p> <ul> <li>Clear description of the proposed feature</li> <li>Use case and motivation</li> <li>Potential implementation approach</li> <li>Any relevant literature or references</li> </ul>"},{"location":"contributing/#code-contributions","title":"Code Contributions","text":""},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li> <p>Fork the repository and create a feature branch:    <code>bash    git checkout -b feature/your-feature-name</code></p> </li> <li> <p>Make your changes:</p> </li> <li>Follow existing code style and conventions</li> <li>Add tests for new functionality</li> <li>Update documentation as needed</li> <li> <p>Ensure all tests pass</p> </li> <li> <p>Commit your changes:    <code>bash    git add .    git commit -m \"Add: brief description of your changes\"</code></p> </li> <li> <p>Push and create a Pull Request:    <code>bash    git push origin feature/your-feature-name</code></p> </li> </ol>"},{"location":"contributing/#code-review-guidelines","title":"Code Review Guidelines","text":"<ul> <li>All code must pass CI checks</li> <li>New features require tests and documentation</li> <li>Breaking changes need detailed justification</li> <li>Performance implications should be considered</li> </ul>"},{"location":"contributing/#documentation-contributions","title":"Documentation Contributions","text":"<p>Documentation improvements are always welcome! This includes:</p> <ul> <li>Fixing typos and grammatical errors</li> <li>Adding examples and tutorials</li> <li>Improving API documentation</li> <li>Translating content</li> </ul>"},{"location":"contributing/#development-guidelines","title":"Development Guidelines","text":""},{"location":"contributing/#code-organization","title":"Code Organization","text":"<ul> <li>Core functionality: <code>zarrnii/core.py</code></li> <li>Transformations: <code>zarrnii/transform.py</code> </li> <li>Utilities: <code>zarrnii/utils.py</code></li> <li>Enumerations: <code>zarrnii/enums.py</code></li> </ul>"},{"location":"contributing/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>Use descriptive variable and function names</li> <li>Follow PEP 8 naming conventions</li> <li>Use type hints where appropriate</li> <li>Document complex algorithms and edge cases</li> </ul>"},{"location":"contributing/#error-handling","title":"Error Handling","text":"<ul> <li>Raise informative exceptions with clear messages</li> <li>Use appropriate exception types</li> <li>Handle common error scenarios gracefully</li> <li>Log warnings for non-fatal issues</li> </ul>"},{"location":"contributing/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Use Dask for lazy evaluation when possible</li> <li>Minimize memory allocation in hot paths</li> <li>Profile performance-critical code</li> <li>Consider memory usage for large datasets</li> </ul>"},{"location":"contributing/#api-design-principles","title":"API Design Principles","text":""},{"location":"contributing/#consistency","title":"Consistency","text":"<ul> <li>Methods should have predictable interfaces</li> <li>Similar operations should use similar naming patterns</li> <li>Return types should be consistent across methods</li> </ul>"},{"location":"contributing/#flexibility","title":"Flexibility","text":"<ul> <li>Support multiple input formats where reasonable</li> <li>Provide sensible defaults for optional parameters</li> <li>Allow customization through optional arguments</li> </ul>"},{"location":"contributing/#usability","title":"Usability","text":"<ul> <li>Prioritize common use cases in the main API</li> <li>Provide clear error messages</li> <li>Include comprehensive docstrings</li> </ul>"},{"location":"contributing/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"contributing/#test-categories","title":"Test Categories","text":"<ul> <li>Unit tests: Test individual functions and methods</li> <li>Integration tests: Test component interactions</li> <li>End-to-end tests: Test complete workflows</li> <li>Performance tests: Benchmark critical operations</li> </ul>"},{"location":"contributing/#test-data","title":"Test Data","text":"<ul> <li>Use synthetic data when possible</li> <li>Keep test files small</li> <li>Document data requirements clearly</li> <li>Provide utilities for generating test data</li> </ul>"},{"location":"contributing/#test-structure","title":"Test Structure","text":"<pre><code>def test_feature_description():\n    \"\"\"Test that feature works correctly under normal conditions.\"\"\"\n    # Arrange\n    input_data = create_test_data()\n\n    # Act\n    result = function_under_test(input_data)\n\n    # Assert\n    assert result.shape == expected_shape\n    assert np.allclose(result.data, expected_data)\n</code></pre>"},{"location":"contributing/#release-process","title":"Release Process","text":""},{"location":"contributing/#version-numbering","title":"Version Numbering","text":"<p>ZarrNii uses Semantic Versioning: - MAJOR: Incompatible API changes - MINOR: New functionality, backwards compatible - PATCH: Bug fixes, backwards compatible</p>"},{"location":"contributing/#release-checklist","title":"Release Checklist","text":"<ol> <li>Ensure all tests pass and CI is green</li> <li>Update changelog with new features and fixes</li> <li>Build and test documentation</li> <li>Create and push a git tag:    <code>bash    git tag v1.0.0    git push origin v1.0.0</code></li> <li>GitHub Actions will automatically:</li> <li>Build the package using uv</li> <li>Deploy to PyPI using trusted publishing</li> <li>Update GitHub release notes</li> </ol>"},{"location":"contributing/#community-guidelines","title":"Community Guidelines","text":""},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>We are committed to providing a welcoming and inclusive environment. Please:</p> <ul> <li>Be respectful and considerate</li> <li>Use inclusive language</li> <li>Focus on constructive feedback</li> <li>Help create a positive community</li> </ul>"},{"location":"contributing/#communication","title":"Communication","text":"<ul> <li>GitHub Issues: Bug reports and feature requests</li> <li>GitHub Discussions: Questions and general discussion</li> <li>Pull Requests: Code contributions and reviews</li> </ul>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<p>If you need help with development:</p> <ol> <li>Check existing documentation and examples</li> <li>Search through GitHub issues</li> <li>Create a new issue with your question</li> <li>Join community discussions</li> </ol>"},{"location":"contributing/#recognition","title":"Recognition","text":"<p>Contributors will be recognized in: - Project README - Release notes - Documentation acknowledgments</p> <p>Thank you for helping make ZarrNii better!</p>"},{"location":"faq/","title":"FAQ: Frequently Asked Questions","text":"<p>This page addresses common questions and provides troubleshooting tips for using ZarrNii.</p>"},{"location":"faq/#general-questions","title":"General Questions","text":""},{"location":"faq/#1-what-is-zarrnii","title":"1. What is ZarrNii?","text":"<p>ZarrNii is a Python library that bridges the gap between OME-Zarr and NIfTI formats, enabling seamless conversion, transformations, and metadata handling for multidimensional biomedical images.</p>"},{"location":"faq/#2-what-formats-does-zarrnii-support","title":"2. What formats does ZarrNii support?","text":"<p>ZarrNii supports: - OME-Zarr: A format for storing chunked, multidimensional microscopy images. - NIfTI: A format commonly used for neuroimaging data.</p>"},{"location":"faq/#3-can-zarrnii-handle-large-datasets","title":"3. Can ZarrNii handle large datasets?","text":"<p>Yes! ZarrNii uses Dask arrays to handle datasets that don't fit into memory. Most transformations are lazy, meaning computations are only performed when explicitly triggered using <code>.compute()</code>.</p>"},{"location":"faq/#installation-issues","title":"Installation Issues","text":""},{"location":"faq/#1-i-installed-zarrnii-but-i-cant-import-it","title":"1. I installed ZarrNii, but I can't import it.","text":"<p>Ensure that ZarrNii is installed in the correct Python environment. Use <code>uv tree</code> or <code>pip show zarrnii</code> to verify the installation.</p> <p>If you're still encountering issues, try reinstalling the library:</p> <pre><code>uv sync --dev\n</code></pre>"},{"location":"faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"faq/#performance-tips","title":"Performance Tips","text":""},{"location":"faq/#1-how-can-i-speed-up-transformations-on-large-datasets","title":"1. How can I speed up transformations on large datasets?","text":"<ul> <li>Use appropriate chunk sizes with <code>.rechunk()</code> for operations like downsampling or interpolation.</li> <li>Trigger computations only when necessary using <code>.compute()</code>.</li> </ul>"},{"location":"faq/#2-how-do-i-optimize-multiscale-processing","title":"2. How do I optimize multiscale processing?","text":"<p>For OME-Zarr datasets with multiscale pyramids: 1. Use the appropriate <code>level</code> when loading the dataset.</p> <pre><code>znimg = ZarrNii.from_ome_zarr(\"path/to/dataset.zarr\", level=2)\n</code></pre>"},{"location":"faq/#metadata-questions","title":"Metadata Questions","text":""},{"location":"faq/#1-how-do-i-access-ome-zarr-metadata","title":"1. How do I access OME-Zarr metadata?","text":"<p>ZarrNii provides attributes for accessing metadata:</p> <pre><code>print(\"Axes:\", znimg.axes)\nprint(\"Coordinate transformations:\", znimg.coordinate_transformations)\nprint(\"Omero metadata:\", znimg.omero)\n</code></pre>"},{"location":"faq/#2-does-zarrnii-preserve-metadata-during-transformations","title":"2. Does ZarrNii preserve metadata during transformations?","text":"<p>Yes, ZarrNii updates the metadata to remain consistent with transformations like cropping, downsampling, or affine transformations.</p>"},{"location":"faq/#getting-help","title":"Getting Help","text":"<p>If you encounter issues not covered here: 1. Check the API Reference for detailed information about ZarrNii methods. 2. Open an issue on the GitHub repository.</p>"},{"location":"faq/#summary","title":"Summary","text":"<p>This FAQ covers common questions about ZarrNii, troubleshooting tips, and best practices for working with large datasets and metadata. For more in-depth information, explore: - Examples - API Reference</p>"},{"location":"reference/","title":"API Reference","text":"<p>This page documents the core classes, methods, and functions in ZarrNii. </p>"},{"location":"reference/#core-classes","title":"Core Classes","text":""},{"location":"reference/#zarrnii","title":"ZarrNii","text":"<p>The <code>ZarrNii</code> class provides tools for reading, writing, and transforming datasets in OME-Zarr and NIfTI formats.</p>"},{"location":"reference/#key-methods","title":"Key Methods","text":"<ul> <li><code>from_ome_zarr</code>: Load data from OME-Zarr.</li> <li><code>from_nifti</code>: Load data from a NIfTI file.</li> <li><code>from_imaris</code>: Load data from an Imaris (.ims) file.</li> <li><code>to_ome_zarr</code>: Save data as OME-Zarr.</li> <li><code>to_nifti</code>: Save data as a NIfTI file.</li> <li><code>to_tiff_stack</code>: Save data as a stack of 2D TIFF files.</li> <li><code>to_imaris</code>: Save data as an Imaris (.ims) file.</li> <li><code>crop</code>: Extract a region from the dataset.</li> <li><code>downsample</code>: Reduce resolution of datasets.</li> <li><code>upsample</code>: Increase resolution of datasets.</li> <li><code>apply_transform</code>: Apply spatial transformations.</li> </ul> <p>Zarr-based image with NIfTI compatibility using NgffImage internally.</p> <p>This class provides chainable operations on OME-Zarr data while maintaining compatibility with NIfTI workflows. It uses NgffImage objects internally for better multiscale support and metadata preservation.</p> <p>Attributes:</p> Name Type Description <code>ngff_image</code> <code>NgffImage</code> <p>The internal NgffImage object containing data and metadata.</p> <code>axes_order</code> <code>str</code> <p>The order of the axes for NIfTI compatibility ('ZYX' or 'XYZ').</p> <code>xyz_orientation</code> <code>str</code> <p>The anatomical orientation string in XYZ axes order (e.g., 'RAS', 'LPI').</p> <p>Constructor with backward compatibility for old signature.</p> Source code in <code>zarrnii/core.py</code> <pre><code>def __init__(\n    self,\n    darr=None,\n    affine=None,\n    axes_order=\"ZYX\",\n    orientation=\"RAS\",\n    xyz_orientation=None,\n    ngff_image=None,\n    _omero=None,\n    **kwargs,\n):\n    \"\"\"\n    Constructor with backward compatibility for old signature.\n    \"\"\"\n    # Handle backwards compatibility: if xyz_orientation is provided, use it\n    # Otherwise, use orientation for backwards compatibility\n    final_orientation = (\n        xyz_orientation if xyz_orientation is not None else orientation\n    )\n\n    if ngff_image is not None:\n        # New signature\n        object.__setattr__(self, \"ngff_image\", ngff_image)\n        object.__setattr__(self, \"axes_order\", axes_order)\n        object.__setattr__(self, \"xyz_orientation\", final_orientation)\n        object.__setattr__(self, \"_omero\", _omero)\n    elif darr is not None:\n        # Legacy signature - delegate to from_darr\n        instance = self.from_darr(\n            darr=darr,\n            affine=affine,\n            axes_order=axes_order,\n            orientation=final_orientation,\n            **kwargs,\n        )\n        object.__setattr__(self, \"ngff_image\", instance.ngff_image)\n        object.__setattr__(self, \"axes_order\", instance.axes_order)\n        object.__setattr__(self, \"xyz_orientation\", instance.xyz_orientation)\n        object.__setattr__(self, \"_omero\", instance._omero)\n    else:\n        raise ValueError(\"Must provide either ngff_image or darr\")\n</code></pre>"},{"location":"reference/#methods","title":"Methods","text":""},{"location":"reference/#from_ome_zarr","title":"<code>from_ome_zarr</code>","text":"<p>Load ZarrNii from OME-Zarr store with flexible options.</p> <p>Creates a ZarrNii instance from an OME-Zarr store, supporting multiscale pyramids, channel/timepoint selection, and various storage backends. Automatically handles metadata extraction and format conversion.</p> <p>Parameters:</p> Name Type Description Default <code>store_or_path</code> <code>Union[str, Any]</code> <p>Store or path to OME-Zarr file. Supports: - Local file paths - Remote URLs (s3://, http://, etc.) - ZIP files (.zip extension) - Zarr store objects</p> required <code>level</code> <code>int</code> <p>Pyramid level to load (0 = highest resolution). If level exceeds available levels, applies lazy downsampling</p> <code>0</code> <code>channels</code> <code>Optional[List[int]]</code> <p>List of channel indices to load (0-based). Mutually exclusive with channel_labels</p> <code>None</code> <code>channel_labels</code> <code>Optional[List[str]]</code> <p>List of channel names to load by label. Requires OMERO metadata. Mutually exclusive with channels</p> <code>None</code> <code>timepoints</code> <code>Optional[List[int]]</code> <p>List of timepoint indices to load (0-based). If None, loads all available timepoints</p> <code>None</code> <code>storage_options</code> <code>Optional[Dict[str, Any]]</code> <p>Additional options for zarr storage backend (e.g., credentials for cloud storage)</p> <code>None</code> <code>axes_order</code> <code>str</code> <p>Spatial axis order for NIfTI compatibility. Either \"ZYX\" or \"XYZ\"</p> <code>'ZYX'</code> <code>orientation</code> <code>str</code> <p>Default anatomical orientation if not in metadata. Standard orientations like \"RAS\", \"LPI\", etc. This is always interpreted in XYZ axes order for consistency.</p> <code>'RAS'</code> <code>downsample_near_isotropic</code> <code>bool</code> <p>If True, automatically downsample dimensions with smaller voxel sizes to achieve near-isotropic resolution</p> <code>False</code> <code>chunks</code> <code>tuple[int, Ellipsis] | Literal['auto']</code> <p>chunking strategy, or explicit chunk sizes to use if not automatic</p> <code>'auto'</code> <code>rechunk</code> <code>bool</code> <p>If True, rechunks the dataset after lazy loading, based on the chunks parameter</p> <code>False</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>ZarrNii instance with loaded data and metadata</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both channels and channel_labels are specified, or if invalid level/indices are provided</p> <code>FileNotFoundError</code> <p>If store_or_path does not exist</p> <code>KeyError</code> <p>If specified channel labels are not found</p> <code>IOError</code> <p>If unable to read from the storage backend</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Load full resolution data\n&gt;&gt;&gt; znii = ZarrNii.from_ome_zarr(\"/path/to/data.zarr\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Load specific channels and pyramid level\n&gt;&gt;&gt; znii = ZarrNii.from_ome_zarr(\n...     \"/path/to/data.zarr\",\n...     level=1,\n...     channels=[0, 2],\n...     orientation=\"LPI\"\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Load from cloud storage\n&gt;&gt;&gt; znii = ZarrNii.from_ome_zarr(\n...     \"s3://bucket/data.zarr\",\n...     storage_options={\"key\": \"access_key\", \"secret\": \"secret\"}\n... )\n</code></pre> Notes <p>Orientation Metadata Backwards Compatibility:</p> <p>This method implements backwards compatibility for orientation metadata:</p> <ol> <li> <p>Priority Order: Checks for 'xyz_orientation' first (new format),    then falls back to 'orientation' (legacy format)</p> </li> <li> <p>Legacy Fallback: When only legacy 'orientation' is found, the    orientation string is automatically reversed to convert from ZYX-based    encoding (legacy) to XYZ-based encoding (current standard)</p> </li> <li> <p>Default Fallback: If no orientation metadata is found, uses the    provided 'orientation' parameter as the default</p> </li> </ol> <p>Examples of the conversion: - Legacy 'orientation'='SAR' (ZYX) \u2192 'xyz_orientation'='RAS' (XYZ) - Legacy 'orientation'='IPL' (ZYX) \u2192 'xyz_orientation'='LPI' (XYZ)</p> <p>This ensures consistent orientation handling while maintaining backwards compatibility with existing OME-Zarr files that use the legacy format.</p> Source code in <code>zarrnii/core.py</code> <pre><code>@classmethod\ndef from_ome_zarr(\n    cls,\n    store_or_path: Union[str, Any],\n    level: int = 0,\n    channels: Optional[List[int]] = None,\n    channel_labels: Optional[List[str]] = None,\n    timepoints: Optional[List[int]] = None,\n    storage_options: Optional[Dict[str, Any]] = None,\n    axes_order: str = \"ZYX\",\n    orientation: str = \"RAS\",\n    downsample_near_isotropic: bool = False,\n    chunks: tuple[int, Ellipsis] | Literal[\"auto\"] = \"auto\",\n    rechunk: bool = False,\n) -&gt; \"ZarrNii\":\n    \"\"\"Load ZarrNii from OME-Zarr store with flexible options.\n\n    Creates a ZarrNii instance from an OME-Zarr store, supporting multiscale\n    pyramids, channel/timepoint selection, and various storage backends.\n    Automatically handles metadata extraction and format conversion.\n\n    Args:\n        store_or_path: Store or path to OME-Zarr file. Supports:\n            - Local file paths\n            - Remote URLs (s3://, http://, etc.)\n            - ZIP files (.zip extension)\n            - Zarr store objects\n        level: Pyramid level to load (0 = highest resolution). If level\n            exceeds available levels, applies lazy downsampling\n        channels: List of channel indices to load (0-based). Mutually\n            exclusive with channel_labels\n        channel_labels: List of channel names to load by label. Requires\n            OMERO metadata. Mutually exclusive with channels\n        timepoints: List of timepoint indices to load (0-based). If None,\n            loads all available timepoints\n        storage_options: Additional options for zarr storage backend\n            (e.g., credentials for cloud storage)\n        axes_order: Spatial axis order for NIfTI compatibility.\n            Either \"ZYX\" or \"XYZ\"\n        orientation: Default anatomical orientation if not in metadata.\n            Standard orientations like \"RAS\", \"LPI\", etc. This is always\n            interpreted in XYZ axes order for consistency.\n        downsample_near_isotropic: If True, automatically downsample\n            dimensions with smaller voxel sizes to achieve near-isotropic\n            resolution\n        chunks: chunking strategy, or explicit chunk sizes to use if not automatic\n        rechunk: If True, rechunks the dataset after lazy loading, based\n            on the chunks parameter\n\n    Returns:\n        ZarrNii instance with loaded data and metadata\n\n    Raises:\n        ValueError: If both channels and channel_labels are specified,\n            or if invalid level/indices are provided\n        FileNotFoundError: If store_or_path does not exist\n        KeyError: If specified channel labels are not found\n        IOError: If unable to read from the storage backend\n\n    Examples:\n        &gt;&gt;&gt; # Load full resolution data\n        &gt;&gt;&gt; znii = ZarrNii.from_ome_zarr(\"/path/to/data.zarr\")\n\n        &gt;&gt;&gt; # Load specific channels and pyramid level\n        &gt;&gt;&gt; znii = ZarrNii.from_ome_zarr(\n        ...     \"/path/to/data.zarr\",\n        ...     level=1,\n        ...     channels=[0, 2],\n        ...     orientation=\"LPI\"\n        ... )\n\n        &gt;&gt;&gt; # Load from cloud storage\n        &gt;&gt;&gt; znii = ZarrNii.from_ome_zarr(\n        ...     \"s3://bucket/data.zarr\",\n        ...     storage_options={\"key\": \"access_key\", \"secret\": \"secret\"}\n        ... )\n\n    Notes:\n        **Orientation Metadata Backwards Compatibility:**\n\n        This method implements backwards compatibility for orientation metadata:\n\n        1. **Priority Order**: Checks for 'xyz_orientation' first (new format),\n           then falls back to 'orientation' (legacy format)\n\n        2. **Legacy Fallback**: When only legacy 'orientation' is found, the\n           orientation string is automatically reversed to convert from ZYX-based\n           encoding (legacy) to XYZ-based encoding (current standard)\n\n        3. **Default Fallback**: If no orientation metadata is found, uses the\n           provided 'orientation' parameter as the default\n\n        Examples of the conversion:\n        - Legacy 'orientation'='SAR' (ZYX) \u2192 'xyz_orientation'='RAS' (XYZ)\n        - Legacy 'orientation'='IPL' (ZYX) \u2192 'xyz_orientation'='LPI' (XYZ)\n\n        This ensures consistent orientation handling while maintaining backwards\n        compatibility with existing OME-Zarr files that use the legacy format.\n    \"\"\"\n    # Validate channel and timepoint selection arguments\n    if channels is not None and channel_labels is not None:\n        raise ValueError(\"Cannot specify both 'channels' and 'channel_labels'\")\n\n    # Load the multiscales object\n    try:\n        if isinstance(store_or_path, str):\n            # Handle ZIP files by creating a ZipStore\n            if store_or_path.endswith(\".zip\"):\n                import zarr\n\n                store = zarr.storage.ZipStore(store_or_path, mode=\"r\")\n                multiscales = nz.from_ngff_zarr(\n                    store, storage_options=storage_options or {}\n                )\n                # Note: We'll close the store after extracting metadata\n            else:\n                multiscales = nz.from_ngff_zarr(\n                    store_or_path, storage_options=storage_options or {}\n                )\n        else:\n            multiscales = nz.from_ngff_zarr(store_or_path)\n    except Exception as e:\n        # Fallback for older zarr/ngff_zarr versions\n        if isinstance(store_or_path, str):\n            if store_or_path.endswith(\".zip\"):\n                import zarr\n\n                store = zarr.storage.ZipStore(store_or_path, mode=\"r\")\n                multiscales = nz.from_ngff_zarr(store)\n            else:\n                store = fsspec.get_mapper(store_or_path, **storage_options or {})\n                multiscales = nz.from_ngff_zarr(store)\n        else:\n            store = store_or_path\n            multiscales = nz.from_ngff_zarr(store)\n\n    # Extract omero metadata if available\n    omero_metadata = None\n    try:\n        import zarr\n\n        if isinstance(store_or_path, str):\n            if store_or_path.endswith(\".zip\"):\n                zip_store = zarr.storage.ZipStore(store_or_path, mode=\"r\")\n                group = zarr.open_group(zip_store, mode=\"r\")\n                # Close zip store after getting group\n                zip_store.close()\n            else:\n                group = zarr.open_group(store_or_path, mode=\"r\")\n\n        else:\n            group = zarr.open_group(store_or_path, mode=\"r\")\n\n        if \"omero\" in group.attrs:\n            omero_dict = group.attrs[\"omero\"]\n\n            # Create a simple object to hold omero metadata\n            class OmeroMetadata:\n                def __init__(self, omero_dict):\n                    self.channels = []\n                    if \"channels\" in omero_dict:\n                        for ch_dict in omero_dict[\"channels\"]:\n                            # Create channel objects\n                            class ChannelMetadata:\n                                def __init__(self, ch_dict):\n                                    self.label = ch_dict.get(\"label\", \"\")\n                                    self.color = ch_dict.get(\"color\", \"\")\n                                    if \"window\" in ch_dict:\n\n                                        class WindowMetadata:\n                                            def __init__(self, win_dict):\n                                                self.min = win_dict.get(\"min\", 0.0)\n                                                self.max = win_dict.get(\n                                                    \"max\", 65535.0\n                                                )\n                                                self.start = win_dict.get(\n                                                    \"start\", 0.0\n                                                )\n                                                self.end = win_dict.get(\n                                                    \"end\", 65535.0\n                                                )\n\n                                        self.window = WindowMetadata(\n                                            ch_dict[\"window\"]\n                                        )\n                                    else:\n                                        self.window = None\n\n                            self.channels.append(ChannelMetadata(ch_dict))\n\n            omero_metadata = OmeroMetadata(omero_dict)\n    except Exception:\n        # If we can't load omero metadata, that's okay\n        pass\n\n    # Read orientation metadata with backwards compatibility support\n    # Priority: xyz_orientation (new) &gt; orientation (legacy, with reversal)\n    try:\n        import zarr\n\n        if isinstance(store_or_path, str):\n            if store_or_path.endswith(\".zip\"):\n                zip_store = zarr.storage.ZipStore(store_or_path, mode=\"r\")\n                group = zarr.open_group(zip_store, mode=\"r\")\n                # Check for new xyz_orientation first, then fallback to legacy orientation\n                if \"xyz_orientation\" in group.attrs:\n                    orientation = group.attrs[\"xyz_orientation\"]\n                elif \"orientation\" in group.attrs:\n                    # Legacy orientation is ZYX-based, reverse it to get XYZ-based orientation\n                    legacy_orientation = group.attrs[\"orientation\"]\n                    orientation = reverse_orientation_string(legacy_orientation)\n                # If neither found, use the provided default orientation\n                zip_store.close()\n            else:\n                group = zarr.open_group(store_or_path, mode=\"r\")\n                # Check for new xyz_orientation first, then fallback to legacy orientation\n                if \"xyz_orientation\" in group.attrs:\n                    orientation = group.attrs[\"xyz_orientation\"]\n                elif \"orientation\" in group.attrs:\n                    # Legacy orientation is ZYX-based, reverse it to get XYZ-based orientation\n                    legacy_orientation = group.attrs[\"orientation\"]\n                    orientation = reverse_orientation_string(legacy_orientation)\n                # If neither found, use the provided default orientation\n        else:\n            group = zarr.open_group(store_or_path, mode=\"r\")\n            # Check for new xyz_orientation first, then fallback to legacy orientation\n            if \"xyz_orientation\" in group.attrs:\n                orientation = group.attrs[\"xyz_orientation\"]\n            elif \"orientation\" in group.attrs:\n                # Legacy orientation is ZYX-based, reverse it to get XYZ-based orientation\n                legacy_orientation = group.attrs[\"orientation\"]\n                orientation = reverse_orientation_string(legacy_orientation)\n            # If neither found, use the provided default orientation\n\n    except Exception:\n        # If we can't read orientation metadata, use the provided default\n        pass\n\n    # Determine the available pyramid levels and handle lazy downsampling\n    max_level = len(multiscales.images) - 1\n    actual_level = min(level, max_level)\n    do_downsample = level &gt; max_level\n\n    # Get the highest available level\n    ngff_image = multiscales.images[actual_level]\n\n    # Handle channel and timepoint selection and filter omero metadata accordingly\n    filtered_omero = omero_metadata\n    if channels is not None or channel_labels is not None or timepoints is not None:\n        ngff_image, filtered_omero = _select_dimensions_from_image_with_omero(\n            ngff_image,\n            multiscales,\n            channels,\n            channel_labels,\n            timepoints,\n            omero_metadata,\n        )\n\n    # Create ZarrNii instance with xyz_orientation\n    znimg = cls(\n        ngff_image=ngff_image,\n        axes_order=axes_order,\n        xyz_orientation=orientation,\n        _omero=filtered_omero,\n    )\n\n    # Apply lazy downsampling if needed\n    if do_downsample:\n        level_ds = level - max_level\n        downsample_factor = 2**level_ds\n\n        # Get spatial dims based on axes order\n        spatial_dims = [\"z\", \"y\", \"x\"] if axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n\n        # Apply downsampling using the existing method\n        znimg = znimg.downsample(\n            factors=downsample_factor, spatial_dims=spatial_dims\n        )\n\n    # Apply near-isotropic downsampling if requested\n    if downsample_near_isotropic:\n        znimg = _apply_near_isotropic_downsampling(znimg, axes_order)\n\n    if rechunk:\n        znimg.data = znimg.data.rechunk(chunks)\n\n    return znimg\n</code></pre>"},{"location":"reference/#from_nifti","title":"<code>from_nifti</code>","text":"<p>Load ZarrNii from NIfTI file with flexible loading options.</p> <p>Creates a ZarrNii instance from a NIfTI file, automatically converting the data to dask arrays and extracting spatial transformation information. Supports both full data loading and reference-only loading for memory efficiency.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, bytes]</code> <p>File path to NIfTI file (.nii, .nii.gz, .img/.hdr)</p> required <code>chunks</code> <code>Union[str, Tuple[int, ...]]</code> <p>Dask array chunking strategy. Can be: - \"auto\": Automatic chunking based on file size - Tuple of ints: Manual chunk sizes for each dimension - Dict mapping axis to chunk size</p> <code>'auto'</code> <code>axes_order</code> <code>str</code> <p>Spatial axis ordering convention. Either: - \"XYZ\": X=left-right, Y=anterior-posterior, Z=inferior-superior - \"ZYX\": Z=inferior-superior, Y=anterior-posterior, X=left-right</p> <code>'XYZ'</code> <code>name</code> <code>Optional[str]</code> <p>Optional name for the resulting NgffImage. If None, uses filename without extension</p> <code>None</code> <code>as_ref</code> <code>bool</code> <p>If True, creates empty dask array with correct shape/metadata without loading actual image data (memory efficient for templates)</p> <code>False</code> <code>zooms</code> <code>Optional[Tuple[float, float, float]]</code> <p>Target voxel spacing as (x, y, z) in mm. Only valid when as_ref=True. Adjusts shape and affine accordingly</p> <code>None</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>ZarrNii instance containing NIfTI data and spatial metadata</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If zooms specified with as_ref=False, or invalid axes_order</p> <code>FileNotFoundError</code> <p>If NIfTI file does not exist</p> <code>OSError</code> <p>If unable to read NIfTI file</p> <code>ImageFileError</code> <p>If file is not valid NIfTI</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Load full NIfTI data\n&gt;&gt;&gt; znii = ZarrNii.from_nifti(\"/path/to/brain.nii.gz\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Load with custom chunking and axis order\n&gt;&gt;&gt; znii = ZarrNii.from_nifti(\n...     \"/path/to/data.nii\",\n...     chunks=(64, 64, 64),\n...     axes_order=\"ZYX\"\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Create reference with target resolution\n&gt;&gt;&gt; znii_ref = ZarrNii.from_nifti(\n...     \"/path/to/template.nii.gz\",\n...     as_ref=True,\n...     zooms=(2.0, 2.0, 2.0)\n... )\n</code></pre> Notes <p>The method automatically handles NIfTI orientation codes and converts them to the specified axes_order for consistency with OME-Zarr workflows.</p> Source code in <code>zarrnii/core.py</code> <pre><code>@classmethod\ndef from_nifti(\n    cls,\n    path: Union[str, bytes],\n    chunks: Union[str, Tuple[int, ...]] = \"auto\",\n    axes_order: str = \"XYZ\",\n    name: Optional[str] = None,\n    as_ref: bool = False,\n    zooms: Optional[Tuple[float, float, float]] = None,\n) -&gt; \"ZarrNii\":\n    \"\"\"Load ZarrNii from NIfTI file with flexible loading options.\n\n    Creates a ZarrNii instance from a NIfTI file, automatically converting\n    the data to dask arrays and extracting spatial transformation information.\n    Supports both full data loading and reference-only loading for memory\n    efficiency.\n\n    Args:\n        path: File path to NIfTI file (.nii, .nii.gz, .img/.hdr)\n        chunks: Dask array chunking strategy. Can be:\n            - \"auto\": Automatic chunking based on file size\n            - Tuple of ints: Manual chunk sizes for each dimension\n            - Dict mapping axis to chunk size\n        axes_order: Spatial axis ordering convention. Either:\n            - \"XYZ\": X=left-right, Y=anterior-posterior, Z=inferior-superior\n            - \"ZYX\": Z=inferior-superior, Y=anterior-posterior, X=left-right\n        name: Optional name for the resulting NgffImage. If None,\n            uses filename without extension\n        as_ref: If True, creates empty dask array with correct shape/metadata\n            without loading actual image data (memory efficient for templates)\n        zooms: Target voxel spacing as (x, y, z) in mm. Only valid when\n            as_ref=True. Adjusts shape and affine accordingly\n\n    Returns:\n        ZarrNii instance containing NIfTI data and spatial metadata\n\n    Raises:\n        ValueError: If zooms specified with as_ref=False, or invalid axes_order\n        FileNotFoundError: If NIfTI file does not exist\n        OSError: If unable to read NIfTI file\n        nibabel.filebasedimages.ImageFileError: If file is not valid NIfTI\n\n    Examples:\n        &gt;&gt;&gt; # Load full NIfTI data\n        &gt;&gt;&gt; znii = ZarrNii.from_nifti(\"/path/to/brain.nii.gz\")\n\n        &gt;&gt;&gt; # Load with custom chunking and axis order\n        &gt;&gt;&gt; znii = ZarrNii.from_nifti(\n        ...     \"/path/to/data.nii\",\n        ...     chunks=(64, 64, 64),\n        ...     axes_order=\"ZYX\"\n        ... )\n\n        &gt;&gt;&gt; # Create reference with target resolution\n        &gt;&gt;&gt; znii_ref = ZarrNii.from_nifti(\n        ...     \"/path/to/template.nii.gz\",\n        ...     as_ref=True,\n        ...     zooms=(2.0, 2.0, 2.0)\n        ... )\n\n    Notes:\n        The method automatically handles NIfTI orientation codes and converts\n        them to the specified axes_order for consistency with OME-Zarr workflows.\n    \"\"\"\n    if not as_ref and zooms is not None:\n        raise ValueError(\"`zooms` can only be used when `as_ref=True`.\")\n\n    # Load NIfTI file\n    nifti_img = nib.load(path)\n    shape = nifti_img.header.get_data_shape()\n    affine_matrix = nifti_img.affine.copy()\n\n    # infer orientation from the affine\n    orientation = affine_to_orientation(affine_matrix)\n\n    # Adjust shape and affine if zooms are provided\n    if zooms is not None:\n        in_zooms = np.sqrt(\n            (affine_matrix[:3, :3] ** 2).sum(axis=0)\n        )  # Current voxel spacing\n        scaling_factor = in_zooms / zooms\n        new_shape = [\n            int(np.floor(shape[0] * scaling_factor[2])),  # Z\n            int(np.floor(shape[1] * scaling_factor[1])),  # Y\n            int(np.floor(shape[2] * scaling_factor[0])),  # X\n        ]\n        np.fill_diagonal(affine_matrix[:3, :3], zooms)\n    else:\n        new_shape = shape\n\n    if as_ref:\n        # Create an empty dask array with the adjusted shape\n        darr = da.empty((1, *new_shape), chunks=chunks, dtype=\"float32\")\n    else:\n        # Load the NIfTI data and convert to a dask array\n        array = nifti_img.get_fdata()\n        darr = da.from_array(array, chunks=chunks)\n\n    # Add channel and time dimensions if not present\n    original_ndim = len(darr.shape)\n\n    if original_ndim == 3:\n        # 3D data: add channel dimension -&gt; (c, z, y, x) or (c, x, y, z)\n        darr = darr[np.newaxis, ...]\n    elif original_ndim == 4:\n        # 4D data: could be (c, z, y, x) or (t, z, y, x) - assume channel by default\n        # User can specify if it's time by using appropriate axes_order\n        pass  # Keep as is - 4D is already handled\n    elif original_ndim == 5:\n        # 5D data: assume (t, z, y, x, c) and handle appropriately\n        pass  # Keep as is - 5D is already the target format\n    else:\n        # For 1D, 2D, or &gt;5D data, add channel dimension and let user handle\n        darr = darr[np.newaxis, ...]\n\n    # Create dimensions based on data shape after dimension adjustments\n    final_ndim = len(darr.shape)\n    if final_ndim == 4:\n        # 4D: (c, z, y, x) or (c, x, y, z) - standard case\n        dims = [\"c\"] + list(axes_order.lower())\n    elif final_ndim == 5:\n        # 5D: (t, c, z, y, x) or (t, c, x, y, z) - time dimension included\n        dims = [\"t\", \"c\"] + list(axes_order.lower())\n    else:\n        # Fallback for other cases\n        dims = [\"c\"] + list(axes_order.lower())\n\n    # Extract scale and translation from affine\n    scale = {}\n    translation = {}\n    spatial_dims = [\"z\", \"y\", \"x\"] if axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n\n    for i, dim in enumerate(spatial_dims):\n        scale[dim] = np.sqrt((affine_matrix[i, :3] ** 2).sum())\n        translation[dim] = affine_matrix[i, 3]\n\n    # Create NgffImage\n    if name is None:\n        name = f\"nifti_image_{path}\"\n\n    ngff_image = nz.NgffImage(\n        data=darr, dims=dims, scale=scale, translation=translation, name=name\n    )\n\n    return cls(\n        ngff_image=ngff_image, axes_order=axes_order, xyz_orientation=orientation\n    )\n</code></pre>"},{"location":"reference/#from_imaris","title":"<code>from_imaris</code>","text":"<p>Load from Imaris (.ims) file format.</p> <p>Imaris files use HDF5 format with specific dataset structure. This method requires the 'imaris' extra dependency (h5py).</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to Imaris (.ims) file</p> required <code>level</code> <code>int</code> <p>Resolution level to load (0 = full resolution)</p> <code>0</code> <code>timepoint</code> <code>int</code> <p>Time point to load (default: 0)</p> <code>0</code> <code>channel</code> <code>int</code> <p>Channel to load (default: 0)</p> <code>0</code> <code>chunks</code> <code>str</code> <p>Chunking strategy for dask array</p> <code>'auto'</code> <code>axes_order</code> <code>str</code> <p>Spatial axes order for compatibility (default: \"ZYX\")</p> <code>'ZYX'</code> <code>orientation</code> <code>str</code> <p>Default orientation (default: \"RAS\")</p> <code>'RAS'</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>ZarrNii instance</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If h5py is not available</p> <code>ValueError</code> <p>If the file is not a valid Imaris file</p> Source code in <code>zarrnii/core.py</code> <pre><code>@classmethod\ndef from_imaris(\n    cls,\n    path: str,\n    level: int = 0,\n    timepoint: int = 0,\n    channel: int = 0,\n    chunks: str = \"auto\",\n    axes_order: str = \"ZYX\",\n    orientation: str = \"RAS\",\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Load from Imaris (.ims) file format.\n\n    Imaris files use HDF5 format with specific dataset structure.\n    This method requires the 'imaris' extra dependency (h5py).\n\n    Args:\n        path: Path to Imaris (.ims) file\n        level: Resolution level to load (0 = full resolution)\n        timepoint: Time point to load (default: 0)\n        channel: Channel to load (default: 0)\n        chunks: Chunking strategy for dask array\n        axes_order: Spatial axes order for compatibility (default: \"ZYX\")\n        orientation: Default orientation (default: \"RAS\")\n\n    Returns:\n        ZarrNii instance\n\n    Raises:\n        ImportError: If h5py is not available\n        ValueError: If the file is not a valid Imaris file\n    \"\"\"\n    try:\n        import h5py\n    except ImportError:\n        raise ImportError(\n            \"h5py is required for Imaris support. \"\n            \"Install with: pip install zarrnii[imaris] or pip install h5py\"\n        )\n\n    # Open Imaris file\n    with h5py.File(path, \"r\") as f:\n        # Verify it's an Imaris file by checking for standard structure\n        if \"DataSet\" not in f:\n            raise ValueError(\n                f\"File {path} does not appear to be a valid Imaris file (missing DataSet group)\"\n            )\n\n        # Navigate to the specific dataset\n        dataset_group = f[\"DataSet\"]\n\n        # Find available resolution levels\n        resolution_levels = [\n            key for key in dataset_group.keys() if key.startswith(\"ResolutionLevel\")\n        ]\n        if not resolution_levels:\n            raise ValueError(\"No resolution levels found in Imaris file\")\n\n        # Validate level parameter\n        if level &gt;= len(resolution_levels):\n            raise ValueError(\n                f\"Level {level} not available. Available levels: 0-{len(resolution_levels)-1}\"\n            )\n\n        # Navigate to specified resolution level\n        res_level_key = f\"ResolutionLevel {level}\"\n        if res_level_key not in dataset_group:\n            raise ValueError(f\"Resolution level {level} not found\")\n\n        res_group = dataset_group[res_level_key]\n\n        # Find available timepoints\n        timepoints = [\n            key for key in res_group.keys() if key.startswith(\"TimePoint\")\n        ]\n        if not timepoints:\n            raise ValueError(\"No timepoints found in Imaris file\")\n\n        # Validate timepoint parameter\n        if timepoint &gt;= len(timepoints):\n            raise ValueError(\n                f\"Timepoint {timepoint} not available. Available timepoints: 0-{len(timepoints)-1}\"\n            )\n\n        # Navigate to specified timepoint\n        time_key = f\"TimePoint {timepoint}\"\n        if time_key not in res_group:\n            raise ValueError(f\"Timepoint {timepoint} not found\")\n\n        time_group = res_group[time_key]\n\n        # Find available channels\n        channels = [key for key in time_group.keys() if key.startswith(\"Channel\")]\n        if not channels:\n            raise ValueError(\"No channels found in Imaris file\")\n\n        # Validate channel parameter\n        if channel &gt;= len(channels):\n            raise ValueError(\n                f\"Channel {channel} not available. Available channels: 0-{len(channels)-1}\"\n            )\n\n        # Navigate to specified channel\n        channel_key = f\"Channel {channel}\"\n        if channel_key not in time_group:\n            raise ValueError(f\"Channel {channel} not found\")\n\n        channel_group = time_group[channel_key]\n\n        # Load the actual data\n        if \"Data\" not in channel_group:\n            raise ValueError(\"No Data dataset found in channel group\")\n\n        data_dataset = channel_group[\"Data\"]\n\n        # Load data into memory first (necessary because HDF5 file will be closed)\n        data_numpy = data_dataset[:]\n\n        # Create dask array from numpy array\n        data_array = da.from_array(data_numpy, chunks=chunks)\n\n        # Add channel dimension if not present\n        if len(data_array.shape) == 3:\n            data_array = data_array[np.newaxis, ...]\n\n        # Extract spatial metadata\n        # Try to get spacing information from Imaris metadata\n        spacing = [1.0, 1.0, 1.0]  # Default spacing\n        origin = [0.0, 0.0, 0.0]  # Default origin\n\n        # Look for ImageSizeX, ImageSizeY, ImageSizeZ attributes\n        try:\n            # Navigate back to get image info\n            if \"ImageSizeX\" in f.attrs:\n                x_size = f.attrs[\"ImageSizeX\"]\n                y_size = f.attrs[\"ImageSizeY\"]\n                z_size = f.attrs[\"ImageSizeZ\"]\n\n                # Calculate spacing based on physical size and voxel count\n                if data_array.shape[-1] &gt; 0:  # X dimension\n                    spacing[0] = x_size / data_array.shape[-1]\n                if data_array.shape[-2] &gt; 0:  # Y dimension\n                    spacing[1] = y_size / data_array.shape[-2]\n                if data_array.shape[-3] &gt; 0:  # Z dimension\n                    spacing[2] = z_size / data_array.shape[-3]\n        except (KeyError, IndexError):\n            # Use default spacing if metadata is not available\n            pass\n\n        # Create dimensions\n        dims = [\"c\"] + list(axes_order.lower())\n\n        # Create scale and translation dictionaries\n        scale_dict = {}\n        translation_dict = {}\n        spatial_dims = [\"z\", \"y\", \"x\"] if axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n\n        for i, dim in enumerate(spatial_dims):\n            scale_dict[dim] = spacing[i]\n            translation_dict[dim] = origin[i]\n\n        # Create NgffImage\n        ngff_image = nz.NgffImage(\n            data=data_array,\n            dims=dims,\n            scale=scale_dict,\n            translation=translation_dict,\n            name=f\"imaris_image_{path}_{level}_{timepoint}_{channel}\",\n        )\n\n    # Create and return ZarrNii instance\n    return cls(\n        ngff_image=ngff_image,\n        axes_order=axes_order,\n        xyz_orientation=orientation,\n        _omero=None,\n    )\n</code></pre>"},{"location":"reference/#to_ome_zarr","title":"<code>to_ome_zarr</code>","text":"<p>Save to OME-Zarr store with multiscale pyramid.</p> <p>Creates an OME-Zarr dataset with automatic multiscale pyramid generation for efficient visualization and processing at multiple resolutions. Preserves spatial metadata and supports various storage backends.</p> <p>Parameters:</p> Name Type Description Default <code>store_or_path</code> <code>Union[str, Any]</code> <p>Target location for OME-Zarr store. Supports: - Local directory path - Remote URLs (s3://, gs://, etc.) - ZIP files (.zip extension for compressed storage) - Zarr store objects</p> required <code>max_layer</code> <code>int</code> <p>Maximum number of pyramid levels to create (including level 0). Higher values create more downsampled levels</p> <code>4</code> <code>scale_factors</code> <code>Optional[List[int]]</code> <p>Custom downsampling factors for each pyramid level. If None, uses powers of 2: [2, 4, 8, 16, ...]</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to underlying to_ngff_zarr function. May include compression options, chunk sizes, etc.</p> <code>{}</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>Self for method chaining</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If unable to write to target location</p> <code>ValueError</code> <p>If invalid scale_factors provided</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Save with default pyramid levels\n&gt;&gt;&gt; znii.to_ome_zarr(\"/path/to/output.zarr\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Save to compressed ZIP with custom pyramid\n&gt;&gt;&gt; znii.to_ome_zarr(\n...     \"/path/to/output.zarr.zip\",\n...     max_layer=3,\n...     scale_factors=[2, 4]\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Chain with other operations\n&gt;&gt;&gt; result = (znii.downsample(2)\n...               .crop((0,0,0), (100,100,100))\n...               .to_ome_zarr(\"processed.zarr\"))\n</code></pre> Notes <ul> <li>OME-Zarr files are always saved in ZYX axis order</li> <li>Automatic axis reordering if current order is XYZ</li> <li>Spatial transformations and metadata are preserved</li> <li>Orientation information is stored using the new 'xyz_orientation'   metadata key for consistency and future compatibility</li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def to_ome_zarr(\n    self,\n    store_or_path: Union[str, Any],\n    max_layer: int = 4,\n    scale_factors: Optional[List[int]] = None,\n    **kwargs: Any,\n) -&gt; \"ZarrNii\":\n    \"\"\"Save to OME-Zarr store with multiscale pyramid.\n\n    Creates an OME-Zarr dataset with automatic multiscale pyramid generation\n    for efficient visualization and processing at multiple resolutions.\n    Preserves spatial metadata and supports various storage backends.\n\n    Args:\n        store_or_path: Target location for OME-Zarr store. Supports:\n            - Local directory path\n            - Remote URLs (s3://, gs://, etc.)\n            - ZIP files (.zip extension for compressed storage)\n            - Zarr store objects\n        max_layer: Maximum number of pyramid levels to create (including level 0).\n            Higher values create more downsampled levels\n        scale_factors: Custom downsampling factors for each pyramid level.\n            If None, uses powers of 2: [2, 4, 8, 16, ...]\n        **kwargs: Additional arguments passed to underlying to_ngff_zarr function.\n            May include compression options, chunk sizes, etc.\n\n    Returns:\n        Self for method chaining\n\n    Raises:\n        OSError: If unable to write to target location\n        ValueError: If invalid scale_factors provided\n\n    Examples:\n        &gt;&gt;&gt; # Save with default pyramid levels\n        &gt;&gt;&gt; znii.to_ome_zarr(\"/path/to/output.zarr\")\n\n        &gt;&gt;&gt; # Save to compressed ZIP with custom pyramid\n        &gt;&gt;&gt; znii.to_ome_zarr(\n        ...     \"/path/to/output.zarr.zip\",\n        ...     max_layer=3,\n        ...     scale_factors=[2, 4]\n        ... )\n\n        &gt;&gt;&gt; # Chain with other operations\n        &gt;&gt;&gt; result = (znii.downsample(2)\n        ...               .crop((0,0,0), (100,100,100))\n        ...               .to_ome_zarr(\"processed.zarr\"))\n\n    Notes:\n        - OME-Zarr files are always saved in ZYX axis order\n        - Automatic axis reordering if current order is XYZ\n        - Spatial transformations and metadata are preserved\n        - Orientation information is stored using the new 'xyz_orientation'\n          metadata key for consistency and future compatibility\n    \"\"\"\n    # Determine the image to save\n    if self.axes_order == \"XYZ\":\n        # Need to reorder data from XYZ to ZYX for OME-Zarr\n        ngff_image_to_save = self._create_zyx_ngff_image()\n    else:\n        # Already in ZYX order\n        ngff_image_to_save = self.ngff_image\n\n    save_ngff_image(\n        ngff_image_to_save,\n        store_or_path,\n        max_layer,\n        scale_factors,\n        xyz_orientation=(\n            self.xyz_orientation if hasattr(self, \"xyz_orientation\") else None\n        ),\n        **kwargs,\n    )\n\n    # Add orientation metadata to the zarr store (only for non-ZIP files)\n    # For ZIP files, orientation is handled inside save_ngff_image\n    if not (isinstance(store_or_path, str) and store_or_path.endswith(\".zip\")):\n        try:\n            import zarr\n\n            if isinstance(store_or_path, str):\n                group = zarr.open_group(store_or_path, mode=\"r+\")\n            else:\n                group = zarr.open_group(store_or_path, mode=\"r+\")\n\n            # Add metadata for xyz_orientation (new format)\n            if hasattr(self, \"xyz_orientation\") and self.xyz_orientation:\n                group.attrs[\"xyz_orientation\"] = self.xyz_orientation\n        except Exception:\n            # If we can't write orientation metadata, that's not critical\n            pass\n\n    return self\n</code></pre>"},{"location":"reference/#to_nifti","title":"<code>to_nifti</code>","text":"<p>Convert to NIfTI format with automatic dimension handling.</p> <p>Converts the ZarrNii image to NIfTI-1 format, handling dimension reordering, singleton dimension removal, and spatial transformation conversion. NIfTI files are always written in XYZ axis order.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>Optional[Union[str, bytes]]</code> <p>Output file path for saving. Supported extensions: - .nii: Uncompressed NIfTI - .nii.gz: Compressed NIfTI (recommended) If None, returns nibabel image object without saving</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Nifti1Image, str]</code> <p>If filename is None: nibabel.Nifti1Image object</p> <code>Union[Nifti1Image, str]</code> <p>If filename provided: path to saved file</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data has non-singleton time or channel dimensions (NIfTI doesn't support &gt;4D data)</p> <code>OSError</code> <p>If unable to write to specified filename</p> Notes <ul> <li>Automatically reorders data from ZYX to XYZ if necessary</li> <li>Removes singleton time/channel dimensions automatically</li> <li>Spatial transformations are converted to NIfTI affine format</li> <li>For 5D data (T,C,Z,Y,X), only singleton T/C dimensions are supported</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Save to compressed NIfTI file\n&gt;&gt;&gt; znii.to_nifti(\"output.nii.gz\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Get nibabel object without saving\n&gt;&gt;&gt; nifti_img = znii.to_nifti()\n&gt;&gt;&gt; print(nifti_img.shape)\n</code></pre> <pre><code>&gt;&gt;&gt; # Handle multi-channel data by selecting single channel first\n&gt;&gt;&gt; znii.select_channels([0]).to_nifti(\"channel0.nii.gz\")\n</code></pre> Source code in <code>zarrnii/core.py</code> <pre><code>def to_nifti(\n    self, filename: Optional[Union[str, bytes]] = None\n) -&gt; Union[nib.Nifti1Image, str]:\n    \"\"\"Convert to NIfTI format with automatic dimension handling.\n\n    Converts the ZarrNii image to NIfTI-1 format, handling dimension\n    reordering, singleton dimension removal, and spatial transformation\n    conversion. NIfTI files are always written in XYZ axis order.\n\n    Args:\n        filename: Output file path for saving. Supported extensions:\n            - .nii: Uncompressed NIfTI\n            - .nii.gz: Compressed NIfTI (recommended)\n            If None, returns nibabel image object without saving\n\n    Returns:\n        If filename is None: nibabel.Nifti1Image object\n        If filename provided: path to saved file\n\n    Raises:\n        ValueError: If data has non-singleton time or channel dimensions\n            (NIfTI doesn't support &gt;4D data)\n        OSError: If unable to write to specified filename\n\n    Notes:\n        - Automatically reorders data from ZYX to XYZ if necessary\n        - Removes singleton time/channel dimensions automatically\n        - Spatial transformations are converted to NIfTI affine format\n        - For 5D data (T,C,Z,Y,X), only singleton T/C dimensions are supported\n\n    Examples:\n        &gt;&gt;&gt; # Save to compressed NIfTI file\n        &gt;&gt;&gt; znii.to_nifti(\"output.nii.gz\")\n\n        &gt;&gt;&gt; # Get nibabel object without saving\n        &gt;&gt;&gt; nifti_img = znii.to_nifti()\n        &gt;&gt;&gt; print(nifti_img.shape)\n\n        &gt;&gt;&gt; # Handle multi-channel data by selecting single channel first\n        &gt;&gt;&gt; znii.select_channels([0]).to_nifti(\"channel0.nii.gz\")\n\n    Warnings:\n        Large images will be computed in memory during conversion.\n        Consider downsampling or cropping first for very large datasets.\n    \"\"\"\n    # Get data and dimensions\n    data = self.data.compute()\n\n    dims = self.dims\n\n    # Handle dimensional reduction for NIfTI compatibility\n    # NIfTI supports up to 4D, so we need to remove singleton dimensions\n    squeeze_axes = []\n    remaining_dims = []\n\n    for i, dim in enumerate(dims):\n        if dim in [\"t\", \"c\"] and data.shape[i] == 1:\n            # Remove singleton time or channel dimensions\n            squeeze_axes.append(i)\n        elif dim in [\"t\", \"c\"] and data.shape[i] &gt; 1:\n            # Non-singleton time or channel dimensions - NIfTI can't handle this\n            raise ValueError(\n                f\"NIfTI format doesn't support non-singleton {dim} dimension. \"\n                f\"Dimension '{dim}' has size {data.shape[i]}. \"\n                f\"Consider selecting specific timepoints/channels first.\"\n            )\n        else:\n            remaining_dims.append(dim)\n\n    # Squeeze out singleton dimensions\n    if squeeze_axes:\n        data = np.squeeze(data, axis=tuple(squeeze_axes))\n\n    # Check final dimensionality\n    if data.ndim &gt; 4:\n        raise ValueError(\n            f\"Resulting data has {data.ndim} dimensions, but NIfTI supports maximum 4D\"\n        )\n\n    # Now handle spatial reordering based on axes_order\n    if self.axes_order == \"ZYX\":\n        # Data spatial dimensions are in ZYX order, need to transpose to XYZ\n        if data.ndim == 3:\n            # Pure spatial data: ZYX -&gt; XYZ\n            data = data.transpose(2, 1, 0)\n        elif data.ndim == 4:\n            # 4D data with one non-spatial dimension remaining\n            # Could be (T,Z,Y,X) or (C,Z,Y,X) - spatial part needs ZYX-&gt;XYZ\n            # The non-spatial dimension stays first\n            data = data.transpose(0, 3, 2, 1)\n\n        # Get affine matrix in XYZ order\n        affine_matrix = self.get_affine_matrix(axes_order=\"XYZ\")\n    else:\n        # Data is already in XYZ order\n        affine_matrix = self.get_affine_matrix(axes_order=\"XYZ\")\n\n    # Create NIfTI image\n    nifti_img = nib.Nifti1Image(data, affine_matrix)\n\n    if filename is not None:\n        nib.save(nifti_img, filename)\n        return filename\n    else:\n        return nifti_img\n</code></pre>"},{"location":"reference/#to_tiff_stack","title":"<code>to_tiff_stack</code>","text":"<p>Save data as a stack of 2D TIFF images.</p> <p>Saves the image data as a series of 2D TIFF files, with each Z-slice saved as a separate file. This format is useful for compatibility with tools that don't support OME-Zarr or napari plugins that require individual TIFF files.</p> <p>Parameters:</p> Name Type Description Default <code>filename_pattern</code> <code>str</code> <p>Output filename pattern. Should contain '{z:04d}' or similar format specifier for the Z-slice number. Examples: - \"output_z{z:04d}.tif\" - \"data/slice_{z:03d}.tiff\" If pattern doesn't contain format specifier, '_{z:04d}' is appended before the extension.</p> required <code>channel</code> <code>Optional[int]</code> <p>Channel index to save (0-based). If None and data has multiple channels, all channels will be saved as separate channel dimensions in each TIFF file (multi-channel TIFFs).</p> <code>None</code> <code>timepoint</code> <code>Optional[int]</code> <p>Timepoint index to save (0-based). If None and data has multiple timepoints, raises ValueError (must select single timepoint).</p> <code>None</code> <code>compress</code> <code>bool</code> <p>Whether to use LZW compression (default: True)</p> <code>True</code> <code>dtype</code> <code>Optional[str]</code> <p>Output data type for TIFF files. Options: - 'uint8': 8-bit unsigned integer (0-255) - 'uint16': 16-bit unsigned integer (0-65535) [default] - 'int16': 16-bit signed integer (-32768 to 32767) - 'float32': 32-bit float (preserves original data) Default 'uint16' provides good range and compatibility.</p> <code>'uint16'</code> <code>rescale</code> <code>bool</code> <p>Whether to rescale data to fit the output dtype range. If True, data is linearly scaled from [min, max] to the full range of the output dtype. If False, data is clipped to the output dtype range. Default: True</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Base directory path where files were saved</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data has multiple timepoints but none selected, or if selected channel/timepoint is out of range, or if dtype is not supported</p> <code>OSError</code> <p>If unable to write to specified directory</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Save as 16-bit with auto-rescaling (default, recommended)\n&gt;&gt;&gt; znii.to_tiff_stack(\"output_z{z:04d}.tif\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Save as 8-bit for smaller file sizes\n&gt;&gt;&gt; znii.to_tiff_stack(\"output_z{z:04d}.tif\", dtype='uint8')\n</code></pre> <pre><code>&gt;&gt;&gt; # Save specific channel without rescaling\n&gt;&gt;&gt; znii.to_tiff_stack(\"channel0_z{z:04d}.tif\", channel=0, rescale=False)\n</code></pre> <pre><code>&gt;&gt;&gt; # Save as float32 to preserve original precision\n&gt;&gt;&gt; znii.to_tiff_stack(\"precise_z{z:04d}.tif\", dtype='float32')\n</code></pre> Notes <ul> <li>Z-dimension becomes the stack (file) dimension</li> <li>Time and channel dimensions are handled as specified</li> <li>Spatial transformations are not preserved in TIFF format</li> <li>For 5D data (T,C,Z,Y,X), you must select a single timepoint</li> <li>Multi-channel data can be saved as multi-channel TIFFs or selected</li> <li>Data type conversion helps ensure compatibility with analysis tools</li> <li>uint16 is recommended for most scientific applications (good range + compatibility)</li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def to_tiff_stack(\n    self,\n    filename_pattern: str,\n    channel: Optional[int] = None,\n    timepoint: Optional[int] = None,\n    compress: bool = True,\n    dtype: Optional[str] = \"uint16\",\n    rescale: bool = True,\n) -&gt; str:\n    \"\"\"Save data as a stack of 2D TIFF images.\n\n    Saves the image data as a series of 2D TIFF files, with each Z-slice\n    saved as a separate file. This format is useful for compatibility with\n    tools that don't support OME-Zarr or napari plugins that require\n    individual TIFF files.\n\n    Args:\n        filename_pattern: Output filename pattern. Should contain '{z:04d}' or similar\n            format specifier for the Z-slice number. Examples:\n            - \"output_z{z:04d}.tif\"\n            - \"data/slice_{z:03d}.tiff\"\n            If pattern doesn't contain format specifier, '_{z:04d}' is appended\n            before the extension.\n        channel: Channel index to save (0-based). If None and data has multiple\n            channels, all channels will be saved as separate channel dimensions\n            in each TIFF file (multi-channel TIFFs).\n        timepoint: Timepoint index to save (0-based). If None and data has multiple\n            timepoints, raises ValueError (must select single timepoint).\n        compress: Whether to use LZW compression (default: True)\n        dtype: Output data type for TIFF files. Options:\n            - 'uint8': 8-bit unsigned integer (0-255)\n            - 'uint16': 16-bit unsigned integer (0-65535) [default]\n            - 'int16': 16-bit signed integer (-32768 to 32767)\n            - 'float32': 32-bit float (preserves original data)\n            Default 'uint16' provides good range and compatibility.\n        rescale: Whether to rescale data to fit the output dtype range.\n            If True, data is linearly scaled from [min, max] to the full\n            range of the output dtype. If False, data is clipped to the\n            output dtype range. Default: True\n\n    Returns:\n        Base directory path where files were saved\n\n    Raises:\n        ValueError: If data has multiple timepoints but none selected,\n            or if selected channel/timepoint is out of range,\n            or if dtype is not supported\n        OSError: If unable to write to specified directory\n\n    Examples:\n        &gt;&gt;&gt; # Save as 16-bit with auto-rescaling (default, recommended)\n        &gt;&gt;&gt; znii.to_tiff_stack(\"output_z{z:04d}.tif\")\n\n        &gt;&gt;&gt; # Save as 8-bit for smaller file sizes\n        &gt;&gt;&gt; znii.to_tiff_stack(\"output_z{z:04d}.tif\", dtype='uint8')\n\n        &gt;&gt;&gt; # Save specific channel without rescaling\n        &gt;&gt;&gt; znii.to_tiff_stack(\"channel0_z{z:04d}.tif\", channel=0, rescale=False)\n\n        &gt;&gt;&gt; # Save as float32 to preserve original precision\n        &gt;&gt;&gt; znii.to_tiff_stack(\"precise_z{z:04d}.tif\", dtype='float32')\n\n    Warnings:\n        This method loads all data into memory. For large datasets,\n        consider cropping or downsampling first to reduce memory usage.\n        The cellseg3d napari plugin and similar tools work best with\n        cropped regions rather than full-resolution whole-brain images.\n\n    Notes:\n        - Z-dimension becomes the stack (file) dimension\n        - Time and channel dimensions are handled as specified\n        - Spatial transformations are not preserved in TIFF format\n        - For 5D data (T,C,Z,Y,X), you must select a single timepoint\n        - Multi-channel data can be saved as multi-channel TIFFs or selected\n        - Data type conversion helps ensure compatibility with analysis tools\n        - uint16 is recommended for most scientific applications (good range + compatibility)\n    \"\"\"\n    try:\n        import tifffile\n    except ImportError:\n        raise ImportError(\n            \"tifffile is required for TIFF stack support. \"\n            \"Install with: pip install tifffile\"\n        )\n\n    # Get data and dimensions\n    data = self.data.compute()\n    dims = self.dims\n\n    # Create output directory if needed\n    import os\n\n    output_dir = os.path.dirname(filename_pattern)\n    if output_dir and not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Handle dimensional selection and validation\n    # Remove singleton dimensions first, similar to to_nifti\n    squeeze_axes = []\n    remaining_dims = []\n    time_dim_size = 1\n    channel_dim_size = 1\n\n    for i, dim in enumerate(dims):\n        if dim == \"t\":\n            time_dim_size = data.shape[i]\n            if data.shape[i] == 1:\n                squeeze_axes.append(i)\n            elif timepoint is None:\n                raise ValueError(\n                    f\"Data has {data.shape[i]} timepoints. \"\n                    f\"Must specify 'timepoint' parameter to select a single timepoint.\"\n                )\n            elif timepoint &gt;= data.shape[i]:\n                raise ValueError(\n                    f\"Timepoint {timepoint} is out of range (data has {data.shape[i]} timepoints)\"\n                )\n            else:\n                remaining_dims.append(dim)\n        elif dim == \"c\":\n            channel_dim_size = data.shape[i]\n            if data.shape[i] == 1:\n                squeeze_axes.append(i)\n            elif channel is None:\n                raise ValueError(\n                    f\"Data has {data.shape[i]} channels. \"\n                    f\"Must specify 'channel' parameter to select a single channel.\"\n                )\n            elif channel &gt;= data.shape[i]:\n                raise ValueError(\n                    f\"Channel {channel} is out of range (data has {data.shape[i]} channels)\"\n                )\n            else:\n                remaining_dims.append(dim)\n        else:\n            remaining_dims.append(dim)\n\n    # Select specific timepoint if needed\n    if time_dim_size &gt; 1 and timepoint is not None:\n        time_axis = dims.index(\"t\")\n        data = np.take(data, timepoint, axis=time_axis)\n        # Update dims list\n        dims = [d for i, d in enumerate(dims) if i != time_axis]\n\n    # Select specific channel if needed\n    if channel_dim_size &gt; 1 and channel is not None:\n        channel_axis = dims.index(\"c\")\n        data = np.take(data, channel, axis=channel_axis)\n        # Update dims list\n        dims = [d for i, d in enumerate(dims) if i != channel_axis]\n\n    # Squeeze singleton dimensions\n    if squeeze_axes:\n        # Recalculate squeeze axes after potential dimension removal\n        current_squeeze_axes = []\n        for axis in squeeze_axes:\n            # Count how many axes were removed before this one\n            removed_before = sum(\n                1\n                for removed_axis in [\n                    (\n                        dims.index(\"t\")\n                        if time_dim_size &gt; 1 and timepoint is not None\n                        else -1\n                    ),\n                    (\n                        dims.index(\"c\")\n                        if channel_dim_size &gt; 1 and channel is not None\n                        else -1\n                    ),\n                ]\n                if removed_axis != -1 and removed_axis &lt; axis\n            )\n            current_squeeze_axes.append(axis - removed_before)\n\n        data = np.squeeze(data, axis=tuple(current_squeeze_axes))\n        dims = [dim for i, dim in enumerate(dims) if i not in current_squeeze_axes]\n\n    # Find Z dimension for stacking\n    if \"z\" not in dims:\n        raise ValueError(\"Data must have a Z dimension for TIFF stack export\")\n\n    z_axis = dims.index(\"z\")\n    z_size = data.shape[z_axis]\n\n    # Check filename pattern contains format specifier\n    if \"{z\" not in filename_pattern:\n        # Add default z format before extension\n        name, ext = os.path.splitext(filename_pattern)\n        filename_pattern = f\"{name}_{{z:04d}}{ext}\"\n\n    # Move Z axis to first position for easy iteration\n    axes_order = list(range(data.ndim))\n    axes_order[0], axes_order[z_axis] = axes_order[z_axis], axes_order[0]\n    data = data.transpose(axes_order)\n\n    # Handle data type conversion and rescaling\n    supported_dtypes = {\n        \"uint8\": np.uint8,\n        \"uint16\": np.uint16,\n        \"int16\": np.int16,\n        \"float32\": np.float32,\n    }\n\n    if dtype not in supported_dtypes:\n        raise ValueError(\n            f\"Unsupported dtype '{dtype}'. Supported types: {list(supported_dtypes.keys())}\"\n        )\n\n    target_dtype = supported_dtypes[dtype]\n\n    if rescale and dtype != \"float32\":\n        # Get the data range\n        data_min = np.min(data)\n        data_max = np.max(data)\n\n        if data_min == data_max:\n            # Handle constant data case\n            data_scaled = np.zeros_like(data, dtype=target_dtype)\n        else:\n            # Get target range for the dtype\n            if dtype == \"uint8\":\n                target_min, target_max = 0, 255\n            elif dtype == \"uint16\":\n                target_min, target_max = 0, 65535\n            elif dtype == \"int16\":\n                target_min, target_max = -32768, 32767\n\n            # Linear rescaling: new_value = (value - data_min) * (target_max - target_min) / (data_max - data_min) + target_min\n            data_scaled = (\n                (data - data_min)\n                * (target_max - target_min)\n                / (data_max - data_min)\n                + target_min\n            ).astype(target_dtype)\n\n        print(\n            f\"Rescaled data from [{data_min:.3f}, {data_max:.3f}] to {dtype} range\"\n        )\n    else:\n        # No rescaling - just clip and convert\n        if dtype == \"uint8\":\n            data_scaled = np.clip(data, 0, 255).astype(target_dtype)\n        elif dtype == \"uint16\":\n            data_scaled = np.clip(data, 0, 65535).astype(target_dtype)\n        elif dtype == \"int16\":\n            data_scaled = np.clip(data, -32768, 32767).astype(target_dtype)\n        else:  # float32\n            data_scaled = data.astype(target_dtype)\n\n        if dtype != \"float32\":\n            print(f\"Converted data to {dtype} with clipping (no rescaling)\")\n\n    data = data_scaled\n\n    # Save each Z-slice as a separate TIFF file\n    compression = \"lzw\" if compress else None\n    saved_files = []\n\n    for z_idx in range(z_size):\n        slice_data = data[z_idx]\n\n        # Generate filename for this slice\n        filename = filename_pattern.format(z=z_idx)\n\n        # Save the 2D slice\n        tifffile.imwrite(filename, slice_data, compression=compression)\n        saved_files.append(filename)\n\n    print(f\"Saved {len(saved_files)} TIFF files to {output_dir or '.'}\")\n    print(\n        f\"Files: {os.path.basename(saved_files[0])} ... {os.path.basename(saved_files[-1])}\"\n    )\n\n    return output_dir or \".\"\n</code></pre>"},{"location":"reference/#to_imaris","title":"<code>to_imaris</code>","text":"<p>Save to Imaris (.ims) file format using HDF5.</p> <p>This method creates Imaris files compatible with Imaris software by following the exact HDF5 structure from correctly-formed reference files. All attributes use byte-array encoding as required by Imaris.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Output path for Imaris (.ims) file</p> required <code>compression</code> <code>str</code> <p>HDF5 compression method (default: \"gzip\")</p> <code>'gzip'</code> <code>compression_opts</code> <code>int</code> <p>Compression level (default: 6)</p> <code>6</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the saved file</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If h5py is not available</p> Source code in <code>zarrnii/core.py</code> <pre><code>def to_imaris(\n    self, path: str, compression: str = \"gzip\", compression_opts: int = 6\n) -&gt; str:\n    \"\"\"\n    Save to Imaris (.ims) file format using HDF5.\n\n    This method creates Imaris files compatible with Imaris software by\n    following the exact HDF5 structure from correctly-formed reference files.\n    All attributes use byte-array encoding as required by Imaris.\n\n    Args:\n        path: Output path for Imaris (.ims) file\n        compression: HDF5 compression method (default: \"gzip\")\n        compression_opts: Compression level (default: 6)\n\n    Returns:\n        str: Path to the saved file\n\n    Raises:\n        ImportError: If h5py is not available\n    \"\"\"\n    try:\n        import h5py\n    except ImportError:\n        raise ImportError(\n            \"h5py is required for Imaris support. \"\n            \"Install with: pip install zarrnii[imaris] or pip install h5py\"\n        )\n\n    # Ensure path has .ims extension\n    if not path.endswith(\".ims\"):\n        path = path + \".ims\"\n\n    def _string_to_byte_array(s: str) -&gt; np.ndarray:\n        \"\"\"Convert string to byte array as required by Imaris.\"\"\"\n        return np.array([c.encode() for c in s])\n\n    # Get data and metadata\n    if hasattr(self.darr, \"compute\"):\n        data = self.darr.compute()  # Convert Dask array to numpy array\n    else:\n        data = np.asarray(self.darr)  # Handle numpy arrays directly\n\n    # Handle dimensions: expect ZYX or CZYX\n    if len(data.shape) == 4:\n        # CZYX format\n        n_channels = data.shape[0]\n        z, y, x = data.shape[1:]\n    elif len(data.shape) == 3:\n        # ZYX format - single channel\n        n_channels = 1\n        z, y, x = data.shape\n        data = data[np.newaxis, ...]  # Add channel dimension\n    else:\n        raise ValueError(\n            f\"Unsupported data shape: {data.shape}. Expected 3D (ZYX) or 4D (CZYX)\"\n        )\n\n    # Create Imaris file structure exactly matching reference file\n    with h5py.File(path, \"w\") as f:\n        # Root attributes - use exact byte array format from reference\n        f.attrs[\"DataSetDirectoryName\"] = _string_to_byte_array(\"DataSet\")\n        f.attrs[\"DataSetInfoDirectoryName\"] = _string_to_byte_array(\"DataSetInfo\")\n        f.attrs[\"ImarisDataSet\"] = _string_to_byte_array(\"ImarisDataSet\")\n        f.attrs[\"ImarisVersion\"] = _string_to_byte_array(\"5.5.0\")\n        f.attrs[\"NumberOfDataSets\"] = np.array([1], dtype=np.uint32)\n        f.attrs[\"ThumbnailDirectoryName\"] = _string_to_byte_array(\"Thumbnail\")\n\n        # Create main DataSet group structure\n        dataset_group = f.create_group(\"DataSet\")\n        res_group = dataset_group.create_group(\"ResolutionLevel 0\")\n        time_group = res_group.create_group(\"TimePoint 0\")\n\n        # Create channels with proper attributes\n        for c in range(n_channels):\n            channel_group = time_group.create_group(f\"Channel {c}\")\n            channel_data = data[c]  # (Z, Y, X)\n\n            # Channel attributes - use byte array format exactly like reference\n            channel_group.attrs[\"ImageSizeX\"] = _string_to_byte_array(str(x))\n            channel_group.attrs[\"ImageSizeY\"] = _string_to_byte_array(str(y))\n            channel_group.attrs[\"ImageSizeZ\"] = _string_to_byte_array(str(z))\n            channel_group.attrs[\"ImageBlockSizeX\"] = _string_to_byte_array(str(x))\n            channel_group.attrs[\"ImageBlockSizeY\"] = _string_to_byte_array(str(y))\n            channel_group.attrs[\"ImageBlockSizeZ\"] = _string_to_byte_array(\n                str(min(z, 16))\n            )\n\n            # Histogram range attributes\n            data_min, data_max = float(channel_data.min()), float(\n                channel_data.max()\n            )\n            channel_group.attrs[\"HistogramMin\"] = _string_to_byte_array(\n                f\"{data_min:.3f}\"\n            )\n            channel_group.attrs[\"HistogramMax\"] = _string_to_byte_array(\n                f\"{data_max:.3f}\"\n            )\n\n            # Create data dataset with proper compression\n            # Preserve original data type but ensure it's compatible with Imaris\n            if channel_data.dtype == np.float32 or channel_data.dtype == np.float64:\n                # Keep float data as is for round-trip compatibility\n                data_for_storage = channel_data.astype(np.float32)\n            elif channel_data.dtype in [np.uint16, np.int16]:\n                # Keep 16-bit data as is\n                data_for_storage = channel_data\n            else:\n                # Convert other types to uint8\n                data_for_storage = channel_data.astype(np.uint8)\n\n            channel_group.create_dataset(\n                \"Data\",\n                data=data_for_storage,\n                compression=compression,\n                compression_opts=compression_opts,\n                chunks=True,\n            )\n\n            # Create histogram\n            hist_data, _ = np.histogram(\n                channel_data.flatten(), bins=256, range=(data_min, data_max)\n            )\n            channel_group.create_dataset(\n                \"Histogram\", data=hist_data.astype(np.uint64)\n            )\n\n        # Get spacing directly from scale dictionary with proper XYZ order\n        try:\n            # Extract voxel sizes directly from ngff_image scale dictionary\n            # This ensures we get X, Y, Z in the correct order regardless of axes_order\n            sx = self.ngff_image.scale.get(\"x\", 1.0)\n            sy = self.ngff_image.scale.get(\"y\", 1.0)\n            sz = self.ngff_image.scale.get(\"z\", 1.0)\n        except:\n            sx = sy = sz = 1.0\n\n        # Calculate extents (physical coordinates)\n        ext_x = sx * x\n        ext_y = sy * y\n        ext_z = sz * z\n\n        # Create comprehensive DataSetInfo structure matching reference\n        info_group = f.create_group(\"DataSetInfo\")\n\n        # Create channel info groups\n        for c in range(n_channels):\n            channel_info = info_group.create_group(f\"Channel {c}\")\n\n            # Essential channel attributes in byte array format\n            channel_info.attrs[\"Color\"] = _string_to_byte_array(\n                \"1.000 0.000 0.000\"\n                if c == 0\n                else f\"0.000 {1.0 if c == 1 else 0.0:.3f} {1.0 if c == 2 else 0.0:.3f}\"\n            )\n            channel_info.attrs[\"Name\"] = _string_to_byte_array(f\"Channel {c}\")\n            channel_info.attrs[\"ColorMode\"] = _string_to_byte_array(\"BaseColor\")\n            channel_info.attrs[\"ColorOpacity\"] = _string_to_byte_array(\"1.000\")\n            channel_info.attrs[\"ColorRange\"] = _string_to_byte_array(\"0 255\")\n            channel_info.attrs[\"GammaCorrection\"] = _string_to_byte_array(\"1.000\")\n            channel_info.attrs[\"LSMEmissionWavelength\"] = _string_to_byte_array(\n                \"500\"\n            )\n            channel_info.attrs[\"LSMExcitationWavelength\"] = _string_to_byte_array(\n                \"500\"\n            )\n            channel_info.attrs[\"LSMPhotons\"] = _string_to_byte_array(\"1\")\n            channel_info.attrs[\"LSMPinhole\"] = _string_to_byte_array(\"0\")\n\n            # Add description\n            description = f\"Channel {c} created by ZarrNii\"\n            channel_info.attrs[\"Description\"] = _string_to_byte_array(description)\n\n        # Create CRITICAL Image group with voxel size information (this was missing!)\n        image_info = info_group.create_group(\"Image\")\n\n        # Add essential image metadata with proper voxel size information\n        image_info.attrs[\"X\"] = _string_to_byte_array(str(x))\n        image_info.attrs[\"Y\"] = _string_to_byte_array(str(y))\n        image_info.attrs[\"Z\"] = _string_to_byte_array(str(z))\n        image_info.attrs[\"Unit\"] = _string_to_byte_array(\"um\")\n        image_info.attrs[\"Noc\"] = _string_to_byte_array(str(n_channels))\n\n        # CRITICAL: Set proper physical extents that define voxel size\n        # Imaris reads voxel size from these extent values\n        image_info.attrs[\"ExtMin0\"] = _string_to_byte_array(f\"{-ext_x/2:.3f}\")\n        image_info.attrs[\"ExtMax0\"] = _string_to_byte_array(f\"{ext_x/2:.3f}\")\n        image_info.attrs[\"ExtMin1\"] = _string_to_byte_array(f\"{-ext_y/2:.3f}\")\n        image_info.attrs[\"ExtMax1\"] = _string_to_byte_array(f\"{ext_y/2:.3f}\")\n        image_info.attrs[\"ExtMin2\"] = _string_to_byte_array(f\"{-ext_z/2:.3f}\")\n        image_info.attrs[\"ExtMax2\"] = _string_to_byte_array(f\"{ext_z/2:.3f}\")\n\n        # Add device/acquisition metadata\n        image_info.attrs[\"ManufactorString\"] = _string_to_byte_array(\"ZarrNii\")\n        image_info.attrs[\"ManufactorType\"] = _string_to_byte_array(\"Generic\")\n        image_info.attrs[\"LensPower\"] = _string_to_byte_array(\"\")\n        image_info.attrs[\"NumericalAperture\"] = _string_to_byte_array(\"\")\n        image_info.attrs[\"RecordingDate\"] = _string_to_byte_array(\n            \"2024-01-01 00:00:00.000\"\n        )\n        image_info.attrs[\"Filename\"] = _string_to_byte_array(path.split(\"/\")[-1])\n        image_info.attrs[\"Name\"] = _string_to_byte_array(\"ZarrNii Export\")\n        image_info.attrs[\"Compression\"] = _string_to_byte_array(\"5794\")\n\n        # Add description\n        description = (\n            f\"Imaris file created by ZarrNii from {self.axes_order} format data. \"\n            f\"Original shape: {self.darr.shape}. Converted to Imaris format \"\n            f\"with {n_channels} channel(s) and dimensions {z}x{y}x{x}. \"\n            f\"Voxel size: {sx:.3f} x {sy:.3f} x {sz:.3f} um.\"\n        )\n        image_info.attrs[\"Description\"] = _string_to_byte_array(description)\n\n        # Create Imaris metadata group\n        imaris_info = info_group.create_group(\"Imaris\")\n        imaris_info.attrs[\"Version\"] = _string_to_byte_array(\"7.0\")\n        imaris_info.attrs[\"ThumbnailMode\"] = _string_to_byte_array(\"thumbnailMIP\")\n        imaris_info.attrs[\"ThumbnailSize\"] = _string_to_byte_array(\"256\")\n\n        # Create ImarisDataSet metadata\n        dataset_info = info_group.create_group(\"ImarisDataSet\")\n        dataset_info.attrs[\"Creator\"] = _string_to_byte_array(\"Imaris\")\n        dataset_info.attrs[\"Version\"] = _string_to_byte_array(\"7.0\")\n        dataset_info.attrs[\"NumberOfImages\"] = _string_to_byte_array(\"1\")\n\n        # Add version-specific groups as seen in reference\n        dataset_info_ver = info_group.create_group(\"ImarisDataSet       0.0.0\")\n        dataset_info_ver.attrs[\"NumberOfImages\"] = _string_to_byte_array(\"1\")\n        dataset_info_ver2 = info_group.create_group(\"ImarisDataSet      0.0.0\")\n        dataset_info_ver2.attrs[\"NumberOfImages\"] = _string_to_byte_array(\"1\")\n\n        # Create TimeInfo group\n        time_info = info_group.create_group(\"TimeInfo\")\n        time_info.attrs[\"DatasetTimePoints\"] = _string_to_byte_array(\"1\")\n        time_info.attrs[\"FileTimePoints\"] = _string_to_byte_array(\"1\")\n        time_info.attrs[\"TimePoint1\"] = _string_to_byte_array(\n            \"2024-01-01 00:00:00.000\"\n        )\n\n        # Create Log group (basic processing log)\n        log_group = info_group.create_group(\"Log\")\n        log_group.attrs[\"Entries\"] = _string_to_byte_array(\"1\")\n        log_group.attrs[\"Entry0\"] = _string_to_byte_array(\n            f\"&lt;ZarrNiiExport channels=\\\"{' '.join(['on'] * n_channels)}\\\"/&gt;\"\n        )\n\n        # Create thumbnail group with proper multi-channel thumbnail\n        thumbnail_group = f.create_group(\"Thumbnail\")\n\n        # Create a combined thumbnail (256x1024 for multi-channel as in reference)\n        if n_channels &gt; 1:\n            # Multi-channel thumbnail: concatenate channels horizontally\n            thumb_width = 256 * n_channels\n            thumbnail_data = np.zeros((256, thumb_width), dtype=np.uint8)\n\n            for c in range(n_channels):\n                # Downsample each channel to 256x256\n                channel_data = data[c]\n                # Take MIP (Maximum Intensity Projection) along Z\n                mip = np.max(channel_data, axis=0)\n                # Resize to 256x256 (simple decimation)\n                step_y = max(1, mip.shape[0] // 256)\n                step_x = max(1, mip.shape[1] // 256)\n                thumb_channel = mip[::step_y, ::step_x]\n\n                # Pad or crop to exactly 256x256\n                if thumb_channel.shape[0] &lt; 256 or thumb_channel.shape[1] &lt; 256:\n                    padded = np.zeros((256, 256), dtype=thumb_channel.dtype)\n                    h, w = thumb_channel.shape\n                    padded[:h, :w] = thumb_channel\n                    thumb_channel = padded\n                else:\n                    thumb_channel = thumb_channel[:256, :256]\n\n                # Place in thumbnail\n                thumbnail_data[:, c * 256 : (c + 1) * 256] = thumb_channel\n        else:\n            # Single channel: 256x256 thumbnail\n            channel_data = data[0]\n            mip = np.max(channel_data, axis=0)\n            step_y = max(1, mip.shape[0] // 256)\n            step_x = max(1, mip.shape[1] // 256)\n            thumbnail_data = mip[::step_y, ::step_x]\n\n            if thumbnail_data.shape[0] &lt; 256 or thumbnail_data.shape[1] &lt; 256:\n                padded = np.zeros((256, 256), dtype=thumbnail_data.dtype)\n                h, w = thumbnail_data.shape\n                padded[:h, :w] = thumbnail_data\n                thumbnail_data = padded\n            else:\n                thumbnail_data = thumbnail_data[:256, :256]\n\n        thumbnail_group.create_dataset(\"Data\", data=thumbnail_data.astype(np.uint8))\n\n    return path\n</code></pre>"},{"location":"reference/#crop","title":"<code>crop</code>","text":"<p>Extract a spatial region or multiple regions from the image.</p> <p>Crops the image to the specified bounding box coordinates, preserving all metadata and non-spatial dimensions (channels, time). The cropping is performed in voxel coordinates by default, or physical coordinates if specified. Can crop a single region or multiple regions at once.</p> <p>Parameters:</p> Name Type Description Default <code>bbox_min</code> <code>Union[Tuple[float, ...], List[Tuple[Tuple[float, ...], Tuple[float, ...]]]]</code> <p>Either: - Minimum corner coordinates of bounding box as tuple   (when bbox_max is provided). Length should match number of   spatial dimensions (x, y, z order) - List of (bbox_min, bbox_max) tuples for batch cropping   (when bbox_max is None)</p> required <code>bbox_max</code> <code>Optional[Tuple[float, ...]]</code> <p>Maximum corner coordinates of bounding box as tuple. Length should match number of spatial dimensions (x, y, z order). Should be None when bbox_min is a list of bounding boxes.</p> <code>None</code> <code>spatial_dims</code> <code>Optional[List[str]]</code> <p>Names of spatial dimensions to crop. If None, automatically derived from axes_order (\"z\",\"y\",\"x\" for ZYX or \"x\",\"y\",\"z\" for XYZ)</p> <code>None</code> <code>physical_coords</code> <code>bool</code> <p>If True, bbox_min and bbox_max are in physical/world coordinates (mm). If False, they are in voxel coordinates. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union['ZarrNii', List['ZarrNii']]</code> <p>New ZarrNii instance with cropped data (single crop) or list of</p> <code>Union['ZarrNii', List['ZarrNii']]</code> <p>ZarrNii instances (batch crop) with updated spatial metadata</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If bbox coordinates are invalid or out of bounds, or if both list and bbox_max are provided</p> <code>IndexError</code> <p>If bbox dimensions don't match spatial dimensions</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Crop 3D region (voxel coordinates)\n&gt;&gt;&gt; cropped = znii.crop((10, 20, 30), (110, 120, 130))\n</code></pre> <pre><code>&gt;&gt;&gt; # Crop with physical coordinates\n&gt;&gt;&gt; cropped = znii.crop((10.5, 20.5, 30.5), (110.5, 120.5, 130.5),\n...                      physical_coords=True)\n</code></pre> <pre><code>&gt;&gt;&gt; # Crop with explicit spatial dimensions\n&gt;&gt;&gt; cropped = znii.crop(\n...     (50, 60, 70), (150, 160, 170),\n...     spatial_dims=[\"x\", \"y\", \"z\"]\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Batch crop multiple regions\n&gt;&gt;&gt; bboxes = [\n...     ((10, 20, 30), (60, 70, 80)),\n...     ((100, 110, 120), (150, 160, 170))\n... ]\n&gt;&gt;&gt; cropped_list = znii.crop(bboxes, physical_coords=True)\n</code></pre> Notes <ul> <li>Coordinates are in voxel space (0-based indexing) by default</li> <li>Physical coordinates are in RAS orientation (Right-Anterior-Superior)</li> <li>The cropped region includes bbox_min but excludes bbox_max</li> <li>All non-spatial dimensions (channels, time) are preserved</li> <li>Spatial transformations are automatically updated</li> <li>When batch cropping, all patches share the same spatial_dims and   physical_coords settings</li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def crop(\n    self,\n    bbox_min: Union[\n        Tuple[float, ...], List[Tuple[Tuple[float, ...], Tuple[float, ...]]]\n    ],\n    bbox_max: Optional[Tuple[float, ...]] = None,\n    spatial_dims: Optional[List[str]] = None,\n    physical_coords: bool = False,\n) -&gt; Union[\"ZarrNii\", List[\"ZarrNii\"]]:\n    \"\"\"Extract a spatial region or multiple regions from the image.\n\n    Crops the image to the specified bounding box coordinates, preserving\n    all metadata and non-spatial dimensions (channels, time). The cropping\n    is performed in voxel coordinates by default, or physical coordinates\n    if specified. Can crop a single region or multiple regions at once.\n\n    Args:\n        bbox_min: Either:\n            - Minimum corner coordinates of bounding box as tuple\n              (when bbox_max is provided). Length should match number of\n              spatial dimensions (x, y, z order)\n            - List of (bbox_min, bbox_max) tuples for batch cropping\n              (when bbox_max is None)\n        bbox_max: Maximum corner coordinates of bounding box as tuple.\n            Length should match number of spatial dimensions (x, y, z order).\n            Should be None when bbox_min is a list of bounding boxes.\n        spatial_dims: Names of spatial dimensions to crop. If None,\n            automatically derived from axes_order (\"z\",\"y\",\"x\" for ZYX\n            or \"x\",\"y\",\"z\" for XYZ)\n        physical_coords: If True, bbox_min and bbox_max are in physical/world\n            coordinates (mm). If False, they are in voxel coordinates.\n            Default is False.\n\n    Returns:\n        New ZarrNii instance with cropped data (single crop) or list of\n        ZarrNii instances (batch crop) with updated spatial metadata\n\n    Raises:\n        ValueError: If bbox coordinates are invalid or out of bounds, or\n            if both list and bbox_max are provided\n        IndexError: If bbox dimensions don't match spatial dimensions\n\n    Examples:\n        &gt;&gt;&gt; # Crop 3D region (voxel coordinates)\n        &gt;&gt;&gt; cropped = znii.crop((10, 20, 30), (110, 120, 130))\n\n        &gt;&gt;&gt; # Crop with physical coordinates\n        &gt;&gt;&gt; cropped = znii.crop((10.5, 20.5, 30.5), (110.5, 120.5, 130.5),\n        ...                      physical_coords=True)\n\n        &gt;&gt;&gt; # Crop with explicit spatial dimensions\n        &gt;&gt;&gt; cropped = znii.crop(\n        ...     (50, 60, 70), (150, 160, 170),\n        ...     spatial_dims=[\"x\", \"y\", \"z\"]\n        ... )\n\n        &gt;&gt;&gt; # Batch crop multiple regions\n        &gt;&gt;&gt; bboxes = [\n        ...     ((10, 20, 30), (60, 70, 80)),\n        ...     ((100, 110, 120), (150, 160, 170))\n        ... ]\n        &gt;&gt;&gt; cropped_list = znii.crop(bboxes, physical_coords=True)\n\n    Notes:\n        - Coordinates are in voxel space (0-based indexing) by default\n        - Physical coordinates are in RAS orientation (Right-Anterior-Superior)\n        - The cropped region includes bbox_min but excludes bbox_max\n        - All non-spatial dimensions (channels, time) are preserved\n        - Spatial transformations are automatically updated\n        - When batch cropping, all patches share the same spatial_dims and\n          physical_coords settings\n    \"\"\"\n    # Check if this is batch cropping (list of bounding boxes)\n    # A batch crop is a list of (bbox_min, bbox_max) tuples\n    # Each element should be a tuple/list of two elements\n    is_batch_crop = (\n        isinstance(bbox_min, list)\n        and len(bbox_min) &gt; 0\n        and isinstance(bbox_min[0], (tuple, list))\n        and len(bbox_min[0]) == 2\n    )\n\n    if is_batch_crop:\n        if bbox_max is not None:\n            raise ValueError(\n                \"bbox_max should be None when bbox_min is a list of bounding boxes\"\n            )\n        # Batch crop: recursively call crop for each bounding box\n        return [\n            self.crop(bmin, bmax, spatial_dims, physical_coords)\n            for bmin, bmax in bbox_min\n        ]\n\n    # Single crop: original implementation\n    if bbox_max is None:\n        raise ValueError(\"bbox_max is required when bbox_min is not a list\")\n\n    if spatial_dims is None:\n        spatial_dims = (\n            [\"z\", \"y\", \"x\"] if self.axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n        )\n\n    # Convert physical coordinates to voxel coordinates if needed\n    if physical_coords:\n        # Physical coords are always in (x, y, z) order\n        # Convert to homogeneous coordinates\n        phys_min = np.array(list(bbox_min) + [1.0])\n        phys_max = np.array(list(bbox_max) + [1.0])\n\n        # Get inverse affine to convert from physical to voxel\n        affine_inv = np.linalg.inv(self.affine.matrix)\n\n        # Transform to voxel coordinates\n        voxel_min = affine_inv @ phys_min\n        voxel_max = affine_inv @ phys_max\n\n        # Extract voxel coordinates (x, y, z)\n        voxel_min_xyz = voxel_min[:3]\n        voxel_max_xyz = voxel_max[:3]\n\n        # Round to nearest integer voxel indices\n        voxel_min_xyz = np.round(voxel_min_xyz).astype(int)\n        voxel_max_xyz = np.round(voxel_max_xyz).astype(int)\n\n        # Ensure max &gt;= min\n        voxel_min_xyz = np.minimum(voxel_min_xyz, voxel_max_xyz)\n        voxel_max_xyz = np.maximum(\n            np.round(affine_inv @ phys_min).astype(int)[:3],\n            np.round(affine_inv @ phys_max).astype(int)[:3],\n        )\n\n        # Create mapping from x,y,z to voxel coordinates\n        xyz_to_voxel = {\n            \"x\": voxel_min_xyz[0],\n            \"y\": voxel_min_xyz[1],\n            \"z\": voxel_min_xyz[2],\n        }\n        xyz_to_voxel_max = {\n            \"x\": voxel_max_xyz[0],\n            \"y\": voxel_max_xyz[1],\n            \"z\": voxel_max_xyz[2],\n        }\n\n        # Reorder according to spatial_dims\n        bbox_min = tuple(xyz_to_voxel[dim.lower()] for dim in spatial_dims)\n        bbox_max = tuple(xyz_to_voxel_max[dim.lower()] for dim in spatial_dims)\n\n    cropped_image = crop_ngff_image(\n        self.ngff_image, bbox_min, bbox_max, spatial_dims\n    )\n    return ZarrNii(\n        ngff_image=cropped_image,\n        axes_order=self.axes_order,\n        xyz_orientation=self.xyz_orientation,\n        _omero=self._omero,\n    )\n</code></pre>"},{"location":"reference/#downsample","title":"<code>downsample</code>","text":"<p>Reduce image resolution by downsampling.</p> <p>Performs spatial downsampling by averaging blocks of voxels, effectively reducing image resolution and size. Multiple parameter options provide flexibility for different downsampling strategies.</p> <p>Parameters:</p> Name Type Description Default <code>factors</code> <code>Optional[Union[int, List[int]]]</code> <p>Downsampling factors for spatial dimensions. Can be: - int: Same factor applied to all spatial dimensions - List[int]: Per-dimension factors matching spatial_dims order - None: Use other parameters to determine factors</p> <code>None</code> <code>along_x</code> <code>int</code> <p>Downsampling factor for X dimension (legacy parameter)</p> <code>1</code> <code>along_y</code> <code>int</code> <p>Downsampling factor for Y dimension (legacy parameter)</p> <code>1</code> <code>along_z</code> <code>int</code> <p>Downsampling factor for Z dimension (legacy parameter)</p> <code>1</code> <code>level</code> <code>Optional[int]</code> <p>Power-of-2 downsampling level (factors = 2^level). Takes precedence over along_* parameters</p> <code>None</code> <code>spatial_dims</code> <code>Optional[List[str]]</code> <p>Names of spatial dimensions. If None, derived from axes_order</p> <code>None</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>New ZarrNii instance with downsampled data and updated metadata</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If conflicting parameters provided or invalid factors</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Isotropic downsampling by factor of 2\n&gt;&gt;&gt; downsampled = znii.downsample(factors=2)\n</code></pre> <pre><code>&gt;&gt;&gt; # Anisotropic downsampling\n&gt;&gt;&gt; downsampled = znii.downsample(factors=[1, 2, 2])\n</code></pre> <pre><code>&gt;&gt;&gt; # Using legacy parameters\n&gt;&gt;&gt; downsampled = znii.downsample(along_x=2, along_y=2, along_z=1)\n</code></pre> <pre><code>&gt;&gt;&gt; # Power-of-2 downsampling\n&gt;&gt;&gt; downsampled = znii.downsample(level=2)  # factors = 4\n</code></pre> Notes <ul> <li>Downsampling uses block averaging for anti-aliasing</li> <li>Spatial transformations are automatically scaled</li> <li>Non-spatial dimensions (channels, time) are preserved</li> <li>Original data remains unchanged (creates new instance)</li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def downsample(\n    self,\n    factors: Optional[Union[int, List[int]]] = None,\n    along_x: int = 1,\n    along_y: int = 1,\n    along_z: int = 1,\n    level: Optional[int] = None,\n    spatial_dims: Optional[List[str]] = None,\n) -&gt; \"ZarrNii\":\n    \"\"\"Reduce image resolution by downsampling.\n\n    Performs spatial downsampling by averaging blocks of voxels, effectively\n    reducing image resolution and size. Multiple parameter options provide\n    flexibility for different downsampling strategies.\n\n    Args:\n        factors: Downsampling factors for spatial dimensions. Can be:\n            - int: Same factor applied to all spatial dimensions\n            - List[int]: Per-dimension factors matching spatial_dims order\n            - None: Use other parameters to determine factors\n        along_x: Downsampling factor for X dimension (legacy parameter)\n        along_y: Downsampling factor for Y dimension (legacy parameter)\n        along_z: Downsampling factor for Z dimension (legacy parameter)\n        level: Power-of-2 downsampling level (factors = 2^level).\n            Takes precedence over along_* parameters\n        spatial_dims: Names of spatial dimensions. If None, derived\n            from axes_order\n\n    Returns:\n        New ZarrNii instance with downsampled data and updated metadata\n\n    Raises:\n        ValueError: If conflicting parameters provided or invalid factors\n\n    Examples:\n        &gt;&gt;&gt; # Isotropic downsampling by factor of 2\n        &gt;&gt;&gt; downsampled = znii.downsample(factors=2)\n\n        &gt;&gt;&gt; # Anisotropic downsampling\n        &gt;&gt;&gt; downsampled = znii.downsample(factors=[1, 2, 2])\n\n        &gt;&gt;&gt; # Using legacy parameters\n        &gt;&gt;&gt; downsampled = znii.downsample(along_x=2, along_y=2, along_z=1)\n\n        &gt;&gt;&gt; # Power-of-2 downsampling\n        &gt;&gt;&gt; downsampled = znii.downsample(level=2)  # factors = 4\n\n    Notes:\n        - Downsampling uses block averaging for anti-aliasing\n        - Spatial transformations are automatically scaled\n        - Non-spatial dimensions (channels, time) are preserved\n        - Original data remains unchanged (creates new instance)\n    \"\"\"\n    # Handle legacy parameters\n    if factors is None:\n        if level is not None:\n            factors = 2**level\n        else:\n            factors = (\n                [along_z, along_y, along_x]\n                if self.axes_order == \"ZYX\"\n                else [along_x, along_y, along_z]\n            )\n\n    if spatial_dims is None:\n        spatial_dims = (\n            [\"z\", \"y\", \"x\"] if self.axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n        )\n\n    downsampled_image = downsample_ngff_image(\n        self.ngff_image, factors, spatial_dims\n    )\n    return ZarrNii(\n        ngff_image=downsampled_image,\n        axes_order=self.axes_order,\n        xyz_orientation=self.xyz_orientation,\n        _omero=self._omero,\n    )\n</code></pre>"},{"location":"reference/#upsample","title":"<code>upsample</code>","text":"<p>Upsamples the ZarrNii instance using <code>scipy.ndimage.zoom</code>.</p> <p>Parameters:</p> Name Type Description Default <code>along_x</code> <code>int</code> <p>Upsampling factor along the X-axis (default: 1).</p> <code>1</code> <code>along_y</code> <code>int</code> <p>Upsampling factor along the Y-axis (default: 1).</p> <code>1</code> <code>along_z</code> <code>int</code> <p>Upsampling factor along the Z-axis (default: 1).</p> <code>1</code> <code>to_shape</code> <code>tuple</code> <p>Target shape for upsampling. Should include all dimensions                          (e.g., <code>(c, z, y, x)</code> for ZYX or <code>(c, x, y, z)</code> for XYZ).                          If provided, <code>along_x</code>, <code>along_y</code>, and <code>along_z</code> are ignored.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ZarrNii</code> <p>A new ZarrNii instance with the upsampled data and updated affine.</p> Notes <ul> <li>This method supports both direct scaling via <code>along_*</code> factors or target shape via <code>to_shape</code>.</li> <li>If <code>to_shape</code> is provided, chunk sizes and scaling factors are dynamically calculated.</li> <li>The affine matrix is updated to reflect the new voxel size after upsampling.</li> </ul> Example Source code in <code>zarrnii/core.py</code> <pre><code>def upsample(self, along_x=1, along_y=1, along_z=1, to_shape=None):\n    \"\"\"\n    Upsamples the ZarrNii instance using `scipy.ndimage.zoom`.\n\n    Parameters:\n        along_x (int, optional): Upsampling factor along the X-axis (default: 1).\n        along_y (int, optional): Upsampling factor along the Y-axis (default: 1).\n        along_z (int, optional): Upsampling factor along the Z-axis (default: 1).\n        to_shape (tuple, optional): Target shape for upsampling. Should include all dimensions\n                                     (e.g., `(c, z, y, x)` for ZYX or `(c, x, y, z)` for XYZ).\n                                     If provided, `along_x`, `along_y`, and `along_z` are ignored.\n\n    Returns:\n        ZarrNii: A new ZarrNii instance with the upsampled data and updated affine.\n\n    Notes:\n        - This method supports both direct scaling via `along_*` factors or target shape via `to_shape`.\n        - If `to_shape` is provided, chunk sizes and scaling factors are dynamically calculated.\n        - The affine matrix is updated to reflect the new voxel size after upsampling.\n\n    Example:\n        # Upsample with scaling factors\n        upsampled_znimg = znimg.upsample(along_x=2, along_y=2, along_z=2)\n\n        # Upsample to a specific shape\n        upsampled_znimg = znimg.upsample(to_shape=(1, 256, 256, 256))\n    \"\"\"\n    # Determine scaling and chunks based on input parameters\n    if to_shape is None:\n        if self.axes_order == \"XYZ\":\n            scaling = (1, along_x, along_y, along_z)\n        else:\n            scaling = (1, along_z, along_y, along_x)\n\n        chunks_out = tuple(\n            tuple(c * scale for c in chunks_i)\n            for chunks_i, scale in zip(self.data.chunks, scaling)\n        )\n    else:\n        chunks_out, scaling = self.__get_upsampled_chunks(to_shape)\n\n    # Define block-wise upsampling function\n    def zoom_blocks(x, block_info=None):\n        \"\"\"\n        Scales blocks to the desired size using `scipy.ndimage.zoom`.\n\n        Parameters:\n            x (np.ndarray): Input block data.\n            block_info (dict, optional): Metadata about the current block.\n\n        Returns:\n            np.ndarray: The upscaled block.\n        \"\"\"\n        # Calculate scaling factors based on input and output chunk shapes\n        scaling = tuple(\n            out_n / in_n\n            for out_n, in_n in zip(block_info[None][\"chunk-shape\"], x.shape)\n        )\n        return zoom(x, scaling, order=1, prefilter=False)\n\n    # Perform block-wise upsampling\n    darr_scaled = da.map_blocks(\n        zoom_blocks, self.data, dtype=self.data.dtype, chunks=chunks_out\n    )\n\n    # Update the affine matrix to reflect the new voxel size\n    if self.axes_order == \"XYZ\":\n        scaling_matrix = np.diag(\n            (1 / scaling[1], 1 / scaling[2], 1 / scaling[3], 1)\n        )\n    else:\n        scaling_matrix = np.diag(\n            (1 / scaling[-1], 1 / scaling[-2], 1 / scaling[-3], 1)\n        )\n    new_affine = AffineTransform.from_array(scaling_matrix @ self.affine.matrix)\n\n    # Create new NgffImage with upsampled data\n    dims = self.dims\n    if self.axes_order == \"XYZ\":\n        new_scale = {\n            dims[1]: self.scale[dims[1]] / scaling[1],\n            dims[2]: self.scale[dims[2]] / scaling[2],\n            dims[3]: self.scale[dims[3]] / scaling[3],\n        }\n    else:\n        new_scale = {\n            dims[1]: self.scale[dims[1]] / scaling[1],\n            dims[2]: self.scale[dims[2]] / scaling[2],\n            dims[3]: self.scale[dims[3]] / scaling[3],\n        }\n\n    upsampled_ngff = nz.to_ngff_image(\n        darr_scaled,\n        dims=dims,\n        scale=new_scale,\n        translation=self.translation.copy(),\n        name=self.name,\n    )\n\n    # Return a new ZarrNii instance with the upsampled data\n    return ZarrNii.from_ngff_image(\n        upsampled_ngff,\n        axes_order=self.axes_order,\n        xyz_orientation=self.xyz_orientation,\n        omero=self.omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.upsample--upsample-with-scaling-factors","title":"Upsample with scaling factors","text":"<p>upsampled_znimg = znimg.upsample(along_x=2, along_y=2, along_z=2)</p>"},{"location":"reference/#zarrnii.ZarrNii.upsample--upsample-to-a-specific-shape","title":"Upsample to a specific shape","text":"<p>upsampled_znimg = znimg.upsample(to_shape=(1, 256, 256, 256))</p>"},{"location":"reference/#apply_transform","title":"<code>apply_transform</code>","text":"<p>Apply spatial transformations to image data.</p> <p>Transforms the image data to align with a reference image space using the provided transformation(s). This enables registration, resampling, and coordinate system conversions.</p> <p>Parameters:</p> Name Type Description Default <code>*transforms</code> <code>Transform</code> <p>Variable number of Transform objects to apply sequentially. Supported transform types: - AffineTransform: Linear transformations (rotation, scaling, translation) - DisplacementTransform: Non-linear deformation fields</p> <code>()</code> <code>ref_znimg</code> <code>'ZarrNii'</code> <p>Reference ZarrNii image defining the target coordinate system, grid spacing, and field of view for the output</p> required <code>spatial_dims</code> <code>Optional[List[str]]</code> <p>Names of spatial dimensions for transformation. If None, automatically derived from axes_order</p> <code>None</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>New ZarrNii instance with transformed data in reference space</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no transforms provided or reference image incompatible</p> <code>TypeError</code> <p>If transforms are not valid Transform objects</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Apply affine transformation\n&gt;&gt;&gt; affine = AffineTransform.from_txt(\"transform.txt\")\n&gt;&gt;&gt; transformed = moving.apply_transform(affine, ref_znimg=reference)\n</code></pre> <pre><code>&gt;&gt;&gt; # Apply multiple transforms sequentially\n&gt;&gt;&gt; affine = AffineTransform.identity()\n&gt;&gt;&gt; warp = DisplacementTransform.from_nifti(\"warp.nii.gz\")\n&gt;&gt;&gt; result = moving.apply_transform(affine, warp, ref_znimg=reference)\n</code></pre> Notes <ul> <li>Transformations are applied in the order specified</li> <li>Output data inherits spatial properties from ref_znimg</li> <li>Uses interpolation for non-integer coordinate mappings</li> <li>Non-spatial dimensions (channels, time) are preserved</li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def apply_transform(\n    self,\n    *transforms: Transform,\n    ref_znimg: \"ZarrNii\",\n    spatial_dims: Optional[List[str]] = None,\n) -&gt; \"ZarrNii\":\n    \"\"\"Apply spatial transformations to image data.\n\n    Transforms the image data to align with a reference image space using\n    the provided transformation(s). This enables registration, resampling,\n    and coordinate system conversions.\n\n    Args:\n        *transforms: Variable number of Transform objects to apply sequentially.\n            Supported transform types:\n            - AffineTransform: Linear transformations (rotation, scaling, translation)\n            - DisplacementTransform: Non-linear deformation fields\n        ref_znimg: Reference ZarrNii image defining the target coordinate system,\n            grid spacing, and field of view for the output\n        spatial_dims: Names of spatial dimensions for transformation. If None,\n            automatically derived from axes_order\n\n    Returns:\n        New ZarrNii instance with transformed data in reference space\n\n    Raises:\n        ValueError: If no transforms provided or reference image incompatible\n        TypeError: If transforms are not valid Transform objects\n\n    Examples:\n        &gt;&gt;&gt; # Apply affine transformation\n        &gt;&gt;&gt; affine = AffineTransform.from_txt(\"transform.txt\")\n        &gt;&gt;&gt; transformed = moving.apply_transform(affine, ref_znimg=reference)\n\n        &gt;&gt;&gt; # Apply multiple transforms sequentially\n        &gt;&gt;&gt; affine = AffineTransform.identity()\n        &gt;&gt;&gt; warp = DisplacementTransform.from_nifti(\"warp.nii.gz\")\n        &gt;&gt;&gt; result = moving.apply_transform(affine, warp, ref_znimg=reference)\n\n    Notes:\n        - Transformations are applied in the order specified\n        - Output data inherits spatial properties from ref_znimg\n        - Uses interpolation for non-integer coordinate mappings\n        - Non-spatial dimensions (channels, time) are preserved\n    \"\"\"\n    if spatial_dims is None:\n        spatial_dims = (\n            [\"z\", \"y\", \"x\"] if self.axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n        )\n\n    # For now, just apply the first transform (placeholder)\n    if transforms:\n        transformed_image = apply_transform_to_ngff_image(\n            self.ngff_image, transforms[0], ref_znimg.ngff_image, spatial_dims\n        )\n    else:\n        transformed_image = self.ngff_image\n\n    return ZarrNii(\n        ngff_image=transformed_image,\n        axes_order=self.axes_order,\n        xyz_orientation=self.xyz_orientation,\n        _omero=self._omero,\n    )\n</code></pre>"},{"location":"reference/#remaining-methods-and-functions","title":"Remaining Methods and Functions","text":"<p>Zarr-based image with NIfTI compatibility using NgffImage internally.</p> <p>This class provides chainable operations on OME-Zarr data while maintaining compatibility with NIfTI workflows. It uses NgffImage objects internally for better multiscale support and metadata preservation.</p> <p>Attributes:</p> Name Type Description <code>ngff_image</code> <code>NgffImage</code> <p>The internal NgffImage object containing data and metadata.</p> <code>axes_order</code> <code>str</code> <p>The order of the axes for NIfTI compatibility ('ZYX' or 'XYZ').</p> <code>xyz_orientation</code> <code>str</code> <p>The anatomical orientation string in XYZ axes order (e.g., 'RAS', 'LPI').</p> <p>Constructor with backward compatibility for old signature.</p> Source code in <code>zarrnii/core.py</code> <pre><code>def __init__(\n    self,\n    darr=None,\n    affine=None,\n    axes_order=\"ZYX\",\n    orientation=\"RAS\",\n    xyz_orientation=None,\n    ngff_image=None,\n    _omero=None,\n    **kwargs,\n):\n    \"\"\"\n    Constructor with backward compatibility for old signature.\n    \"\"\"\n    # Handle backwards compatibility: if xyz_orientation is provided, use it\n    # Otherwise, use orientation for backwards compatibility\n    final_orientation = (\n        xyz_orientation if xyz_orientation is not None else orientation\n    )\n\n    if ngff_image is not None:\n        # New signature\n        object.__setattr__(self, \"ngff_image\", ngff_image)\n        object.__setattr__(self, \"axes_order\", axes_order)\n        object.__setattr__(self, \"xyz_orientation\", final_orientation)\n        object.__setattr__(self, \"_omero\", _omero)\n    elif darr is not None:\n        # Legacy signature - delegate to from_darr\n        instance = self.from_darr(\n            darr=darr,\n            affine=affine,\n            axes_order=axes_order,\n            orientation=final_orientation,\n            **kwargs,\n        )\n        object.__setattr__(self, \"ngff_image\", instance.ngff_image)\n        object.__setattr__(self, \"axes_order\", instance.axes_order)\n        object.__setattr__(self, \"xyz_orientation\", instance.xyz_orientation)\n        object.__setattr__(self, \"_omero\", instance._omero)\n    else:\n        raise ValueError(\"Must provide either ngff_image or darr\")\n</code></pre> <p>               Bases: <code>Transform</code></p> <p>Affine transformation for spatial coordinate mapping.</p> <p>Represents a 4x4 affine transformation matrix that can be used to transform 3D coordinates between different coordinate systems. Supports various operations including matrix multiplication, inversion, and point transformation.</p> <p>Attributes:</p> Name Type Description <code>matrix</code> <code>ndarray</code> <p>4x4 affine transformation matrix</p> <p>               Bases: <code>Transform</code></p> <p>Non-linear displacement field transformation.</p> <p>Represents a displacement field transformation where each point in space has an associated displacement vector. Uses interpolation to compute displacements for arbitrary coordinates.</p> <p>Attributes:</p> Name Type Description <code>disp_xyz</code> <code>ndarray</code> <p>Displacement vectors at grid points (4D array: x, y, z, vector_component)</p> <code>disp_grid</code> <code>Tuple[ndarray, ...]</code> <p>Grid coordinates for displacement field</p> <code>disp_affine</code> <code>AffineTransform</code> <p>Affine transformation from world to displacement field coordinates</p>"},{"location":"reference/#zarrnii.ZarrNii._omero","title":"<code>_omero = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/#zarrnii.ZarrNii.affine","title":"<code>affine</code>  <code>property</code>","text":"<p>Affine transformation matrix derived from NgffImage scale and translation.</p> <p>Returns:</p> Name Type Description <code>AffineTransform</code> <code>AffineTransform</code> <p>4x4 affine transformation matrix</p>"},{"location":"reference/#zarrnii.ZarrNii.axes","title":"<code>axes</code>  <code>property</code>","text":"<p>Axes metadata - derived from NgffImage for compatibility.</p>"},{"location":"reference/#zarrnii.ZarrNii.axes_order","title":"<code>axes_order = 'ZYX'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/#zarrnii.ZarrNii.coordinate_transformations","title":"<code>coordinate_transformations</code>  <code>property</code>","text":"<p>Coordinate transformations - derived from NgffImage scale/translation.</p>"},{"location":"reference/#zarrnii.ZarrNii.darr","title":"<code>darr</code>  <code>property</code> <code>writable</code>","text":"<p>Legacy property name for image data.</p>"},{"location":"reference/#zarrnii.ZarrNii.data","title":"<code>data</code>  <code>property</code> <code>writable</code>","text":"<p>Access the image data (dask array).</p>"},{"location":"reference/#zarrnii.ZarrNii.dims","title":"<code>dims</code>  <code>property</code>","text":"<p>Dimension names.</p>"},{"location":"reference/#zarrnii.ZarrNii.name","title":"<code>name</code>  <code>property</code>","text":"<p>Image name from NgffImage.</p>"},{"location":"reference/#zarrnii.ZarrNii.ngff_image","title":"<code>ngff_image</code>  <code>instance-attribute</code>","text":""},{"location":"reference/#zarrnii.ZarrNii.omero","title":"<code>omero</code>  <code>property</code>","text":"<p>Omero metadata object.</p>"},{"location":"reference/#zarrnii.ZarrNii.orientation","title":"<code>orientation</code>  <code>property</code> <code>writable</code>","text":"<p>Legacy property for backward compatibility.</p> <p>Returns the xyz_orientation attribute to maintain backward compatibility with code that expects the 'orientation' property.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The anatomical orientation string in XYZ axes order</p>"},{"location":"reference/#zarrnii.ZarrNii.scale","title":"<code>scale</code>  <code>property</code>","text":"<p>Scale information from NgffImage.</p>"},{"location":"reference/#zarrnii.ZarrNii.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>Shape of the image data.</p>"},{"location":"reference/#zarrnii.ZarrNii.translation","title":"<code>translation</code>  <code>property</code>","text":"<p>Translation information from NgffImage.</p>"},{"location":"reference/#zarrnii.ZarrNii.xyz_orientation","title":"<code>xyz_orientation = 'RAS'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/#zarrnii.ZarrNii.__get_upsampled_chunks","title":"<code>__get_upsampled_chunks(target_shape, return_scaling=True)</code>","text":"<p>Calculates new chunk sizes for a dask array to match a target shape, while ensuring the chunks sum precisely to the target shape. Optionally, returns the scaling factors for each dimension.</p> <p>This method is useful for upsampling data or ensuring 1:1 correspondence between downsampled and upsampled arrays.</p> <p>Parameters:</p> Name Type Description Default <code>target_shape</code> <code>tuple</code> <p>The desired shape of the array after upsampling.</p> required <code>return_scaling</code> <code>bool</code> <p>Whether to return the scaling factors                              for each dimension (default: True).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>new_chunks (tuple): A tuple of tuples specifying the new chunk sizes                     for each dimension. scaling (list): A list of scaling factors for each dimension                 (only if <code>return_scaling=True</code>).</p> <p>OR</p> <code>tuple</code> <p>new_chunks (tuple): A tuple of tuples specifying the new chunk sizes                     for each dimension (if <code>return_scaling=False</code>).</p> Notes <ul> <li>The scaling factor for each dimension is calculated as:   <code>scaling_factor = target_shape[dim] / original_shape[dim]</code></li> <li>The last chunk in each dimension is adjusted to account for rounding   errors, ensuring the sum of chunks matches the target shape.</li> </ul> Example Source code in <code>zarrnii/core.py</code> <pre><code>def __get_upsampled_chunks(self, target_shape, return_scaling=True):\n    \"\"\"\n    Calculates new chunk sizes for a dask array to match a target shape,\n    while ensuring the chunks sum precisely to the target shape. Optionally,\n    returns the scaling factors for each dimension.\n\n    This method is useful for upsampling data or ensuring 1:1 correspondence\n    between downsampled and upsampled arrays.\n\n    Parameters:\n        target_shape (tuple): The desired shape of the array after upsampling.\n        return_scaling (bool, optional): Whether to return the scaling factors\n                                         for each dimension (default: True).\n\n    Returns:\n        tuple:\n            new_chunks (tuple): A tuple of tuples specifying the new chunk sizes\n                                for each dimension.\n            scaling (list): A list of scaling factors for each dimension\n                            (only if `return_scaling=True`).\n\n        OR\n\n        tuple:\n            new_chunks (tuple): A tuple of tuples specifying the new chunk sizes\n                                for each dimension (if `return_scaling=False`).\n\n    Notes:\n        - The scaling factor for each dimension is calculated as:\n          `scaling_factor = target_shape[dim] / original_shape[dim]`\n        - The last chunk in each dimension is adjusted to account for rounding\n          errors, ensuring the sum of chunks matches the target shape.\n\n    Example:\n        # Calculate upsampled chunks and scaling factors\n        new_chunks, scaling = znimg.__get_upsampled_chunks((256, 256, 256))\n        print(\"New chunks:\", new_chunks)\n        print(\"Scaling factors:\", scaling)\n\n        # Calculate only the new chunks\n        new_chunks = znimg.__get_upsampled_chunks((256, 256, 256), return_scaling=False)\n    \"\"\"\n    new_chunks = []\n    scaling = []\n\n    for dim, (orig_shape, orig_chunks, new_shape) in enumerate(\n        zip(self.data.shape, self.data.chunks, target_shape)\n    ):\n        # Calculate the scaling factor for this dimension\n        scaling_factor = new_shape / orig_shape\n\n        # Scale each chunk size and round to get an initial estimate\n        scaled_chunks = [\n            int(round(chunk * scaling_factor)) for chunk in orig_chunks\n        ]\n        total = sum(scaled_chunks)\n\n        # Adjust the chunks to ensure they sum up to the target shape exactly\n        diff = new_shape - total\n        if diff != 0:\n            # Correct rounding errors by adjusting the last chunk size in the dimension\n            scaled_chunks[-1] += diff\n\n        new_chunks.append(tuple(scaled_chunks))\n        scaling.append(scaling_factor)\n\n    if return_scaling:\n        return tuple(new_chunks), scaling\n    else:\n        return tuple(new_chunks)\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.__get_upsampled_chunks--calculate-upsampled-chunks-and-scaling-factors","title":"Calculate upsampled chunks and scaling factors","text":"<p>new_chunks, scaling = znimg.__get_upsampled_chunks((256, 256, 256)) print(\"New chunks:\", new_chunks) print(\"Scaling factors:\", scaling)</p>"},{"location":"reference/#zarrnii.ZarrNii.__get_upsampled_chunks--calculate-only-the-new-chunks","title":"Calculate only the new chunks","text":"<p>new_chunks = znimg.__get_upsampled_chunks((256, 256, 256), return_scaling=False)</p>"},{"location":"reference/#zarrnii.ZarrNii.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation.</p> Source code in <code>zarrnii/core.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation.\"\"\"\n    return (\n        f\"ZarrNii(name='{self.name}', \"\n        f\"shape={self.shape}, \"\n        f\"dims={self.dims}, \"\n        f\"scale={self.scale})\"\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii._create_zyx_ngff_image","title":"<code>_create_zyx_ngff_image()</code>","text":"<p>Create a new NgffImage with data reordered from XYZ to ZYX.</p> <p>This is used when saving to OME-Zarr format which expects ZYX ordering. The data array is transposed and scale/translation are reordered accordingly.</p> <p>Returns:</p> Name Type Description <code>NgffImage</code> <code>NgffImage</code> <p>New image with ZYX-ordered data and metadata</p> Source code in <code>zarrnii/core.py</code> <pre><code>def _create_zyx_ngff_image(self) -&gt; nz.NgffImage:\n    \"\"\"\n    Create a new NgffImage with data reordered from XYZ to ZYX.\n\n    This is used when saving to OME-Zarr format which expects ZYX ordering.\n    The data array is transposed and scale/translation are reordered accordingly.\n\n    Returns:\n        NgffImage: New image with ZYX-ordered data and metadata\n    \"\"\"\n    if self.axes_order != \"XYZ\":\n        raise ValueError(\"This method should only be called when axes_order is XYZ\")\n\n    # Transpose data from XYZ to ZYX (reverse the spatial dimensions)\n    # Assuming data shape is [C, X, Y, Z] -&gt; [C, Z, Y, X]\n    data = self.ngff_image.data\n\n    # Find spatial dimension indices\n    spatial_axes = []\n    channel_axes = []\n    for i, dim_name in enumerate(self.ngff_image.dims):\n        if dim_name.lower() in [\"x\", \"y\", \"z\"]:\n            spatial_axes.append(i)\n        else:\n            channel_axes.append(i)\n\n    # Create transpose indices: reverse the spatial axes order\n    transpose_indices = channel_axes + spatial_axes[::-1]\n    transposed_data = data.transpose(transpose_indices)\n\n    # Create new dims list with ZYX ordering\n    new_dims = []\n    for i, dim_name in enumerate(self.ngff_image.dims):\n        if dim_name.lower() not in [\"x\", \"y\", \"z\"]:\n            new_dims.append(dim_name)\n    # Add spatial dims in ZYX order\n    spatial_dim_names = [self.ngff_image.dims[i] for i in spatial_axes]\n    new_dims.extend(spatial_dim_names[::-1])\n\n    # Reorder scale and translation from XYZ to ZYX\n    current_scale = self.ngff_image.scale\n    current_translation = self.ngff_image.translation\n\n    new_scale = {}\n    new_translation = {}\n\n    # Copy non-spatial dimensions\n    for key, value in current_scale.items():\n        if key.lower() not in [\"x\", \"y\", \"z\"]:\n            new_scale[key] = value\n\n    for key, value in current_translation.items():\n        if key.lower() not in [\"x\", \"y\", \"z\"]:\n            new_translation[key] = value\n\n    # Reorder spatial dimensions from XYZ to ZYX\n    if \"x\" in current_scale and \"y\" in current_scale and \"z\" in current_scale:\n        new_scale[\"z\"] = current_scale[\"z\"]\n        new_scale[\"y\"] = current_scale[\"y\"]\n        new_scale[\"x\"] = current_scale[\"x\"]\n\n    if (\n        \"x\" in current_translation\n        and \"y\" in current_translation\n        and \"z\" in current_translation\n    ):\n        new_translation[\"z\"] = current_translation[\"z\"]\n        new_translation[\"y\"] = current_translation[\"y\"]\n        new_translation[\"x\"] = current_translation[\"x\"]\n\n    # Create new NgffImage with ZYX ordering\n    zyx_image = nz.NgffImage(\n        data=transposed_data,\n        dims=new_dims,\n        scale=new_scale,\n        translation=new_translation,\n        name=self.ngff_image.name,\n    )\n\n    return zyx_image\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.apply_scaled_processing","title":"<code>apply_scaled_processing(plugin, downsample_factor=4, chunk_size=None, upsampled_ome_zarr_path=None, **kwargs)</code>","text":"<p>Apply scaled processing plugin using multi-resolution approach.</p> <p>This method implements a multi-resolution processing pipeline where: 1. The image is downsampled for efficient computation 2. The plugin's lowres_func is applied to the downsampled data 3. The result is upsampled using dask-based upsampling 4. The plugin's highres_func applies the result to full-resolution data</p> <p>Parameters:</p> Name Type Description Default <code>plugin</code> <p>ScaledProcessingPlugin instance or class to apply</p> required <code>downsample_factor</code> <code>int</code> <p>Factor for downsampling (default: 4)</p> <code>4</code> <code>chunk_size</code> <code>Optional[Tuple[int, ...]]</code> <p>Optional chunk size for low-res processing. If None, uses (1, 10, 10, 10).</p> <code>None</code> <code>upsampled_ome_zarr_path</code> <code>Optional[str]</code> <p>Path to save intermediate OME-Zarr, default saved in system temp directory.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to the plugin</p> <code>{}</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>New ZarrNii instance with processed data</p> Source code in <code>zarrnii/core.py</code> <pre><code>def apply_scaled_processing(\n    self,\n    plugin,\n    downsample_factor: int = 4,\n    chunk_size: Optional[Tuple[int, ...]] = None,\n    upsampled_ome_zarr_path: Optional[str] = None,\n    **kwargs,\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Apply scaled processing plugin using multi-resolution approach.\n\n    This method implements a multi-resolution processing pipeline where:\n    1. The image is downsampled for efficient computation\n    2. The plugin's lowres_func is applied to the downsampled data\n    3. The result is upsampled using dask-based upsampling\n    4. The plugin's highres_func applies the result to full-resolution data\n\n    Args:\n        plugin: ScaledProcessingPlugin instance or class to apply\n        downsample_factor: Factor for downsampling (default: 4)\n        chunk_size: Optional chunk size for low-res processing. If None, uses (1, 10, 10, 10).\n        upsampled_ome_zarr_path: Path to save intermediate OME-Zarr, default saved in system temp directory.\n        **kwargs: Additional arguments passed to the plugin\n\n    Returns:\n        New ZarrNii instance with processed data\n    \"\"\"\n    from .plugins.scaled_processing import ScaledProcessingPlugin\n\n    # Handle plugin instance or class\n    if isinstance(plugin, type) and issubclass(plugin, ScaledProcessingPlugin):\n        plugin = plugin(**kwargs)\n    elif not isinstance(plugin, ScaledProcessingPlugin):\n        raise TypeError(\n            \"Plugin must be an instance or subclass of ScaledProcessingPlugin\"\n        )\n\n    # Step 1: Downsample the data for low-resolution processing\n    lowres_znimg = self.downsample(level=int(np.log2(downsample_factor)))\n\n    # Convert to numpy array for lowres processing\n    lowres_array = lowres_znimg.data.compute()\n\n    # Step 2: Apply low-resolution function and prepare for upsampling\n    # Use chunk_size parameter for the low-res processing chunks\n    lowres_chunks = chunk_size if chunk_size is not None else (1, 10, 10, 10)\n    lowres_znimg.data = da.from_array(\n        plugin.lowres_func(lowres_array), chunks=lowres_chunks\n    )\n\n    # Use temporary OME-Zarr to break up dask graph for performance\n    import tempfile\n\n    if upsampled_ome_zarr_path is None:\n        upsampled_ome_zarr_path = tempfile.mkdtemp(suffix=\"_SPIM.ome.zarr\")\n\n    # Step 3: Upsample using dask-based upsampling, save to ome zarr\n    lowres_znimg.upsample(to_shape=self.shape).to_ome_zarr(\n        upsampled_ome_zarr_path, max_layer=0\n    )\n\n    upsampled_znimg = ZarrNii.from_ome_zarr(upsampled_ome_zarr_path)\n\n    corrected_znimg = self.copy()\n\n    # Step 4: Apply high-resolution function\n    # rechunk original data to use same chunksize as upsampled_data, before multiplying\n    corrected_znimg.data = plugin.highres_func(\n        self.data.rechunk(upsampled_znimg.data.chunks), upsampled_znimg.data\n    )\n\n    return corrected_znimg\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.apply_transform","title":"<code>apply_transform(*transforms, ref_znimg, spatial_dims=None)</code>","text":"<p>Apply spatial transformations to image data.</p> <p>Transforms the image data to align with a reference image space using the provided transformation(s). This enables registration, resampling, and coordinate system conversions.</p> <p>Parameters:</p> Name Type Description Default <code>*transforms</code> <code>Transform</code> <p>Variable number of Transform objects to apply sequentially. Supported transform types: - AffineTransform: Linear transformations (rotation, scaling, translation) - DisplacementTransform: Non-linear deformation fields</p> <code>()</code> <code>ref_znimg</code> <code>'ZarrNii'</code> <p>Reference ZarrNii image defining the target coordinate system, grid spacing, and field of view for the output</p> required <code>spatial_dims</code> <code>Optional[List[str]]</code> <p>Names of spatial dimensions for transformation. If None, automatically derived from axes_order</p> <code>None</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>New ZarrNii instance with transformed data in reference space</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no transforms provided or reference image incompatible</p> <code>TypeError</code> <p>If transforms are not valid Transform objects</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Apply affine transformation\n&gt;&gt;&gt; affine = AffineTransform.from_txt(\"transform.txt\")\n&gt;&gt;&gt; transformed = moving.apply_transform(affine, ref_znimg=reference)\n</code></pre> <pre><code>&gt;&gt;&gt; # Apply multiple transforms sequentially\n&gt;&gt;&gt; affine = AffineTransform.identity()\n&gt;&gt;&gt; warp = DisplacementTransform.from_nifti(\"warp.nii.gz\")\n&gt;&gt;&gt; result = moving.apply_transform(affine, warp, ref_znimg=reference)\n</code></pre> Notes <ul> <li>Transformations are applied in the order specified</li> <li>Output data inherits spatial properties from ref_znimg</li> <li>Uses interpolation for non-integer coordinate mappings</li> <li>Non-spatial dimensions (channels, time) are preserved</li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def apply_transform(\n    self,\n    *transforms: Transform,\n    ref_znimg: \"ZarrNii\",\n    spatial_dims: Optional[List[str]] = None,\n) -&gt; \"ZarrNii\":\n    \"\"\"Apply spatial transformations to image data.\n\n    Transforms the image data to align with a reference image space using\n    the provided transformation(s). This enables registration, resampling,\n    and coordinate system conversions.\n\n    Args:\n        *transforms: Variable number of Transform objects to apply sequentially.\n            Supported transform types:\n            - AffineTransform: Linear transformations (rotation, scaling, translation)\n            - DisplacementTransform: Non-linear deformation fields\n        ref_znimg: Reference ZarrNii image defining the target coordinate system,\n            grid spacing, and field of view for the output\n        spatial_dims: Names of spatial dimensions for transformation. If None,\n            automatically derived from axes_order\n\n    Returns:\n        New ZarrNii instance with transformed data in reference space\n\n    Raises:\n        ValueError: If no transforms provided or reference image incompatible\n        TypeError: If transforms are not valid Transform objects\n\n    Examples:\n        &gt;&gt;&gt; # Apply affine transformation\n        &gt;&gt;&gt; affine = AffineTransform.from_txt(\"transform.txt\")\n        &gt;&gt;&gt; transformed = moving.apply_transform(affine, ref_znimg=reference)\n\n        &gt;&gt;&gt; # Apply multiple transforms sequentially\n        &gt;&gt;&gt; affine = AffineTransform.identity()\n        &gt;&gt;&gt; warp = DisplacementTransform.from_nifti(\"warp.nii.gz\")\n        &gt;&gt;&gt; result = moving.apply_transform(affine, warp, ref_znimg=reference)\n\n    Notes:\n        - Transformations are applied in the order specified\n        - Output data inherits spatial properties from ref_znimg\n        - Uses interpolation for non-integer coordinate mappings\n        - Non-spatial dimensions (channels, time) are preserved\n    \"\"\"\n    if spatial_dims is None:\n        spatial_dims = (\n            [\"z\", \"y\", \"x\"] if self.axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n        )\n\n    # For now, just apply the first transform (placeholder)\n    if transforms:\n        transformed_image = apply_transform_to_ngff_image(\n            self.ngff_image, transforms[0], ref_znimg.ngff_image, spatial_dims\n        )\n    else:\n        transformed_image = self.ngff_image\n\n    return ZarrNii(\n        ngff_image=transformed_image,\n        axes_order=self.axes_order,\n        xyz_orientation=self.xyz_orientation,\n        _omero=self._omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.apply_transform_flo_to_ref_indices","title":"<code>apply_transform_flo_to_ref_indices(*transforms, ref_znimg, indices)</code>","text":"<p>Transform indices from floating to reference space.</p> Source code in <code>zarrnii/core.py</code> <pre><code>def apply_transform_flo_to_ref_indices(self, *transforms, ref_znimg, indices):\n    \"\"\"Transform indices from floating to reference space.\"\"\"\n    # Placeholder implementation - would need full transform logic\n    return indices\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.apply_transform_ref_to_flo_indices","title":"<code>apply_transform_ref_to_flo_indices(*transforms, ref_znimg, indices)</code>","text":"<p>Transform indices from reference to floating space.</p> Source code in <code>zarrnii/core.py</code> <pre><code>def apply_transform_ref_to_flo_indices(self, *transforms, ref_znimg, indices):\n    \"\"\"Transform indices from reference to floating space.\"\"\"\n    # Placeholder implementation - would need full transform logic\n    return indices\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.compute","title":"<code>compute()</code>","text":"<p>Compute the dask array and return the underlying NgffImage.</p> <p>This triggers computation of any lazy operations and returns the NgffImage with computed data.</p> <p>Returns:</p> Type Description <code>NgffImage</code> <p>NgffImage with computed data</p> Source code in <code>zarrnii/core.py</code> <pre><code>def compute(self) -&gt; nz.NgffImage:\n    \"\"\"\n    Compute the dask array and return the underlying NgffImage.\n\n    This triggers computation of any lazy operations and returns\n    the NgffImage with computed data.\n\n    Returns:\n        NgffImage with computed data\n    \"\"\"\n    computed_data = self.ngff_image.data.compute()\n\n    # Create new NgffImage with computed data\n    computed_image = nz.NgffImage(\n        data=computed_data,\n        dims=self.ngff_image.dims,\n        scale=self.ngff_image.scale,\n        translation=self.ngff_image.translation,\n        name=self.ngff_image.name,\n    )\n    return computed_image\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.compute_histogram","title":"<code>compute_histogram(bins=256, range=None, mask=None, **kwargs)</code>","text":"<p>Compute histogram of the image.</p> <p>This method computes the histogram of image intensities, optionally using a mask to weight the computation. The histogram is computed using dask for efficient processing of large datasets.</p> <p>Parameters:</p> Name Type Description Default <code>bins</code> <code>int</code> <p>Number of histogram bins (default: 256)</p> <code>256</code> <code>range</code> <code>Optional[Tuple[float, float]]</code> <p>Optional tuple (min, max) defining histogram range. If None, uses the full range of the data</p> <code>None</code> <code>mask</code> <code>Optional['ZarrNii']</code> <p>Optional ZarrNii mask of same shape as image. Only pixels where mask &gt; 0 are included in histogram computation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to dask.array.histogram</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>Tuple of (histogram_counts, bin_edges) where:</p> <code>Array</code> <ul> <li>histogram_counts: dask array of histogram bin counts</li> </ul> <code>Tuple[Array, Array]</code> <ul> <li>bin_edges: dask array of bin edge values (length = bins + 1)</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Compute histogram\n&gt;&gt;&gt; hist, bin_edges = znimg.compute_histogram(bins=128)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Compute histogram with mask\n&gt;&gt;&gt; mask = znimg &gt; 0.5\n&gt;&gt;&gt; hist_masked, _ = znimg.compute_histogram(mask=mask)\n</code></pre> Source code in <code>zarrnii/core.py</code> <pre><code>def compute_histogram(\n    self,\n    bins: int = 256,\n    range: Optional[Tuple[float, float]] = None,\n    mask: Optional[\"ZarrNii\"] = None,\n    **kwargs: Any,\n) -&gt; Tuple[da.Array, da.Array]:\n    \"\"\"\n    Compute histogram of the image.\n\n    This method computes the histogram of image intensities, optionally using\n    a mask to weight the computation. The histogram is computed using dask for\n    efficient processing of large datasets.\n\n    Args:\n        bins: Number of histogram bins (default: 256)\n        range: Optional tuple (min, max) defining histogram range. If None,\n            uses the full range of the data\n        mask: Optional ZarrNii mask of same shape as image. Only pixels\n            where mask &gt; 0 are included in histogram computation\n        **kwargs: Additional arguments passed to dask.array.histogram\n\n    Returns:\n        Tuple of (histogram_counts, bin_edges) where:\n        - histogram_counts: dask array of histogram bin counts\n        - bin_edges: dask array of bin edge values (length = bins + 1)\n\n    Examples:\n        &gt;&gt;&gt; # Compute histogram\n        &gt;&gt;&gt; hist, bin_edges = znimg.compute_histogram(bins=128)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Compute histogram with mask\n        &gt;&gt;&gt; mask = znimg &gt; 0.5\n        &gt;&gt;&gt; hist_masked, _ = znimg.compute_histogram(mask=mask)\n    \"\"\"\n    from .analysis import compute_histogram\n\n    mask_data = mask.darr if mask is not None else None\n    return compute_histogram(\n        self.darr, bins=bins, range=range, mask=mask_data, **kwargs\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.compute_otsu_thresholds","title":"<code>compute_otsu_thresholds(classes=2, bins=256, range=None, mask=None)</code>","text":"<p>Compute Otsu multi-level thresholds for the image.</p> <p>This method first computes the histogram of the image, then uses scikit-image's threshold_multiotsu to compute optimal threshold values.</p> <p>Parameters:</p> Name Type Description Default <code>classes</code> <code>int</code> <p>Number of classes to separate data into (default: 2). Must be &gt;= 2. For classes=2, returns 1 threshold. For classes=k, returns k-1 thresholds.</p> <code>2</code> <code>bins</code> <code>int</code> <p>Number of histogram bins (default: 256)</p> <code>256</code> <code>range</code> <code>Optional[Tuple[float, float]]</code> <p>Optional tuple (min, max) defining histogram range. If None, uses the full range of the data</p> <code>None</code> <code>mask</code> <code>Optional['ZarrNii']</code> <p>Optional ZarrNii mask of same shape as image. Only pixels where mask &gt; 0 are included in histogram computation</p> <code>None</code> <p>Returns:</p> Type Description <code>List[float]</code> <p>List of threshold values. For classes=k, returns k+1 values:</p> <code>List[float]</code> <p>[0, threshold1, threshold2, ..., threshold_k-1, max_intensity]</p> <code>List[float]</code> <p>where 0 represents the minimum and max_intensity represents the maximum.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Compute binary threshold (2 classes)\n&gt;&gt;&gt; thresholds = znimg.compute_otsu_thresholds(classes=2)\n&gt;&gt;&gt; print(f\"Binary thresholds: {thresholds}\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Compute multi-level thresholds (3 classes)\n&gt;&gt;&gt; thresholds = znimg.compute_otsu_thresholds(classes=3)\n&gt;&gt;&gt; print(f\"Multi-level thresholds: {thresholds}\")\n</code></pre> Source code in <code>zarrnii/core.py</code> <pre><code>def compute_otsu_thresholds(\n    self,\n    classes: int = 2,\n    bins: int = 256,\n    range: Optional[Tuple[float, float]] = None,\n    mask: Optional[\"ZarrNii\"] = None,\n) -&gt; List[float]:\n    \"\"\"\n    Compute Otsu multi-level thresholds for the image.\n\n    This method first computes the histogram of the image, then uses\n    scikit-image's threshold_multiotsu to compute optimal threshold values.\n\n    Args:\n        classes: Number of classes to separate data into (default: 2).\n            Must be &gt;= 2. For classes=2, returns 1 threshold. For classes=k,\n            returns k-1 thresholds.\n        bins: Number of histogram bins (default: 256)\n        range: Optional tuple (min, max) defining histogram range. If None,\n            uses the full range of the data\n        mask: Optional ZarrNii mask of same shape as image. Only pixels\n            where mask &gt; 0 are included in histogram computation\n\n    Returns:\n        List of threshold values. For classes=k, returns k+1 values:\n        [0, threshold1, threshold2, ..., threshold_k-1, max_intensity]\n        where 0 represents the minimum and max_intensity represents the maximum.\n\n    Examples:\n        &gt;&gt;&gt; # Compute binary threshold (2 classes)\n        &gt;&gt;&gt; thresholds = znimg.compute_otsu_thresholds(classes=2)\n        &gt;&gt;&gt; print(f\"Binary thresholds: {thresholds}\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Compute multi-level thresholds (3 classes)\n        &gt;&gt;&gt; thresholds = znimg.compute_otsu_thresholds(classes=3)\n        &gt;&gt;&gt; print(f\"Multi-level thresholds: {thresholds}\")\n    \"\"\"\n    from .analysis import compute_otsu_thresholds\n\n    # First compute histogram\n    hist, bin_edges = self.compute_histogram(bins=bins, range=range, mask=mask)\n\n    # Then compute thresholds\n    return compute_otsu_thresholds(hist, classes=classes, bin_edges=bin_edges)\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.copy","title":"<code>copy()</code>","text":"<p>Create a copy of this ZarrNii.</p> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>New ZarrNii with copied data</p> Source code in <code>zarrnii/core.py</code> <pre><code>def copy(self) -&gt; \"ZarrNii\":\n    \"\"\"\n    Create a copy of this ZarrNii.\n\n    Returns:\n        New ZarrNii with copied data\n    \"\"\"\n    # Create a new NgffImage with the same properties\n    copied_image = nz.NgffImage(\n        data=self.ngff_image.data,  # Dask arrays are lazy so this is efficient\n        dims=self.ngff_image.dims.copy(),\n        scale=self.ngff_image.scale.copy(),\n        translation=self.ngff_image.translation.copy(),\n        name=self.ngff_image.name,\n    )\n    return ZarrNii(\n        ngff_image=copied_image,\n        axes_order=self.axes_order,\n        xyz_orientation=self.xyz_orientation,\n        _omero=self._omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.crop","title":"<code>crop(bbox_min, bbox_max=None, spatial_dims=None, physical_coords=False)</code>","text":"<p>Extract a spatial region or multiple regions from the image.</p> <p>Crops the image to the specified bounding box coordinates, preserving all metadata and non-spatial dimensions (channels, time). The cropping is performed in voxel coordinates by default, or physical coordinates if specified. Can crop a single region or multiple regions at once.</p> <p>Parameters:</p> Name Type Description Default <code>bbox_min</code> <code>Union[Tuple[float, ...], List[Tuple[Tuple[float, ...], Tuple[float, ...]]]]</code> <p>Either: - Minimum corner coordinates of bounding box as tuple   (when bbox_max is provided). Length should match number of   spatial dimensions (x, y, z order) - List of (bbox_min, bbox_max) tuples for batch cropping   (when bbox_max is None)</p> required <code>bbox_max</code> <code>Optional[Tuple[float, ...]]</code> <p>Maximum corner coordinates of bounding box as tuple. Length should match number of spatial dimensions (x, y, z order). Should be None when bbox_min is a list of bounding boxes.</p> <code>None</code> <code>spatial_dims</code> <code>Optional[List[str]]</code> <p>Names of spatial dimensions to crop. If None, automatically derived from axes_order (\"z\",\"y\",\"x\" for ZYX or \"x\",\"y\",\"z\" for XYZ)</p> <code>None</code> <code>physical_coords</code> <code>bool</code> <p>If True, bbox_min and bbox_max are in physical/world coordinates (mm). If False, they are in voxel coordinates. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union['ZarrNii', List['ZarrNii']]</code> <p>New ZarrNii instance with cropped data (single crop) or list of</p> <code>Union['ZarrNii', List['ZarrNii']]</code> <p>ZarrNii instances (batch crop) with updated spatial metadata</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If bbox coordinates are invalid or out of bounds, or if both list and bbox_max are provided</p> <code>IndexError</code> <p>If bbox dimensions don't match spatial dimensions</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Crop 3D region (voxel coordinates)\n&gt;&gt;&gt; cropped = znii.crop((10, 20, 30), (110, 120, 130))\n</code></pre> <pre><code>&gt;&gt;&gt; # Crop with physical coordinates\n&gt;&gt;&gt; cropped = znii.crop((10.5, 20.5, 30.5), (110.5, 120.5, 130.5),\n...                      physical_coords=True)\n</code></pre> <pre><code>&gt;&gt;&gt; # Crop with explicit spatial dimensions\n&gt;&gt;&gt; cropped = znii.crop(\n...     (50, 60, 70), (150, 160, 170),\n...     spatial_dims=[\"x\", \"y\", \"z\"]\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Batch crop multiple regions\n&gt;&gt;&gt; bboxes = [\n...     ((10, 20, 30), (60, 70, 80)),\n...     ((100, 110, 120), (150, 160, 170))\n... ]\n&gt;&gt;&gt; cropped_list = znii.crop(bboxes, physical_coords=True)\n</code></pre> Notes <ul> <li>Coordinates are in voxel space (0-based indexing) by default</li> <li>Physical coordinates are in RAS orientation (Right-Anterior-Superior)</li> <li>The cropped region includes bbox_min but excludes bbox_max</li> <li>All non-spatial dimensions (channels, time) are preserved</li> <li>Spatial transformations are automatically updated</li> <li>When batch cropping, all patches share the same spatial_dims and   physical_coords settings</li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def crop(\n    self,\n    bbox_min: Union[\n        Tuple[float, ...], List[Tuple[Tuple[float, ...], Tuple[float, ...]]]\n    ],\n    bbox_max: Optional[Tuple[float, ...]] = None,\n    spatial_dims: Optional[List[str]] = None,\n    physical_coords: bool = False,\n) -&gt; Union[\"ZarrNii\", List[\"ZarrNii\"]]:\n    \"\"\"Extract a spatial region or multiple regions from the image.\n\n    Crops the image to the specified bounding box coordinates, preserving\n    all metadata and non-spatial dimensions (channels, time). The cropping\n    is performed in voxel coordinates by default, or physical coordinates\n    if specified. Can crop a single region or multiple regions at once.\n\n    Args:\n        bbox_min: Either:\n            - Minimum corner coordinates of bounding box as tuple\n              (when bbox_max is provided). Length should match number of\n              spatial dimensions (x, y, z order)\n            - List of (bbox_min, bbox_max) tuples for batch cropping\n              (when bbox_max is None)\n        bbox_max: Maximum corner coordinates of bounding box as tuple.\n            Length should match number of spatial dimensions (x, y, z order).\n            Should be None when bbox_min is a list of bounding boxes.\n        spatial_dims: Names of spatial dimensions to crop. If None,\n            automatically derived from axes_order (\"z\",\"y\",\"x\" for ZYX\n            or \"x\",\"y\",\"z\" for XYZ)\n        physical_coords: If True, bbox_min and bbox_max are in physical/world\n            coordinates (mm). If False, they are in voxel coordinates.\n            Default is False.\n\n    Returns:\n        New ZarrNii instance with cropped data (single crop) or list of\n        ZarrNii instances (batch crop) with updated spatial metadata\n\n    Raises:\n        ValueError: If bbox coordinates are invalid or out of bounds, or\n            if both list and bbox_max are provided\n        IndexError: If bbox dimensions don't match spatial dimensions\n\n    Examples:\n        &gt;&gt;&gt; # Crop 3D region (voxel coordinates)\n        &gt;&gt;&gt; cropped = znii.crop((10, 20, 30), (110, 120, 130))\n\n        &gt;&gt;&gt; # Crop with physical coordinates\n        &gt;&gt;&gt; cropped = znii.crop((10.5, 20.5, 30.5), (110.5, 120.5, 130.5),\n        ...                      physical_coords=True)\n\n        &gt;&gt;&gt; # Crop with explicit spatial dimensions\n        &gt;&gt;&gt; cropped = znii.crop(\n        ...     (50, 60, 70), (150, 160, 170),\n        ...     spatial_dims=[\"x\", \"y\", \"z\"]\n        ... )\n\n        &gt;&gt;&gt; # Batch crop multiple regions\n        &gt;&gt;&gt; bboxes = [\n        ...     ((10, 20, 30), (60, 70, 80)),\n        ...     ((100, 110, 120), (150, 160, 170))\n        ... ]\n        &gt;&gt;&gt; cropped_list = znii.crop(bboxes, physical_coords=True)\n\n    Notes:\n        - Coordinates are in voxel space (0-based indexing) by default\n        - Physical coordinates are in RAS orientation (Right-Anterior-Superior)\n        - The cropped region includes bbox_min but excludes bbox_max\n        - All non-spatial dimensions (channels, time) are preserved\n        - Spatial transformations are automatically updated\n        - When batch cropping, all patches share the same spatial_dims and\n          physical_coords settings\n    \"\"\"\n    # Check if this is batch cropping (list of bounding boxes)\n    # A batch crop is a list of (bbox_min, bbox_max) tuples\n    # Each element should be a tuple/list of two elements\n    is_batch_crop = (\n        isinstance(bbox_min, list)\n        and len(bbox_min) &gt; 0\n        and isinstance(bbox_min[0], (tuple, list))\n        and len(bbox_min[0]) == 2\n    )\n\n    if is_batch_crop:\n        if bbox_max is not None:\n            raise ValueError(\n                \"bbox_max should be None when bbox_min is a list of bounding boxes\"\n            )\n        # Batch crop: recursively call crop for each bounding box\n        return [\n            self.crop(bmin, bmax, spatial_dims, physical_coords)\n            for bmin, bmax in bbox_min\n        ]\n\n    # Single crop: original implementation\n    if bbox_max is None:\n        raise ValueError(\"bbox_max is required when bbox_min is not a list\")\n\n    if spatial_dims is None:\n        spatial_dims = (\n            [\"z\", \"y\", \"x\"] if self.axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n        )\n\n    # Convert physical coordinates to voxel coordinates if needed\n    if physical_coords:\n        # Physical coords are always in (x, y, z) order\n        # Convert to homogeneous coordinates\n        phys_min = np.array(list(bbox_min) + [1.0])\n        phys_max = np.array(list(bbox_max) + [1.0])\n\n        # Get inverse affine to convert from physical to voxel\n        affine_inv = np.linalg.inv(self.affine.matrix)\n\n        # Transform to voxel coordinates\n        voxel_min = affine_inv @ phys_min\n        voxel_max = affine_inv @ phys_max\n\n        # Extract voxel coordinates (x, y, z)\n        voxel_min_xyz = voxel_min[:3]\n        voxel_max_xyz = voxel_max[:3]\n\n        # Round to nearest integer voxel indices\n        voxel_min_xyz = np.round(voxel_min_xyz).astype(int)\n        voxel_max_xyz = np.round(voxel_max_xyz).astype(int)\n\n        # Ensure max &gt;= min\n        voxel_min_xyz = np.minimum(voxel_min_xyz, voxel_max_xyz)\n        voxel_max_xyz = np.maximum(\n            np.round(affine_inv @ phys_min).astype(int)[:3],\n            np.round(affine_inv @ phys_max).astype(int)[:3],\n        )\n\n        # Create mapping from x,y,z to voxel coordinates\n        xyz_to_voxel = {\n            \"x\": voxel_min_xyz[0],\n            \"y\": voxel_min_xyz[1],\n            \"z\": voxel_min_xyz[2],\n        }\n        xyz_to_voxel_max = {\n            \"x\": voxel_max_xyz[0],\n            \"y\": voxel_max_xyz[1],\n            \"z\": voxel_max_xyz[2],\n        }\n\n        # Reorder according to spatial_dims\n        bbox_min = tuple(xyz_to_voxel[dim.lower()] for dim in spatial_dims)\n        bbox_max = tuple(xyz_to_voxel_max[dim.lower()] for dim in spatial_dims)\n\n    cropped_image = crop_ngff_image(\n        self.ngff_image, bbox_min, bbox_max, spatial_dims\n    )\n    return ZarrNii(\n        ngff_image=cropped_image,\n        axes_order=self.axes_order,\n        xyz_orientation=self.xyz_orientation,\n        _omero=self._omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.crop_with_bounding_box","title":"<code>crop_with_bounding_box(bbox_min, bbox_max, ras_coords=False)</code>","text":"<p>Legacy method name for crop.</p> <p>Parameters:</p> Name Type Description Default <code>bbox_min</code> <p>Minimum corner coordinates</p> required <code>bbox_max</code> <p>Maximum corner coordinates</p> required <code>ras_coords</code> <p>If True, coordinates are in RAS physical space (deprecated, use physical_coords parameter of crop() instead)</p> <code>False</code> Source code in <code>zarrnii/core.py</code> <pre><code>def crop_with_bounding_box(self, bbox_min, bbox_max, ras_coords=False):\n    \"\"\"Legacy method name for crop.\n\n    Args:\n        bbox_min: Minimum corner coordinates\n        bbox_max: Maximum corner coordinates\n        ras_coords: If True, coordinates are in RAS physical space (deprecated,\n            use physical_coords parameter of crop() instead)\n    \"\"\"\n    return self.crop(bbox_min, bbox_max, physical_coords=ras_coords)\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.downsample","title":"<code>downsample(factors=None, along_x=1, along_y=1, along_z=1, level=None, spatial_dims=None)</code>","text":"<p>Reduce image resolution by downsampling.</p> <p>Performs spatial downsampling by averaging blocks of voxels, effectively reducing image resolution and size. Multiple parameter options provide flexibility for different downsampling strategies.</p> <p>Parameters:</p> Name Type Description Default <code>factors</code> <code>Optional[Union[int, List[int]]]</code> <p>Downsampling factors for spatial dimensions. Can be: - int: Same factor applied to all spatial dimensions - List[int]: Per-dimension factors matching spatial_dims order - None: Use other parameters to determine factors</p> <code>None</code> <code>along_x</code> <code>int</code> <p>Downsampling factor for X dimension (legacy parameter)</p> <code>1</code> <code>along_y</code> <code>int</code> <p>Downsampling factor for Y dimension (legacy parameter)</p> <code>1</code> <code>along_z</code> <code>int</code> <p>Downsampling factor for Z dimension (legacy parameter)</p> <code>1</code> <code>level</code> <code>Optional[int]</code> <p>Power-of-2 downsampling level (factors = 2^level). Takes precedence over along_* parameters</p> <code>None</code> <code>spatial_dims</code> <code>Optional[List[str]]</code> <p>Names of spatial dimensions. If None, derived from axes_order</p> <code>None</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>New ZarrNii instance with downsampled data and updated metadata</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If conflicting parameters provided or invalid factors</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Isotropic downsampling by factor of 2\n&gt;&gt;&gt; downsampled = znii.downsample(factors=2)\n</code></pre> <pre><code>&gt;&gt;&gt; # Anisotropic downsampling\n&gt;&gt;&gt; downsampled = znii.downsample(factors=[1, 2, 2])\n</code></pre> <pre><code>&gt;&gt;&gt; # Using legacy parameters\n&gt;&gt;&gt; downsampled = znii.downsample(along_x=2, along_y=2, along_z=1)\n</code></pre> <pre><code>&gt;&gt;&gt; # Power-of-2 downsampling\n&gt;&gt;&gt; downsampled = znii.downsample(level=2)  # factors = 4\n</code></pre> Notes <ul> <li>Downsampling uses block averaging for anti-aliasing</li> <li>Spatial transformations are automatically scaled</li> <li>Non-spatial dimensions (channels, time) are preserved</li> <li>Original data remains unchanged (creates new instance)</li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def downsample(\n    self,\n    factors: Optional[Union[int, List[int]]] = None,\n    along_x: int = 1,\n    along_y: int = 1,\n    along_z: int = 1,\n    level: Optional[int] = None,\n    spatial_dims: Optional[List[str]] = None,\n) -&gt; \"ZarrNii\":\n    \"\"\"Reduce image resolution by downsampling.\n\n    Performs spatial downsampling by averaging blocks of voxels, effectively\n    reducing image resolution and size. Multiple parameter options provide\n    flexibility for different downsampling strategies.\n\n    Args:\n        factors: Downsampling factors for spatial dimensions. Can be:\n            - int: Same factor applied to all spatial dimensions\n            - List[int]: Per-dimension factors matching spatial_dims order\n            - None: Use other parameters to determine factors\n        along_x: Downsampling factor for X dimension (legacy parameter)\n        along_y: Downsampling factor for Y dimension (legacy parameter)\n        along_z: Downsampling factor for Z dimension (legacy parameter)\n        level: Power-of-2 downsampling level (factors = 2^level).\n            Takes precedence over along_* parameters\n        spatial_dims: Names of spatial dimensions. If None, derived\n            from axes_order\n\n    Returns:\n        New ZarrNii instance with downsampled data and updated metadata\n\n    Raises:\n        ValueError: If conflicting parameters provided or invalid factors\n\n    Examples:\n        &gt;&gt;&gt; # Isotropic downsampling by factor of 2\n        &gt;&gt;&gt; downsampled = znii.downsample(factors=2)\n\n        &gt;&gt;&gt; # Anisotropic downsampling\n        &gt;&gt;&gt; downsampled = znii.downsample(factors=[1, 2, 2])\n\n        &gt;&gt;&gt; # Using legacy parameters\n        &gt;&gt;&gt; downsampled = znii.downsample(along_x=2, along_y=2, along_z=1)\n\n        &gt;&gt;&gt; # Power-of-2 downsampling\n        &gt;&gt;&gt; downsampled = znii.downsample(level=2)  # factors = 4\n\n    Notes:\n        - Downsampling uses block averaging for anti-aliasing\n        - Spatial transformations are automatically scaled\n        - Non-spatial dimensions (channels, time) are preserved\n        - Original data remains unchanged (creates new instance)\n    \"\"\"\n    # Handle legacy parameters\n    if factors is None:\n        if level is not None:\n            factors = 2**level\n        else:\n            factors = (\n                [along_z, along_y, along_x]\n                if self.axes_order == \"ZYX\"\n                else [along_x, along_y, along_z]\n            )\n\n    if spatial_dims is None:\n        spatial_dims = (\n            [\"z\", \"y\", \"x\"] if self.axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n        )\n\n    downsampled_image = downsample_ngff_image(\n        self.ngff_image, factors, spatial_dims\n    )\n    return ZarrNii(\n        ngff_image=downsampled_image,\n        axes_order=self.axes_order,\n        xyz_orientation=self.xyz_orientation,\n        _omero=self._omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.from_darr","title":"<code>from_darr(darr, affine=None, axes_order='ZYX', orientation='RAS', spacing=(1.0, 1.0, 1.0), origin=(0.0, 0.0, 0.0), name='image', omero=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create ZarrNii from dask array (legacy compatibility constructor).</p> <p>Parameters:</p> Name Type Description Default <code>darr</code> <code>Array</code> <p>Dask array containing image data</p> required <code>affine</code> <code>Optional[AffineTransform]</code> <p>Optional affine transformation</p> <code>None</code> <code>axes_order</code> <code>str</code> <p>Spatial axes order</p> <code>'ZYX'</code> <code>orientation</code> <code>str</code> <p>Anatomical orientation string</p> <code>'RAS'</code> <code>spacing</code> <code>Tuple[float, float, float]</code> <p>Voxel spacing (used if no affine provided)</p> <code>(1.0, 1.0, 1.0)</code> <code>origin</code> <code>Tuple[float, float, float]</code> <p>Origin offset (used if no affine provided)</p> <code>(0.0, 0.0, 0.0)</code> <code>name</code> <code>str</code> <p>Image name</p> <code>'image'</code> <code>omero</code> <code>Optional[object]</code> <p>Optional omero metadata</p> <code>None</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>ZarrNii instance</p> Source code in <code>zarrnii/core.py</code> <pre><code>@classmethod\ndef from_darr(\n    cls,\n    darr: da.Array,\n    affine: Optional[AffineTransform] = None,\n    axes_order: str = \"ZYX\",\n    orientation: str = \"RAS\",\n    spacing: Tuple[float, float, float] = (1.0, 1.0, 1.0),\n    origin: Tuple[float, float, float] = (0.0, 0.0, 0.0),\n    name: str = \"image\",\n    omero: Optional[object] = None,\n    **kwargs,\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Create ZarrNii from dask array (legacy compatibility constructor).\n\n    Args:\n        darr: Dask array containing image data\n        affine: Optional affine transformation\n        axes_order: Spatial axes order\n        orientation: Anatomical orientation string\n        spacing: Voxel spacing (used if no affine provided)\n        origin: Origin offset (used if no affine provided)\n        name: Image name\n        omero: Optional omero metadata\n\n    Returns:\n        ZarrNii instance\n    \"\"\"\n    # Create scale and translation from affine if provided\n    if affine is not None:\n        # Extract scale and translation from affine matrix\n        affine_matrix = affine.matrix\n        if axes_order == \"ZYX\":\n            scale = {\n                \"z\": affine_matrix[0, 0],\n                \"y\": affine_matrix[1, 1],\n                \"x\": affine_matrix[2, 2],\n            }\n            translation = {\n                \"z\": affine_matrix[0, 3],\n                \"y\": affine_matrix[1, 3],\n                \"x\": affine_matrix[2, 3],\n            }\n        else:  # XYZ\n            scale = {\n                \"x\": affine_matrix[0, 0],\n                \"y\": affine_matrix[1, 1],\n                \"z\": affine_matrix[2, 2],\n            }\n            translation = {\n                \"x\": affine_matrix[0, 3],\n                \"y\": affine_matrix[1, 3],\n                \"z\": affine_matrix[2, 3],\n            }\n    else:\n        # Use spacing and origin\n        if axes_order == \"ZYX\":\n            scale = {\"z\": spacing[0], \"y\": spacing[1], \"x\": spacing[2]}\n            translation = {\"z\": origin[0], \"y\": origin[1], \"x\": origin[2]}\n        else:  # XYZ\n            scale = {\"x\": spacing[0], \"y\": spacing[1], \"z\": spacing[2]}\n            translation = {\"x\": origin[0], \"y\": origin[1], \"z\": origin[2]}\n\n    # Create dimensions based on data shape after dimension adjustments\n    final_ndim = len(darr.shape)\n    if final_ndim == 4:\n        # 4D: (c, z, y, x) or (c, x, y, z) - standard case\n        dims = [\"c\"] + list(axes_order.lower())\n    elif final_ndim == 5:\n        # 5D: (t, c, z, y, x) or (t, c, x, y, z) - time dimension included\n        dims = [\"t\", \"c\"] + list(axes_order.lower())\n    else:\n        # Fallback for other cases\n        dims = [\"c\"] + list(axes_order.lower())\n\n    # Create NgffImage\n    ngff_image = nz.NgffImage(\n        data=darr, dims=dims, scale=scale, translation=translation, name=name\n    )\n\n    return cls(\n        ngff_image=ngff_image,\n        axes_order=axes_order,\n        xyz_orientation=orientation,\n        _omero=omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.from_file","title":"<code>from_file(path, **kwargs)</code>  <code>classmethod</code>","text":"Source code in <code>zarrnii/core.py</code> <pre><code>@classmethod\ndef from_file(cls, path, **kwargs):\n    if path.endswith((\".nii\", \".nii.gz\")):\n        return cls.from_nifti(path, **kwargs)\n    elif path.endswith(\".zarr\") or path.endswith(\".zip\"):\n        return cls.from_ome_zarr(path, **kwargs)\n    else:\n        raise ValueError(f\"Unknown file extension: {path}\")\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.from_imaris","title":"<code>from_imaris(path, level=0, timepoint=0, channel=0, chunks='auto', axes_order='ZYX', orientation='RAS')</code>  <code>classmethod</code>","text":"<p>Load from Imaris (.ims) file format.</p> <p>Imaris files use HDF5 format with specific dataset structure. This method requires the 'imaris' extra dependency (h5py).</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to Imaris (.ims) file</p> required <code>level</code> <code>int</code> <p>Resolution level to load (0 = full resolution)</p> <code>0</code> <code>timepoint</code> <code>int</code> <p>Time point to load (default: 0)</p> <code>0</code> <code>channel</code> <code>int</code> <p>Channel to load (default: 0)</p> <code>0</code> <code>chunks</code> <code>str</code> <p>Chunking strategy for dask array</p> <code>'auto'</code> <code>axes_order</code> <code>str</code> <p>Spatial axes order for compatibility (default: \"ZYX\")</p> <code>'ZYX'</code> <code>orientation</code> <code>str</code> <p>Default orientation (default: \"RAS\")</p> <code>'RAS'</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>ZarrNii instance</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If h5py is not available</p> <code>ValueError</code> <p>If the file is not a valid Imaris file</p> Source code in <code>zarrnii/core.py</code> <pre><code>@classmethod\ndef from_imaris(\n    cls,\n    path: str,\n    level: int = 0,\n    timepoint: int = 0,\n    channel: int = 0,\n    chunks: str = \"auto\",\n    axes_order: str = \"ZYX\",\n    orientation: str = \"RAS\",\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Load from Imaris (.ims) file format.\n\n    Imaris files use HDF5 format with specific dataset structure.\n    This method requires the 'imaris' extra dependency (h5py).\n\n    Args:\n        path: Path to Imaris (.ims) file\n        level: Resolution level to load (0 = full resolution)\n        timepoint: Time point to load (default: 0)\n        channel: Channel to load (default: 0)\n        chunks: Chunking strategy for dask array\n        axes_order: Spatial axes order for compatibility (default: \"ZYX\")\n        orientation: Default orientation (default: \"RAS\")\n\n    Returns:\n        ZarrNii instance\n\n    Raises:\n        ImportError: If h5py is not available\n        ValueError: If the file is not a valid Imaris file\n    \"\"\"\n    try:\n        import h5py\n    except ImportError:\n        raise ImportError(\n            \"h5py is required for Imaris support. \"\n            \"Install with: pip install zarrnii[imaris] or pip install h5py\"\n        )\n\n    # Open Imaris file\n    with h5py.File(path, \"r\") as f:\n        # Verify it's an Imaris file by checking for standard structure\n        if \"DataSet\" not in f:\n            raise ValueError(\n                f\"File {path} does not appear to be a valid Imaris file (missing DataSet group)\"\n            )\n\n        # Navigate to the specific dataset\n        dataset_group = f[\"DataSet\"]\n\n        # Find available resolution levels\n        resolution_levels = [\n            key for key in dataset_group.keys() if key.startswith(\"ResolutionLevel\")\n        ]\n        if not resolution_levels:\n            raise ValueError(\"No resolution levels found in Imaris file\")\n\n        # Validate level parameter\n        if level &gt;= len(resolution_levels):\n            raise ValueError(\n                f\"Level {level} not available. Available levels: 0-{len(resolution_levels)-1}\"\n            )\n\n        # Navigate to specified resolution level\n        res_level_key = f\"ResolutionLevel {level}\"\n        if res_level_key not in dataset_group:\n            raise ValueError(f\"Resolution level {level} not found\")\n\n        res_group = dataset_group[res_level_key]\n\n        # Find available timepoints\n        timepoints = [\n            key for key in res_group.keys() if key.startswith(\"TimePoint\")\n        ]\n        if not timepoints:\n            raise ValueError(\"No timepoints found in Imaris file\")\n\n        # Validate timepoint parameter\n        if timepoint &gt;= len(timepoints):\n            raise ValueError(\n                f\"Timepoint {timepoint} not available. Available timepoints: 0-{len(timepoints)-1}\"\n            )\n\n        # Navigate to specified timepoint\n        time_key = f\"TimePoint {timepoint}\"\n        if time_key not in res_group:\n            raise ValueError(f\"Timepoint {timepoint} not found\")\n\n        time_group = res_group[time_key]\n\n        # Find available channels\n        channels = [key for key in time_group.keys() if key.startswith(\"Channel\")]\n        if not channels:\n            raise ValueError(\"No channels found in Imaris file\")\n\n        # Validate channel parameter\n        if channel &gt;= len(channels):\n            raise ValueError(\n                f\"Channel {channel} not available. Available channels: 0-{len(channels)-1}\"\n            )\n\n        # Navigate to specified channel\n        channel_key = f\"Channel {channel}\"\n        if channel_key not in time_group:\n            raise ValueError(f\"Channel {channel} not found\")\n\n        channel_group = time_group[channel_key]\n\n        # Load the actual data\n        if \"Data\" not in channel_group:\n            raise ValueError(\"No Data dataset found in channel group\")\n\n        data_dataset = channel_group[\"Data\"]\n\n        # Load data into memory first (necessary because HDF5 file will be closed)\n        data_numpy = data_dataset[:]\n\n        # Create dask array from numpy array\n        data_array = da.from_array(data_numpy, chunks=chunks)\n\n        # Add channel dimension if not present\n        if len(data_array.shape) == 3:\n            data_array = data_array[np.newaxis, ...]\n\n        # Extract spatial metadata\n        # Try to get spacing information from Imaris metadata\n        spacing = [1.0, 1.0, 1.0]  # Default spacing\n        origin = [0.0, 0.0, 0.0]  # Default origin\n\n        # Look for ImageSizeX, ImageSizeY, ImageSizeZ attributes\n        try:\n            # Navigate back to get image info\n            if \"ImageSizeX\" in f.attrs:\n                x_size = f.attrs[\"ImageSizeX\"]\n                y_size = f.attrs[\"ImageSizeY\"]\n                z_size = f.attrs[\"ImageSizeZ\"]\n\n                # Calculate spacing based on physical size and voxel count\n                if data_array.shape[-1] &gt; 0:  # X dimension\n                    spacing[0] = x_size / data_array.shape[-1]\n                if data_array.shape[-2] &gt; 0:  # Y dimension\n                    spacing[1] = y_size / data_array.shape[-2]\n                if data_array.shape[-3] &gt; 0:  # Z dimension\n                    spacing[2] = z_size / data_array.shape[-3]\n        except (KeyError, IndexError):\n            # Use default spacing if metadata is not available\n            pass\n\n        # Create dimensions\n        dims = [\"c\"] + list(axes_order.lower())\n\n        # Create scale and translation dictionaries\n        scale_dict = {}\n        translation_dict = {}\n        spatial_dims = [\"z\", \"y\", \"x\"] if axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n\n        for i, dim in enumerate(spatial_dims):\n            scale_dict[dim] = spacing[i]\n            translation_dict[dim] = origin[i]\n\n        # Create NgffImage\n        ngff_image = nz.NgffImage(\n            data=data_array,\n            dims=dims,\n            scale=scale_dict,\n            translation=translation_dict,\n            name=f\"imaris_image_{path}_{level}_{timepoint}_{channel}\",\n        )\n\n    # Create and return ZarrNii instance\n    return cls(\n        ngff_image=ngff_image,\n        axes_order=axes_order,\n        xyz_orientation=orientation,\n        _omero=None,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.from_ngff_image","title":"<code>from_ngff_image(ngff_image, axes_order='ZYX', xyz_orientation='RAS', omero=None)</code>  <code>classmethod</code>","text":"<p>Create ZarrNii from an existing NgffImage.</p> <p>Parameters:</p> Name Type Description Default <code>ngff_image</code> <code>NgffImage</code> <p>NgffImage to wrap</p> required <code>axes_order</code> <code>str</code> <p>Spatial axes order for NIfTI compatibility</p> <code>'ZYX'</code> <code>xyz_orientation</code> <code>str</code> <p>Anatomical orientation string in XYZ axes order</p> <code>'RAS'</code> <code>omero</code> <code>Optional[object]</code> <p>Optional omero metadata object</p> <code>None</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>ZarrNii instance</p> Source code in <code>zarrnii/core.py</code> <pre><code>@classmethod\ndef from_ngff_image(\n    cls,\n    ngff_image: nz.NgffImage,\n    axes_order: str = \"ZYX\",\n    xyz_orientation: str = \"RAS\",\n    omero: Optional[object] = None,\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Create ZarrNii from an existing NgffImage.\n\n    Args:\n        ngff_image: NgffImage to wrap\n        axes_order: Spatial axes order for NIfTI compatibility\n        xyz_orientation: Anatomical orientation string in XYZ axes order\n        omero: Optional omero metadata object\n\n    Returns:\n        ZarrNii instance\n    \"\"\"\n    return cls(\n        ngff_image=ngff_image,\n        axes_order=axes_order,\n        xyz_orientation=xyz_orientation,\n        _omero=omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.from_nifti","title":"<code>from_nifti(path, chunks='auto', axes_order='XYZ', name=None, as_ref=False, zooms=None)</code>  <code>classmethod</code>","text":"<p>Load ZarrNii from NIfTI file with flexible loading options.</p> <p>Creates a ZarrNii instance from a NIfTI file, automatically converting the data to dask arrays and extracting spatial transformation information. Supports both full data loading and reference-only loading for memory efficiency.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, bytes]</code> <p>File path to NIfTI file (.nii, .nii.gz, .img/.hdr)</p> required <code>chunks</code> <code>Union[str, Tuple[int, ...]]</code> <p>Dask array chunking strategy. Can be: - \"auto\": Automatic chunking based on file size - Tuple of ints: Manual chunk sizes for each dimension - Dict mapping axis to chunk size</p> <code>'auto'</code> <code>axes_order</code> <code>str</code> <p>Spatial axis ordering convention. Either: - \"XYZ\": X=left-right, Y=anterior-posterior, Z=inferior-superior - \"ZYX\": Z=inferior-superior, Y=anterior-posterior, X=left-right</p> <code>'XYZ'</code> <code>name</code> <code>Optional[str]</code> <p>Optional name for the resulting NgffImage. If None, uses filename without extension</p> <code>None</code> <code>as_ref</code> <code>bool</code> <p>If True, creates empty dask array with correct shape/metadata without loading actual image data (memory efficient for templates)</p> <code>False</code> <code>zooms</code> <code>Optional[Tuple[float, float, float]]</code> <p>Target voxel spacing as (x, y, z) in mm. Only valid when as_ref=True. Adjusts shape and affine accordingly</p> <code>None</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>ZarrNii instance containing NIfTI data and spatial metadata</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If zooms specified with as_ref=False, or invalid axes_order</p> <code>FileNotFoundError</code> <p>If NIfTI file does not exist</p> <code>OSError</code> <p>If unable to read NIfTI file</p> <code>ImageFileError</code> <p>If file is not valid NIfTI</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Load full NIfTI data\n&gt;&gt;&gt; znii = ZarrNii.from_nifti(\"/path/to/brain.nii.gz\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Load with custom chunking and axis order\n&gt;&gt;&gt; znii = ZarrNii.from_nifti(\n...     \"/path/to/data.nii\",\n...     chunks=(64, 64, 64),\n...     axes_order=\"ZYX\"\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Create reference with target resolution\n&gt;&gt;&gt; znii_ref = ZarrNii.from_nifti(\n...     \"/path/to/template.nii.gz\",\n...     as_ref=True,\n...     zooms=(2.0, 2.0, 2.0)\n... )\n</code></pre> Notes <p>The method automatically handles NIfTI orientation codes and converts them to the specified axes_order for consistency with OME-Zarr workflows.</p> Source code in <code>zarrnii/core.py</code> <pre><code>@classmethod\ndef from_nifti(\n    cls,\n    path: Union[str, bytes],\n    chunks: Union[str, Tuple[int, ...]] = \"auto\",\n    axes_order: str = \"XYZ\",\n    name: Optional[str] = None,\n    as_ref: bool = False,\n    zooms: Optional[Tuple[float, float, float]] = None,\n) -&gt; \"ZarrNii\":\n    \"\"\"Load ZarrNii from NIfTI file with flexible loading options.\n\n    Creates a ZarrNii instance from a NIfTI file, automatically converting\n    the data to dask arrays and extracting spatial transformation information.\n    Supports both full data loading and reference-only loading for memory\n    efficiency.\n\n    Args:\n        path: File path to NIfTI file (.nii, .nii.gz, .img/.hdr)\n        chunks: Dask array chunking strategy. Can be:\n            - \"auto\": Automatic chunking based on file size\n            - Tuple of ints: Manual chunk sizes for each dimension\n            - Dict mapping axis to chunk size\n        axes_order: Spatial axis ordering convention. Either:\n            - \"XYZ\": X=left-right, Y=anterior-posterior, Z=inferior-superior\n            - \"ZYX\": Z=inferior-superior, Y=anterior-posterior, X=left-right\n        name: Optional name for the resulting NgffImage. If None,\n            uses filename without extension\n        as_ref: If True, creates empty dask array with correct shape/metadata\n            without loading actual image data (memory efficient for templates)\n        zooms: Target voxel spacing as (x, y, z) in mm. Only valid when\n            as_ref=True. Adjusts shape and affine accordingly\n\n    Returns:\n        ZarrNii instance containing NIfTI data and spatial metadata\n\n    Raises:\n        ValueError: If zooms specified with as_ref=False, or invalid axes_order\n        FileNotFoundError: If NIfTI file does not exist\n        OSError: If unable to read NIfTI file\n        nibabel.filebasedimages.ImageFileError: If file is not valid NIfTI\n\n    Examples:\n        &gt;&gt;&gt; # Load full NIfTI data\n        &gt;&gt;&gt; znii = ZarrNii.from_nifti(\"/path/to/brain.nii.gz\")\n\n        &gt;&gt;&gt; # Load with custom chunking and axis order\n        &gt;&gt;&gt; znii = ZarrNii.from_nifti(\n        ...     \"/path/to/data.nii\",\n        ...     chunks=(64, 64, 64),\n        ...     axes_order=\"ZYX\"\n        ... )\n\n        &gt;&gt;&gt; # Create reference with target resolution\n        &gt;&gt;&gt; znii_ref = ZarrNii.from_nifti(\n        ...     \"/path/to/template.nii.gz\",\n        ...     as_ref=True,\n        ...     zooms=(2.0, 2.0, 2.0)\n        ... )\n\n    Notes:\n        The method automatically handles NIfTI orientation codes and converts\n        them to the specified axes_order for consistency with OME-Zarr workflows.\n    \"\"\"\n    if not as_ref and zooms is not None:\n        raise ValueError(\"`zooms` can only be used when `as_ref=True`.\")\n\n    # Load NIfTI file\n    nifti_img = nib.load(path)\n    shape = nifti_img.header.get_data_shape()\n    affine_matrix = nifti_img.affine.copy()\n\n    # infer orientation from the affine\n    orientation = affine_to_orientation(affine_matrix)\n\n    # Adjust shape and affine if zooms are provided\n    if zooms is not None:\n        in_zooms = np.sqrt(\n            (affine_matrix[:3, :3] ** 2).sum(axis=0)\n        )  # Current voxel spacing\n        scaling_factor = in_zooms / zooms\n        new_shape = [\n            int(np.floor(shape[0] * scaling_factor[2])),  # Z\n            int(np.floor(shape[1] * scaling_factor[1])),  # Y\n            int(np.floor(shape[2] * scaling_factor[0])),  # X\n        ]\n        np.fill_diagonal(affine_matrix[:3, :3], zooms)\n    else:\n        new_shape = shape\n\n    if as_ref:\n        # Create an empty dask array with the adjusted shape\n        darr = da.empty((1, *new_shape), chunks=chunks, dtype=\"float32\")\n    else:\n        # Load the NIfTI data and convert to a dask array\n        array = nifti_img.get_fdata()\n        darr = da.from_array(array, chunks=chunks)\n\n    # Add channel and time dimensions if not present\n    original_ndim = len(darr.shape)\n\n    if original_ndim == 3:\n        # 3D data: add channel dimension -&gt; (c, z, y, x) or (c, x, y, z)\n        darr = darr[np.newaxis, ...]\n    elif original_ndim == 4:\n        # 4D data: could be (c, z, y, x) or (t, z, y, x) - assume channel by default\n        # User can specify if it's time by using appropriate axes_order\n        pass  # Keep as is - 4D is already handled\n    elif original_ndim == 5:\n        # 5D data: assume (t, z, y, x, c) and handle appropriately\n        pass  # Keep as is - 5D is already the target format\n    else:\n        # For 1D, 2D, or &gt;5D data, add channel dimension and let user handle\n        darr = darr[np.newaxis, ...]\n\n    # Create dimensions based on data shape after dimension adjustments\n    final_ndim = len(darr.shape)\n    if final_ndim == 4:\n        # 4D: (c, z, y, x) or (c, x, y, z) - standard case\n        dims = [\"c\"] + list(axes_order.lower())\n    elif final_ndim == 5:\n        # 5D: (t, c, z, y, x) or (t, c, x, y, z) - time dimension included\n        dims = [\"t\", \"c\"] + list(axes_order.lower())\n    else:\n        # Fallback for other cases\n        dims = [\"c\"] + list(axes_order.lower())\n\n    # Extract scale and translation from affine\n    scale = {}\n    translation = {}\n    spatial_dims = [\"z\", \"y\", \"x\"] if axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n\n    for i, dim in enumerate(spatial_dims):\n        scale[dim] = np.sqrt((affine_matrix[i, :3] ** 2).sum())\n        translation[dim] = affine_matrix[i, 3]\n\n    # Create NgffImage\n    if name is None:\n        name = f\"nifti_image_{path}\"\n\n    ngff_image = nz.NgffImage(\n        data=darr, dims=dims, scale=scale, translation=translation, name=name\n    )\n\n    return cls(\n        ngff_image=ngff_image, axes_order=axes_order, xyz_orientation=orientation\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.from_ome_zarr","title":"<code>from_ome_zarr(store_or_path, level=0, channels=None, channel_labels=None, timepoints=None, storage_options=None, axes_order='ZYX', orientation='RAS', downsample_near_isotropic=False, chunks='auto', rechunk=False)</code>  <code>classmethod</code>","text":"<p>Load ZarrNii from OME-Zarr store with flexible options.</p> <p>Creates a ZarrNii instance from an OME-Zarr store, supporting multiscale pyramids, channel/timepoint selection, and various storage backends. Automatically handles metadata extraction and format conversion.</p> <p>Parameters:</p> Name Type Description Default <code>store_or_path</code> <code>Union[str, Any]</code> <p>Store or path to OME-Zarr file. Supports: - Local file paths - Remote URLs (s3://, http://, etc.) - ZIP files (.zip extension) - Zarr store objects</p> required <code>level</code> <code>int</code> <p>Pyramid level to load (0 = highest resolution). If level exceeds available levels, applies lazy downsampling</p> <code>0</code> <code>channels</code> <code>Optional[List[int]]</code> <p>List of channel indices to load (0-based). Mutually exclusive with channel_labels</p> <code>None</code> <code>channel_labels</code> <code>Optional[List[str]]</code> <p>List of channel names to load by label. Requires OMERO metadata. Mutually exclusive with channels</p> <code>None</code> <code>timepoints</code> <code>Optional[List[int]]</code> <p>List of timepoint indices to load (0-based). If None, loads all available timepoints</p> <code>None</code> <code>storage_options</code> <code>Optional[Dict[str, Any]]</code> <p>Additional options for zarr storage backend (e.g., credentials for cloud storage)</p> <code>None</code> <code>axes_order</code> <code>str</code> <p>Spatial axis order for NIfTI compatibility. Either \"ZYX\" or \"XYZ\"</p> <code>'ZYX'</code> <code>orientation</code> <code>str</code> <p>Default anatomical orientation if not in metadata. Standard orientations like \"RAS\", \"LPI\", etc. This is always interpreted in XYZ axes order for consistency.</p> <code>'RAS'</code> <code>downsample_near_isotropic</code> <code>bool</code> <p>If True, automatically downsample dimensions with smaller voxel sizes to achieve near-isotropic resolution</p> <code>False</code> <code>chunks</code> <code>tuple[int, Ellipsis] | Literal['auto']</code> <p>chunking strategy, or explicit chunk sizes to use if not automatic</p> <code>'auto'</code> <code>rechunk</code> <code>bool</code> <p>If True, rechunks the dataset after lazy loading, based on the chunks parameter</p> <code>False</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>ZarrNii instance with loaded data and metadata</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both channels and channel_labels are specified, or if invalid level/indices are provided</p> <code>FileNotFoundError</code> <p>If store_or_path does not exist</p> <code>KeyError</code> <p>If specified channel labels are not found</p> <code>IOError</code> <p>If unable to read from the storage backend</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Load full resolution data\n&gt;&gt;&gt; znii = ZarrNii.from_ome_zarr(\"/path/to/data.zarr\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Load specific channels and pyramid level\n&gt;&gt;&gt; znii = ZarrNii.from_ome_zarr(\n...     \"/path/to/data.zarr\",\n...     level=1,\n...     channels=[0, 2],\n...     orientation=\"LPI\"\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Load from cloud storage\n&gt;&gt;&gt; znii = ZarrNii.from_ome_zarr(\n...     \"s3://bucket/data.zarr\",\n...     storage_options={\"key\": \"access_key\", \"secret\": \"secret\"}\n... )\n</code></pre> Notes <p>Orientation Metadata Backwards Compatibility:</p> <p>This method implements backwards compatibility for orientation metadata:</p> <ol> <li> <p>Priority Order: Checks for 'xyz_orientation' first (new format),    then falls back to 'orientation' (legacy format)</p> </li> <li> <p>Legacy Fallback: When only legacy 'orientation' is found, the    orientation string is automatically reversed to convert from ZYX-based    encoding (legacy) to XYZ-based encoding (current standard)</p> </li> <li> <p>Default Fallback: If no orientation metadata is found, uses the    provided 'orientation' parameter as the default</p> </li> </ol> <p>Examples of the conversion: - Legacy 'orientation'='SAR' (ZYX) \u2192 'xyz_orientation'='RAS' (XYZ) - Legacy 'orientation'='IPL' (ZYX) \u2192 'xyz_orientation'='LPI' (XYZ)</p> <p>This ensures consistent orientation handling while maintaining backwards compatibility with existing OME-Zarr files that use the legacy format.</p> Source code in <code>zarrnii/core.py</code> <pre><code>@classmethod\ndef from_ome_zarr(\n    cls,\n    store_or_path: Union[str, Any],\n    level: int = 0,\n    channels: Optional[List[int]] = None,\n    channel_labels: Optional[List[str]] = None,\n    timepoints: Optional[List[int]] = None,\n    storage_options: Optional[Dict[str, Any]] = None,\n    axes_order: str = \"ZYX\",\n    orientation: str = \"RAS\",\n    downsample_near_isotropic: bool = False,\n    chunks: tuple[int, Ellipsis] | Literal[\"auto\"] = \"auto\",\n    rechunk: bool = False,\n) -&gt; \"ZarrNii\":\n    \"\"\"Load ZarrNii from OME-Zarr store with flexible options.\n\n    Creates a ZarrNii instance from an OME-Zarr store, supporting multiscale\n    pyramids, channel/timepoint selection, and various storage backends.\n    Automatically handles metadata extraction and format conversion.\n\n    Args:\n        store_or_path: Store or path to OME-Zarr file. Supports:\n            - Local file paths\n            - Remote URLs (s3://, http://, etc.)\n            - ZIP files (.zip extension)\n            - Zarr store objects\n        level: Pyramid level to load (0 = highest resolution). If level\n            exceeds available levels, applies lazy downsampling\n        channels: List of channel indices to load (0-based). Mutually\n            exclusive with channel_labels\n        channel_labels: List of channel names to load by label. Requires\n            OMERO metadata. Mutually exclusive with channels\n        timepoints: List of timepoint indices to load (0-based). If None,\n            loads all available timepoints\n        storage_options: Additional options for zarr storage backend\n            (e.g., credentials for cloud storage)\n        axes_order: Spatial axis order for NIfTI compatibility.\n            Either \"ZYX\" or \"XYZ\"\n        orientation: Default anatomical orientation if not in metadata.\n            Standard orientations like \"RAS\", \"LPI\", etc. This is always\n            interpreted in XYZ axes order for consistency.\n        downsample_near_isotropic: If True, automatically downsample\n            dimensions with smaller voxel sizes to achieve near-isotropic\n            resolution\n        chunks: chunking strategy, or explicit chunk sizes to use if not automatic\n        rechunk: If True, rechunks the dataset after lazy loading, based\n            on the chunks parameter\n\n    Returns:\n        ZarrNii instance with loaded data and metadata\n\n    Raises:\n        ValueError: If both channels and channel_labels are specified,\n            or if invalid level/indices are provided\n        FileNotFoundError: If store_or_path does not exist\n        KeyError: If specified channel labels are not found\n        IOError: If unable to read from the storage backend\n\n    Examples:\n        &gt;&gt;&gt; # Load full resolution data\n        &gt;&gt;&gt; znii = ZarrNii.from_ome_zarr(\"/path/to/data.zarr\")\n\n        &gt;&gt;&gt; # Load specific channels and pyramid level\n        &gt;&gt;&gt; znii = ZarrNii.from_ome_zarr(\n        ...     \"/path/to/data.zarr\",\n        ...     level=1,\n        ...     channels=[0, 2],\n        ...     orientation=\"LPI\"\n        ... )\n\n        &gt;&gt;&gt; # Load from cloud storage\n        &gt;&gt;&gt; znii = ZarrNii.from_ome_zarr(\n        ...     \"s3://bucket/data.zarr\",\n        ...     storage_options={\"key\": \"access_key\", \"secret\": \"secret\"}\n        ... )\n\n    Notes:\n        **Orientation Metadata Backwards Compatibility:**\n\n        This method implements backwards compatibility for orientation metadata:\n\n        1. **Priority Order**: Checks for 'xyz_orientation' first (new format),\n           then falls back to 'orientation' (legacy format)\n\n        2. **Legacy Fallback**: When only legacy 'orientation' is found, the\n           orientation string is automatically reversed to convert from ZYX-based\n           encoding (legacy) to XYZ-based encoding (current standard)\n\n        3. **Default Fallback**: If no orientation metadata is found, uses the\n           provided 'orientation' parameter as the default\n\n        Examples of the conversion:\n        - Legacy 'orientation'='SAR' (ZYX) \u2192 'xyz_orientation'='RAS' (XYZ)\n        - Legacy 'orientation'='IPL' (ZYX) \u2192 'xyz_orientation'='LPI' (XYZ)\n\n        This ensures consistent orientation handling while maintaining backwards\n        compatibility with existing OME-Zarr files that use the legacy format.\n    \"\"\"\n    # Validate channel and timepoint selection arguments\n    if channels is not None and channel_labels is not None:\n        raise ValueError(\"Cannot specify both 'channels' and 'channel_labels'\")\n\n    # Load the multiscales object\n    try:\n        if isinstance(store_or_path, str):\n            # Handle ZIP files by creating a ZipStore\n            if store_or_path.endswith(\".zip\"):\n                import zarr\n\n                store = zarr.storage.ZipStore(store_or_path, mode=\"r\")\n                multiscales = nz.from_ngff_zarr(\n                    store, storage_options=storage_options or {}\n                )\n                # Note: We'll close the store after extracting metadata\n            else:\n                multiscales = nz.from_ngff_zarr(\n                    store_or_path, storage_options=storage_options or {}\n                )\n        else:\n            multiscales = nz.from_ngff_zarr(store_or_path)\n    except Exception as e:\n        # Fallback for older zarr/ngff_zarr versions\n        if isinstance(store_or_path, str):\n            if store_or_path.endswith(\".zip\"):\n                import zarr\n\n                store = zarr.storage.ZipStore(store_or_path, mode=\"r\")\n                multiscales = nz.from_ngff_zarr(store)\n            else:\n                store = fsspec.get_mapper(store_or_path, **storage_options or {})\n                multiscales = nz.from_ngff_zarr(store)\n        else:\n            store = store_or_path\n            multiscales = nz.from_ngff_zarr(store)\n\n    # Extract omero metadata if available\n    omero_metadata = None\n    try:\n        import zarr\n\n        if isinstance(store_or_path, str):\n            if store_or_path.endswith(\".zip\"):\n                zip_store = zarr.storage.ZipStore(store_or_path, mode=\"r\")\n                group = zarr.open_group(zip_store, mode=\"r\")\n                # Close zip store after getting group\n                zip_store.close()\n            else:\n                group = zarr.open_group(store_or_path, mode=\"r\")\n\n        else:\n            group = zarr.open_group(store_or_path, mode=\"r\")\n\n        if \"omero\" in group.attrs:\n            omero_dict = group.attrs[\"omero\"]\n\n            # Create a simple object to hold omero metadata\n            class OmeroMetadata:\n                def __init__(self, omero_dict):\n                    self.channels = []\n                    if \"channels\" in omero_dict:\n                        for ch_dict in omero_dict[\"channels\"]:\n                            # Create channel objects\n                            class ChannelMetadata:\n                                def __init__(self, ch_dict):\n                                    self.label = ch_dict.get(\"label\", \"\")\n                                    self.color = ch_dict.get(\"color\", \"\")\n                                    if \"window\" in ch_dict:\n\n                                        class WindowMetadata:\n                                            def __init__(self, win_dict):\n                                                self.min = win_dict.get(\"min\", 0.0)\n                                                self.max = win_dict.get(\n                                                    \"max\", 65535.0\n                                                )\n                                                self.start = win_dict.get(\n                                                    \"start\", 0.0\n                                                )\n                                                self.end = win_dict.get(\n                                                    \"end\", 65535.0\n                                                )\n\n                                        self.window = WindowMetadata(\n                                            ch_dict[\"window\"]\n                                        )\n                                    else:\n                                        self.window = None\n\n                            self.channels.append(ChannelMetadata(ch_dict))\n\n            omero_metadata = OmeroMetadata(omero_dict)\n    except Exception:\n        # If we can't load omero metadata, that's okay\n        pass\n\n    # Read orientation metadata with backwards compatibility support\n    # Priority: xyz_orientation (new) &gt; orientation (legacy, with reversal)\n    try:\n        import zarr\n\n        if isinstance(store_or_path, str):\n            if store_or_path.endswith(\".zip\"):\n                zip_store = zarr.storage.ZipStore(store_or_path, mode=\"r\")\n                group = zarr.open_group(zip_store, mode=\"r\")\n                # Check for new xyz_orientation first, then fallback to legacy orientation\n                if \"xyz_orientation\" in group.attrs:\n                    orientation = group.attrs[\"xyz_orientation\"]\n                elif \"orientation\" in group.attrs:\n                    # Legacy orientation is ZYX-based, reverse it to get XYZ-based orientation\n                    legacy_orientation = group.attrs[\"orientation\"]\n                    orientation = reverse_orientation_string(legacy_orientation)\n                # If neither found, use the provided default orientation\n                zip_store.close()\n            else:\n                group = zarr.open_group(store_or_path, mode=\"r\")\n                # Check for new xyz_orientation first, then fallback to legacy orientation\n                if \"xyz_orientation\" in group.attrs:\n                    orientation = group.attrs[\"xyz_orientation\"]\n                elif \"orientation\" in group.attrs:\n                    # Legacy orientation is ZYX-based, reverse it to get XYZ-based orientation\n                    legacy_orientation = group.attrs[\"orientation\"]\n                    orientation = reverse_orientation_string(legacy_orientation)\n                # If neither found, use the provided default orientation\n        else:\n            group = zarr.open_group(store_or_path, mode=\"r\")\n            # Check for new xyz_orientation first, then fallback to legacy orientation\n            if \"xyz_orientation\" in group.attrs:\n                orientation = group.attrs[\"xyz_orientation\"]\n            elif \"orientation\" in group.attrs:\n                # Legacy orientation is ZYX-based, reverse it to get XYZ-based orientation\n                legacy_orientation = group.attrs[\"orientation\"]\n                orientation = reverse_orientation_string(legacy_orientation)\n            # If neither found, use the provided default orientation\n\n    except Exception:\n        # If we can't read orientation metadata, use the provided default\n        pass\n\n    # Determine the available pyramid levels and handle lazy downsampling\n    max_level = len(multiscales.images) - 1\n    actual_level = min(level, max_level)\n    do_downsample = level &gt; max_level\n\n    # Get the highest available level\n    ngff_image = multiscales.images[actual_level]\n\n    # Handle channel and timepoint selection and filter omero metadata accordingly\n    filtered_omero = omero_metadata\n    if channels is not None or channel_labels is not None or timepoints is not None:\n        ngff_image, filtered_omero = _select_dimensions_from_image_with_omero(\n            ngff_image,\n            multiscales,\n            channels,\n            channel_labels,\n            timepoints,\n            omero_metadata,\n        )\n\n    # Create ZarrNii instance with xyz_orientation\n    znimg = cls(\n        ngff_image=ngff_image,\n        axes_order=axes_order,\n        xyz_orientation=orientation,\n        _omero=filtered_omero,\n    )\n\n    # Apply lazy downsampling if needed\n    if do_downsample:\n        level_ds = level - max_level\n        downsample_factor = 2**level_ds\n\n        # Get spatial dims based on axes order\n        spatial_dims = [\"z\", \"y\", \"x\"] if axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n\n        # Apply downsampling using the existing method\n        znimg = znimg.downsample(\n            factors=downsample_factor, spatial_dims=spatial_dims\n        )\n\n    # Apply near-isotropic downsampling if requested\n    if downsample_near_isotropic:\n        znimg = _apply_near_isotropic_downsampling(znimg, axes_order)\n\n    if rechunk:\n        znimg.data = znimg.data.rechunk(chunks)\n\n    return znimg\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.get_affine_matrix","title":"<code>get_affine_matrix(axes_order=None)</code>","text":"<p>Get 4x4 affine transformation matrix from NgffImage metadata.</p> <p>Parameters:</p> Name Type Description Default <code>axes_order</code> <code>str</code> <p>Spatial axes order, defaults to self.axes_order</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>4x4 affine transformation matrix</p> Source code in <code>zarrnii/core.py</code> <pre><code>def get_affine_matrix(self, axes_order: str = None) -&gt; np.ndarray:\n    \"\"\"\n    Get 4x4 affine transformation matrix from NgffImage metadata.\n\n    Args:\n        axes_order: Spatial axes order, defaults to self.axes_order\n\n    Returns:\n        4x4 affine transformation matrix\n    \"\"\"\n    if axes_order is None:\n        axes_order = self.axes_order\n\n    # Create identity 4x4 matrix\n    affine = np.eye(4)\n\n    # Map axes order to matrix indices\n    spatial_dims = [\"z\", \"y\", \"x\"] if axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n\n    # Set scale values\n    for i, dim in enumerate(spatial_dims):\n        if dim in self.ngff_image.scale:\n            affine[i, i] = self.ngff_image.scale[dim]\n\n    # Set translation values\n    for i, dim in enumerate(spatial_dims):\n        if dim in self.ngff_image.translation:\n            affine[i, 3] = self.ngff_image.translation[dim]\n\n    # Apply orientation alignment if orientation is available\n    if hasattr(self, \"orientation\") and self.orientation:\n        affine = align_affine_to_input_orientation(affine, self.orientation)\n\n    return affine\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.get_affine_transform","title":"<code>get_affine_transform(axes_order=None)</code>","text":"<p>Get AffineTransform object from NgffImage metadata.</p> <p>Parameters:</p> Name Type Description Default <code>axes_order</code> <code>str</code> <p>Spatial axes order, defaults to self.axes_order</p> <code>None</code> <p>Returns:</p> Type Description <code>AffineTransform</code> <p>AffineTransform object</p> Source code in <code>zarrnii/core.py</code> <pre><code>def get_affine_transform(self, axes_order: str = None) -&gt; AffineTransform:\n    \"\"\"\n    Get AffineTransform object from NgffImage metadata.\n\n    Args:\n        axes_order: Spatial axes order, defaults to self.axes_order\n\n    Returns:\n        AffineTransform object\n    \"\"\"\n    matrix = self.get_affine_matrix(axes_order)\n    return AffineTransform.from_array(matrix)\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.get_orientation","title":"<code>get_orientation()</code>","text":"<p>Get orientation string from affine matrix.</p> Source code in <code>zarrnii/core.py</code> <pre><code>def get_orientation(self):\n    \"\"\"Get orientation string from affine matrix.\"\"\"\n    return affine_to_orientation(self.get_affine_matrix())\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.get_origin","title":"<code>get_origin(axes_order=None)</code>","text":"<p>Get origin (translation) from NgffImage.</p> <p>Parameters:</p> Name Type Description Default <code>axes_order</code> <code>str</code> <p>Spatial axes order, defaults to self.axes_order</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of origin coordinates</p> Source code in <code>zarrnii/core.py</code> <pre><code>def get_origin(self, axes_order: str = None) -&gt; np.ndarray:\n    \"\"\"\n    Get origin (translation) from NgffImage.\n\n    Args:\n        axes_order: Spatial axes order, defaults to self.axes_order\n\n    Returns:\n        Array of origin coordinates\n    \"\"\"\n    if axes_order is None:\n        axes_order = self.axes_order\n\n    spatial_dims = [\"z\", \"y\", \"x\"] if axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n    origin = []\n\n    for dim in spatial_dims:\n        if dim in self.ngff_image.translation:\n            origin.append(self.ngff_image.translation[dim])\n        else:\n            origin.append(0.0)\n\n    return np.array(origin)\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.get_zooms","title":"<code>get_zooms(axes_order=None)</code>","text":"<p>Get voxel spacing (zooms) from NgffImage scale.</p> <p>Parameters:</p> Name Type Description Default <code>axes_order</code> <code>str</code> <p>Spatial axes order, defaults to self.axes_order</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of voxel spacings</p> Source code in <code>zarrnii/core.py</code> <pre><code>def get_zooms(self, axes_order: str = None) -&gt; np.ndarray:\n    \"\"\"\n    Get voxel spacing (zooms) from NgffImage scale.\n\n    Args:\n        axes_order: Spatial axes order, defaults to self.axes_order\n\n    Returns:\n        Array of voxel spacings\n    \"\"\"\n    if axes_order is None:\n        axes_order = self.axes_order\n\n    spatial_dims = [\"z\", \"y\", \"x\"] if axes_order == \"ZYX\" else [\"x\", \"y\", \"z\"]\n    zooms = []\n\n    for dim in spatial_dims:\n        if dim in self.ngff_image.scale:\n            zooms.append(self.ngff_image.scale[dim])\n        else:\n            zooms.append(1.0)\n\n    return np.array(zooms)\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.list_channels","title":"<code>list_channels()</code>","text":"<p>Get list of available channel labels from OMERO metadata.</p> <p>Extracts channel labels from OMERO metadata if available, providing human-readable names for multi-channel datasets.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of channel label strings. Empty list if no OMERO metadata</p> <code>List[str]</code> <p>is available or no channels are defined.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Check available channels\n&gt;&gt;&gt; labels = znii.list_channels()\n&gt;&gt;&gt; print(f\"Available channels: {labels}\")\n&gt;&gt;&gt; # ['DAPI', 'GFP', 'RFP', 'Cy5']\n</code></pre> <pre><code>&gt;&gt;&gt; # Select specific channels by label\n&gt;&gt;&gt; selected = znii.select_channels(channel_labels=['DAPI', 'GFP'])\n</code></pre> Notes <ul> <li>Requires OMERO metadata to be present in the dataset</li> <li>Returns empty list for datasets without channel metadata</li> <li>Labels are extracted from the 'label' field of each channel</li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def list_channels(self) -&gt; List[str]:\n    \"\"\"Get list of available channel labels from OMERO metadata.\n\n    Extracts channel labels from OMERO metadata if available, providing\n    human-readable names for multi-channel datasets.\n\n    Returns:\n        List of channel label strings. Empty list if no OMERO metadata\n        is available or no channels are defined.\n\n    Examples:\n        &gt;&gt;&gt; # Check available channels\n        &gt;&gt;&gt; labels = znii.list_channels()\n        &gt;&gt;&gt; print(f\"Available channels: {labels}\")\n        &gt;&gt;&gt; # ['DAPI', 'GFP', 'RFP', 'Cy5']\n\n        &gt;&gt;&gt; # Select specific channels by label\n        &gt;&gt;&gt; selected = znii.select_channels(channel_labels=['DAPI', 'GFP'])\n\n    Notes:\n        - Requires OMERO metadata to be present in the dataset\n        - Returns empty list for datasets without channel metadata\n        - Labels are extracted from the 'label' field of each channel\n    \"\"\"\n    if self.omero is None or not hasattr(self.omero, \"channels\"):\n        return []\n\n    return [\n        ch.label if hasattr(ch, \"label\") else ch.get(\"label\", \"\")\n        for ch in self.omero.channels\n    ]\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.segment","title":"<code>segment(plugin, chunk_size=None, **kwargs)</code>","text":"<p>Apply segmentation plugin to the image using blockwise processing.</p> <p>This method applies a segmentation plugin to the image data using dask's blockwise processing for efficient computation on large datasets.</p> <p>Parameters:</p> Name Type Description Default <code>plugin</code> <p>Segmentation plugin instance or class to apply</p> required <code>chunk_size</code> <code>Optional[Tuple[int, ...]]</code> <p>Optional chunk size for dask processing. If None, uses current chunks.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to the plugin</p> <code>{}</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>New ZarrNii instance with segmented data as labels</p> Source code in <code>zarrnii/core.py</code> <pre><code>def segment(\n    self, plugin, chunk_size: Optional[Tuple[int, ...]] = None, **kwargs\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Apply segmentation plugin to the image using blockwise processing.\n\n    This method applies a segmentation plugin to the image data using dask's\n    blockwise processing for efficient computation on large datasets.\n\n    Args:\n        plugin: Segmentation plugin instance or class to apply\n        chunk_size: Optional chunk size for dask processing. If None, uses current chunks.\n        **kwargs: Additional arguments passed to the plugin\n\n    Returns:\n        New ZarrNii instance with segmented data as labels\n    \"\"\"\n    from .plugins.segmentation import SegmentationPlugin\n\n    # Handle plugin instance or class\n    if isinstance(plugin, type) and issubclass(plugin, SegmentationPlugin):\n        plugin = plugin(**kwargs)\n    elif not isinstance(plugin, SegmentationPlugin):\n        raise TypeError(\n            \"Plugin must be an instance or subclass of SegmentationPlugin\"\n        )\n\n    # Prepare chunk size\n    if chunk_size is not None:\n        # Rechunk the data if different chunk size requested\n        data = self.data.rechunk(chunk_size)\n    else:\n        data = self.data\n\n    # Create metadata dict to pass to plugin\n    metadata = {\n        \"axes_order\": self.axes_order,\n        \"orientation\": self.xyz_orientation,\n        \"shape\": self.shape,\n        \"dims\": self.dims,\n        \"scale\": self.scale,\n        \"translation\": self.translation,\n    }\n\n    # Create a wrapper function for map_blocks\n    def segment_block(block):\n        \"\"\"Wrapper function to apply segmentation to a single block.\"\"\"\n        # Handle single blocks\n        return plugin.segment(block, metadata)\n\n    # Apply segmentation using dask map_blocks\n    segmented_data = da.map_blocks(\n        segment_block,\n        data,\n        dtype=np.uint8,  # Segmentation results are typically uint8\n        meta=np.array([], dtype=np.uint8),  # Provide meta information\n    )\n\n    # Create new NgffImage with segmented data\n    new_ngff_image = nz.NgffImage(\n        data=segmented_data,\n        dims=self.dims.copy(),\n        scale=self.scale.copy(),\n        translation=self.translation.copy(),\n        name=f\"{self.name}_segmented_{plugin.name.lower().replace(' ', '_')}\",\n    )\n\n    # Return new ZarrNii instance\n    return ZarrNii(\n        ngff_image=new_ngff_image,\n        axes_order=self.axes_order,\n        xyz_orientation=self.xyz_orientation,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.segment_otsu","title":"<code>segment_otsu(nbins=256, chunk_size=None)</code>","text":"<p>Apply local Otsu thresholding segmentation to the image.</p> <p>Convenience method for local Otsu thresholding segmentation. This computes the threshold locally for each processing block.</p> <p>Parameters:</p> Name Type Description Default <code>nbins</code> <code>int</code> <p>Number of bins for histogram computation (default: 256)</p> <code>256</code> <code>chunk_size</code> <code>Optional[Tuple[int, ...]]</code> <p>Optional chunk size for dask processing</p> <code>None</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>New ZarrNii instance with binary segmentation</p> Source code in <code>zarrnii/core.py</code> <pre><code>def segment_otsu(\n    self, nbins: int = 256, chunk_size: Optional[Tuple[int, ...]] = None\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Apply local Otsu thresholding segmentation to the image.\n\n    Convenience method for local Otsu thresholding segmentation.\n    This computes the threshold locally for each processing block.\n\n    Args:\n        nbins: Number of bins for histogram computation (default: 256)\n        chunk_size: Optional chunk size for dask processing\n\n    Returns:\n        New ZarrNii instance with binary segmentation\n    \"\"\"\n    from .plugins.segmentation import LocalOtsuSegmentation\n\n    plugin = LocalOtsuSegmentation(nbins=nbins)\n    return self.segment(plugin, chunk_size=chunk_size)\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.segment_threshold","title":"<code>segment_threshold(thresholds, inclusive=True, chunk_size=None)</code>","text":"<p>Apply threshold-based segmentation to the image.</p> <p>Convenience method for threshold-based segmentation using either manual threshold values or computed thresholds.</p> <p>Parameters:</p> Name Type Description Default <code>thresholds</code> <code>Union[float, List[float]]</code> <p>Single threshold value or list of threshold values. For single threshold, creates binary segmentation (0/1). For multiple thresholds, creates multi-class segmentation (0/1/2/...).</p> required <code>inclusive</code> <code>bool</code> <p>Whether thresholds are inclusive (default: True). If True, pixels &gt;= threshold are labeled as foreground. If False, pixels &gt; threshold are labeled as foreground.</p> <code>True</code> <code>chunk_size</code> <code>Optional[Tuple[int, ...]]</code> <p>Optional chunk size for dask processing</p> <code>None</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>New ZarrNii instance with labeled segmentation</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Binary threshold segmentation\n&gt;&gt;&gt; segmented = znimg.segment_threshold(0.5)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Multi-level threshold segmentation\n&gt;&gt;&gt; thresholds = znimg.compute_otsu_thresholds(classes=3)\n&gt;&gt;&gt; segmented = znimg.segment_threshold(thresholds[1:-1])  # Exclude min/max values\n</code></pre> Source code in <code>zarrnii/core.py</code> <pre><code>def segment_threshold(\n    self,\n    thresholds: Union[float, List[float]],\n    inclusive: bool = True,\n    chunk_size: Optional[Tuple[int, ...]] = None,\n) -&gt; \"ZarrNii\":\n    \"\"\"\n    Apply threshold-based segmentation to the image.\n\n    Convenience method for threshold-based segmentation using either\n    manual threshold values or computed thresholds.\n\n    Args:\n        thresholds: Single threshold value or list of threshold values.\n            For single threshold, creates binary segmentation (0/1).\n            For multiple thresholds, creates multi-class segmentation (0/1/2/...).\n        inclusive: Whether thresholds are inclusive (default: True).\n            If True, pixels &gt;= threshold are labeled as foreground.\n            If False, pixels &gt; threshold are labeled as foreground.\n        chunk_size: Optional chunk size for dask processing\n\n    Returns:\n        New ZarrNii instance with labeled segmentation\n\n    Examples:\n        &gt;&gt;&gt; # Binary threshold segmentation\n        &gt;&gt;&gt; segmented = znimg.segment_threshold(0.5)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Multi-level threshold segmentation\n        &gt;&gt;&gt; thresholds = znimg.compute_otsu_thresholds(classes=3)\n        &gt;&gt;&gt; segmented = znimg.segment_threshold(thresholds[1:-1])  # Exclude min/max values\n    \"\"\"\n    from .plugins.segmentation import ThresholdSegmentation\n\n    plugin = ThresholdSegmentation(thresholds=thresholds, inclusive=inclusive)\n    return self.segment(plugin, chunk_size=chunk_size)\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.select_channels","title":"<code>select_channels(channels=None, channel_labels=None)</code>","text":"<p>Select specific channels from multi-channel image data.</p> <p>Creates a new ZarrNii instance containing only the specified channels, reducing memory usage and focusing analysis on channels of interest. Supports selection by both numeric indices and human-readable labels.</p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>Optional[List[int]]</code> <p>List of 0-based channel indices to select. Mutually exclusive with channel_labels</p> <code>None</code> <code>channel_labels</code> <code>Optional[List[str]]</code> <p>List of channel names to select by label. Requires OMERO metadata. Mutually exclusive with channels</p> <code>None</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>New ZarrNii instance with selected channels and updated metadata</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both channels and channel_labels specified, or if channel_labels used without OMERO metadata, or if labels not found</p> <code>IndexError</code> <p>If channel indices are out of range</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Select channels by index\n&gt;&gt;&gt; selected = znii.select_channels(channels=[0, 2])\n</code></pre> <pre><code>&gt;&gt;&gt; # Select channels by label (requires OMERO metadata)\n&gt;&gt;&gt; selected = znii.select_channels(channel_labels=['DAPI', 'GFP'])\n</code></pre> <pre><code>&gt;&gt;&gt; # Check available labels first\n&gt;&gt;&gt; available = znii.list_channels()\n&gt;&gt;&gt; print(f\"Available: {available}\")\n&gt;&gt;&gt; selected = znii.select_channels(channel_labels=available[:2])\n</code></pre> Notes <ul> <li>Preserves all spatial dimensions and timepoints</li> <li>Updates OMERO metadata to reflect selected channels</li> <li>Maintains spatial transformations and other metadata</li> <li>Channel order in output matches selection order</li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def select_channels(\n    self,\n    channels: Optional[List[int]] = None,\n    channel_labels: Optional[List[str]] = None,\n) -&gt; \"ZarrNii\":\n    \"\"\"Select specific channels from multi-channel image data.\n\n    Creates a new ZarrNii instance containing only the specified channels,\n    reducing memory usage and focusing analysis on channels of interest.\n    Supports selection by both numeric indices and human-readable labels.\n\n    Args:\n        channels: List of 0-based channel indices to select.\n            Mutually exclusive with channel_labels\n        channel_labels: List of channel names to select by label.\n            Requires OMERO metadata. Mutually exclusive with channels\n\n    Returns:\n        New ZarrNii instance with selected channels and updated metadata\n\n    Raises:\n        ValueError: If both channels and channel_labels specified, or if\n            channel_labels used without OMERO metadata, or if labels not found\n        IndexError: If channel indices are out of range\n\n    Examples:\n        &gt;&gt;&gt; # Select channels by index\n        &gt;&gt;&gt; selected = znii.select_channels(channels=[0, 2])\n\n        &gt;&gt;&gt; # Select channels by label (requires OMERO metadata)\n        &gt;&gt;&gt; selected = znii.select_channels(channel_labels=['DAPI', 'GFP'])\n\n        &gt;&gt;&gt; # Check available labels first\n        &gt;&gt;&gt; available = znii.list_channels()\n        &gt;&gt;&gt; print(f\"Available: {available}\")\n        &gt;&gt;&gt; selected = znii.select_channels(channel_labels=available[:2])\n\n    Notes:\n        - Preserves all spatial dimensions and timepoints\n        - Updates OMERO metadata to reflect selected channels\n        - Maintains spatial transformations and other metadata\n        - Channel order in output matches selection order\n    \"\"\"\n    if channels is not None and channel_labels is not None:\n        raise ValueError(\"Cannot specify both 'channels' and 'channel_labels'\")\n\n    if channel_labels is not None:\n        if self.omero is None:\n            raise ValueError(\n                \"Channel labels were specified but no omero metadata found\"\n            )\n\n        available_labels = self.list_channels()\n        channel_indices = []\n        for label in channel_labels:\n            if label not in available_labels:\n                raise ValueError(f\"Channel label '{label}' not found\")\n            channel_indices.append(available_labels.index(label))\n        channels = channel_indices\n\n    if channels is None:\n        # Return a copy with all channels\n        return self.copy()\n\n    # Check if channel dimension exists\n    if \"c\" not in self.dims:\n        raise ValueError(\"No channel dimension found in the data\")\n\n    # Get channel dimension index\n    c_idx = self.dims.index(\"c\")\n\n    # Create slice objects for proper dimension indexing\n    slices = [slice(None)] * len(self.data.shape)\n    slices[c_idx] = channels\n\n    # Select channels from data using proper dimension indexing\n    selected_data = self.data[tuple(slices)]\n\n    # Create new NgffImage with selected data\n    new_ngff_image = nz.NgffImage(\n        data=selected_data,\n        dims=self.dims,\n        scale=self.scale,\n        translation=self.translation,\n        name=self.name,\n    )\n\n    # Filter omero metadata to match selected channels\n    filtered_omero = None\n    if self.omero is not None and hasattr(self.omero, \"channels\"):\n\n        class FilteredOmero:\n            def __init__(self, channels):\n                self.channels = channels\n\n        filtered_channels = [self.omero.channels[i] for i in channels]\n        filtered_omero = FilteredOmero(filtered_channels)\n\n    return ZarrNii(\n        ngff_image=new_ngff_image,\n        axes_order=self.axes_order,\n        xyz_orientation=self.xyz_orientation,\n        _omero=filtered_omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.select_timepoints","title":"<code>select_timepoints(timepoints=None)</code>","text":"<p>Select timepoints from the image data and return a new ZarrNii instance.</p> <p>Parameters:</p> Name Type Description Default <code>timepoints</code> <code>Optional[List[int]]</code> <p>Timepoint indices to select</p> <code>None</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>New ZarrNii instance with selected timepoints</p> Source code in <code>zarrnii/core.py</code> <pre><code>def select_timepoints(self, timepoints: Optional[List[int]] = None) -&gt; \"ZarrNii\":\n    \"\"\"\n    Select timepoints from the image data and return a new ZarrNii instance.\n\n    Args:\n        timepoints: Timepoint indices to select\n\n    Returns:\n        New ZarrNii instance with selected timepoints\n    \"\"\"\n    if timepoints is None:\n        # Return a copy with all timepoints\n        return self.copy()\n\n    # Check if time dimension exists\n    if \"t\" not in self.dims:\n        raise ValueError(\"No time dimension found in the data\")\n\n    # Get time dimension index\n    t_idx = self.dims.index(\"t\")\n\n    # Create slice objects\n    slices = [slice(None)] * len(self.data.shape)\n    slices[t_idx] = timepoints\n\n    # Select timepoints from data\n    selected_data = self.data[tuple(slices)]\n\n    # Create new NgffImage with selected data\n    new_ngff_image = nz.NgffImage(\n        data=selected_data,\n        dims=self.dims,\n        scale=self.scale,\n        translation=self.translation,\n        name=self.name,\n    )\n\n    return ZarrNii(\n        ngff_image=new_ngff_image,\n        axes_order=self.axes_order,\n        xyz_orientation=self.xyz_orientation,\n        _omero=self._omero,  # Timepoint selection doesn't affect omero metadata\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.to_imaris","title":"<code>to_imaris(path, compression='gzip', compression_opts=6)</code>","text":"<p>Save to Imaris (.ims) file format using HDF5.</p> <p>This method creates Imaris files compatible with Imaris software by following the exact HDF5 structure from correctly-formed reference files. All attributes use byte-array encoding as required by Imaris.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Output path for Imaris (.ims) file</p> required <code>compression</code> <code>str</code> <p>HDF5 compression method (default: \"gzip\")</p> <code>'gzip'</code> <code>compression_opts</code> <code>int</code> <p>Compression level (default: 6)</p> <code>6</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the saved file</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If h5py is not available</p> Source code in <code>zarrnii/core.py</code> <pre><code>def to_imaris(\n    self, path: str, compression: str = \"gzip\", compression_opts: int = 6\n) -&gt; str:\n    \"\"\"\n    Save to Imaris (.ims) file format using HDF5.\n\n    This method creates Imaris files compatible with Imaris software by\n    following the exact HDF5 structure from correctly-formed reference files.\n    All attributes use byte-array encoding as required by Imaris.\n\n    Args:\n        path: Output path for Imaris (.ims) file\n        compression: HDF5 compression method (default: \"gzip\")\n        compression_opts: Compression level (default: 6)\n\n    Returns:\n        str: Path to the saved file\n\n    Raises:\n        ImportError: If h5py is not available\n    \"\"\"\n    try:\n        import h5py\n    except ImportError:\n        raise ImportError(\n            \"h5py is required for Imaris support. \"\n            \"Install with: pip install zarrnii[imaris] or pip install h5py\"\n        )\n\n    # Ensure path has .ims extension\n    if not path.endswith(\".ims\"):\n        path = path + \".ims\"\n\n    def _string_to_byte_array(s: str) -&gt; np.ndarray:\n        \"\"\"Convert string to byte array as required by Imaris.\"\"\"\n        return np.array([c.encode() for c in s])\n\n    # Get data and metadata\n    if hasattr(self.darr, \"compute\"):\n        data = self.darr.compute()  # Convert Dask array to numpy array\n    else:\n        data = np.asarray(self.darr)  # Handle numpy arrays directly\n\n    # Handle dimensions: expect ZYX or CZYX\n    if len(data.shape) == 4:\n        # CZYX format\n        n_channels = data.shape[0]\n        z, y, x = data.shape[1:]\n    elif len(data.shape) == 3:\n        # ZYX format - single channel\n        n_channels = 1\n        z, y, x = data.shape\n        data = data[np.newaxis, ...]  # Add channel dimension\n    else:\n        raise ValueError(\n            f\"Unsupported data shape: {data.shape}. Expected 3D (ZYX) or 4D (CZYX)\"\n        )\n\n    # Create Imaris file structure exactly matching reference file\n    with h5py.File(path, \"w\") as f:\n        # Root attributes - use exact byte array format from reference\n        f.attrs[\"DataSetDirectoryName\"] = _string_to_byte_array(\"DataSet\")\n        f.attrs[\"DataSetInfoDirectoryName\"] = _string_to_byte_array(\"DataSetInfo\")\n        f.attrs[\"ImarisDataSet\"] = _string_to_byte_array(\"ImarisDataSet\")\n        f.attrs[\"ImarisVersion\"] = _string_to_byte_array(\"5.5.0\")\n        f.attrs[\"NumberOfDataSets\"] = np.array([1], dtype=np.uint32)\n        f.attrs[\"ThumbnailDirectoryName\"] = _string_to_byte_array(\"Thumbnail\")\n\n        # Create main DataSet group structure\n        dataset_group = f.create_group(\"DataSet\")\n        res_group = dataset_group.create_group(\"ResolutionLevel 0\")\n        time_group = res_group.create_group(\"TimePoint 0\")\n\n        # Create channels with proper attributes\n        for c in range(n_channels):\n            channel_group = time_group.create_group(f\"Channel {c}\")\n            channel_data = data[c]  # (Z, Y, X)\n\n            # Channel attributes - use byte array format exactly like reference\n            channel_group.attrs[\"ImageSizeX\"] = _string_to_byte_array(str(x))\n            channel_group.attrs[\"ImageSizeY\"] = _string_to_byte_array(str(y))\n            channel_group.attrs[\"ImageSizeZ\"] = _string_to_byte_array(str(z))\n            channel_group.attrs[\"ImageBlockSizeX\"] = _string_to_byte_array(str(x))\n            channel_group.attrs[\"ImageBlockSizeY\"] = _string_to_byte_array(str(y))\n            channel_group.attrs[\"ImageBlockSizeZ\"] = _string_to_byte_array(\n                str(min(z, 16))\n            )\n\n            # Histogram range attributes\n            data_min, data_max = float(channel_data.min()), float(\n                channel_data.max()\n            )\n            channel_group.attrs[\"HistogramMin\"] = _string_to_byte_array(\n                f\"{data_min:.3f}\"\n            )\n            channel_group.attrs[\"HistogramMax\"] = _string_to_byte_array(\n                f\"{data_max:.3f}\"\n            )\n\n            # Create data dataset with proper compression\n            # Preserve original data type but ensure it's compatible with Imaris\n            if channel_data.dtype == np.float32 or channel_data.dtype == np.float64:\n                # Keep float data as is for round-trip compatibility\n                data_for_storage = channel_data.astype(np.float32)\n            elif channel_data.dtype in [np.uint16, np.int16]:\n                # Keep 16-bit data as is\n                data_for_storage = channel_data\n            else:\n                # Convert other types to uint8\n                data_for_storage = channel_data.astype(np.uint8)\n\n            channel_group.create_dataset(\n                \"Data\",\n                data=data_for_storage,\n                compression=compression,\n                compression_opts=compression_opts,\n                chunks=True,\n            )\n\n            # Create histogram\n            hist_data, _ = np.histogram(\n                channel_data.flatten(), bins=256, range=(data_min, data_max)\n            )\n            channel_group.create_dataset(\n                \"Histogram\", data=hist_data.astype(np.uint64)\n            )\n\n        # Get spacing directly from scale dictionary with proper XYZ order\n        try:\n            # Extract voxel sizes directly from ngff_image scale dictionary\n            # This ensures we get X, Y, Z in the correct order regardless of axes_order\n            sx = self.ngff_image.scale.get(\"x\", 1.0)\n            sy = self.ngff_image.scale.get(\"y\", 1.0)\n            sz = self.ngff_image.scale.get(\"z\", 1.0)\n        except:\n            sx = sy = sz = 1.0\n\n        # Calculate extents (physical coordinates)\n        ext_x = sx * x\n        ext_y = sy * y\n        ext_z = sz * z\n\n        # Create comprehensive DataSetInfo structure matching reference\n        info_group = f.create_group(\"DataSetInfo\")\n\n        # Create channel info groups\n        for c in range(n_channels):\n            channel_info = info_group.create_group(f\"Channel {c}\")\n\n            # Essential channel attributes in byte array format\n            channel_info.attrs[\"Color\"] = _string_to_byte_array(\n                \"1.000 0.000 0.000\"\n                if c == 0\n                else f\"0.000 {1.0 if c == 1 else 0.0:.3f} {1.0 if c == 2 else 0.0:.3f}\"\n            )\n            channel_info.attrs[\"Name\"] = _string_to_byte_array(f\"Channel {c}\")\n            channel_info.attrs[\"ColorMode\"] = _string_to_byte_array(\"BaseColor\")\n            channel_info.attrs[\"ColorOpacity\"] = _string_to_byte_array(\"1.000\")\n            channel_info.attrs[\"ColorRange\"] = _string_to_byte_array(\"0 255\")\n            channel_info.attrs[\"GammaCorrection\"] = _string_to_byte_array(\"1.000\")\n            channel_info.attrs[\"LSMEmissionWavelength\"] = _string_to_byte_array(\n                \"500\"\n            )\n            channel_info.attrs[\"LSMExcitationWavelength\"] = _string_to_byte_array(\n                \"500\"\n            )\n            channel_info.attrs[\"LSMPhotons\"] = _string_to_byte_array(\"1\")\n            channel_info.attrs[\"LSMPinhole\"] = _string_to_byte_array(\"0\")\n\n            # Add description\n            description = f\"Channel {c} created by ZarrNii\"\n            channel_info.attrs[\"Description\"] = _string_to_byte_array(description)\n\n        # Create CRITICAL Image group with voxel size information (this was missing!)\n        image_info = info_group.create_group(\"Image\")\n\n        # Add essential image metadata with proper voxel size information\n        image_info.attrs[\"X\"] = _string_to_byte_array(str(x))\n        image_info.attrs[\"Y\"] = _string_to_byte_array(str(y))\n        image_info.attrs[\"Z\"] = _string_to_byte_array(str(z))\n        image_info.attrs[\"Unit\"] = _string_to_byte_array(\"um\")\n        image_info.attrs[\"Noc\"] = _string_to_byte_array(str(n_channels))\n\n        # CRITICAL: Set proper physical extents that define voxel size\n        # Imaris reads voxel size from these extent values\n        image_info.attrs[\"ExtMin0\"] = _string_to_byte_array(f\"{-ext_x/2:.3f}\")\n        image_info.attrs[\"ExtMax0\"] = _string_to_byte_array(f\"{ext_x/2:.3f}\")\n        image_info.attrs[\"ExtMin1\"] = _string_to_byte_array(f\"{-ext_y/2:.3f}\")\n        image_info.attrs[\"ExtMax1\"] = _string_to_byte_array(f\"{ext_y/2:.3f}\")\n        image_info.attrs[\"ExtMin2\"] = _string_to_byte_array(f\"{-ext_z/2:.3f}\")\n        image_info.attrs[\"ExtMax2\"] = _string_to_byte_array(f\"{ext_z/2:.3f}\")\n\n        # Add device/acquisition metadata\n        image_info.attrs[\"ManufactorString\"] = _string_to_byte_array(\"ZarrNii\")\n        image_info.attrs[\"ManufactorType\"] = _string_to_byte_array(\"Generic\")\n        image_info.attrs[\"LensPower\"] = _string_to_byte_array(\"\")\n        image_info.attrs[\"NumericalAperture\"] = _string_to_byte_array(\"\")\n        image_info.attrs[\"RecordingDate\"] = _string_to_byte_array(\n            \"2024-01-01 00:00:00.000\"\n        )\n        image_info.attrs[\"Filename\"] = _string_to_byte_array(path.split(\"/\")[-1])\n        image_info.attrs[\"Name\"] = _string_to_byte_array(\"ZarrNii Export\")\n        image_info.attrs[\"Compression\"] = _string_to_byte_array(\"5794\")\n\n        # Add description\n        description = (\n            f\"Imaris file created by ZarrNii from {self.axes_order} format data. \"\n            f\"Original shape: {self.darr.shape}. Converted to Imaris format \"\n            f\"with {n_channels} channel(s) and dimensions {z}x{y}x{x}. \"\n            f\"Voxel size: {sx:.3f} x {sy:.3f} x {sz:.3f} um.\"\n        )\n        image_info.attrs[\"Description\"] = _string_to_byte_array(description)\n\n        # Create Imaris metadata group\n        imaris_info = info_group.create_group(\"Imaris\")\n        imaris_info.attrs[\"Version\"] = _string_to_byte_array(\"7.0\")\n        imaris_info.attrs[\"ThumbnailMode\"] = _string_to_byte_array(\"thumbnailMIP\")\n        imaris_info.attrs[\"ThumbnailSize\"] = _string_to_byte_array(\"256\")\n\n        # Create ImarisDataSet metadata\n        dataset_info = info_group.create_group(\"ImarisDataSet\")\n        dataset_info.attrs[\"Creator\"] = _string_to_byte_array(\"Imaris\")\n        dataset_info.attrs[\"Version\"] = _string_to_byte_array(\"7.0\")\n        dataset_info.attrs[\"NumberOfImages\"] = _string_to_byte_array(\"1\")\n\n        # Add version-specific groups as seen in reference\n        dataset_info_ver = info_group.create_group(\"ImarisDataSet       0.0.0\")\n        dataset_info_ver.attrs[\"NumberOfImages\"] = _string_to_byte_array(\"1\")\n        dataset_info_ver2 = info_group.create_group(\"ImarisDataSet      0.0.0\")\n        dataset_info_ver2.attrs[\"NumberOfImages\"] = _string_to_byte_array(\"1\")\n\n        # Create TimeInfo group\n        time_info = info_group.create_group(\"TimeInfo\")\n        time_info.attrs[\"DatasetTimePoints\"] = _string_to_byte_array(\"1\")\n        time_info.attrs[\"FileTimePoints\"] = _string_to_byte_array(\"1\")\n        time_info.attrs[\"TimePoint1\"] = _string_to_byte_array(\n            \"2024-01-01 00:00:00.000\"\n        )\n\n        # Create Log group (basic processing log)\n        log_group = info_group.create_group(\"Log\")\n        log_group.attrs[\"Entries\"] = _string_to_byte_array(\"1\")\n        log_group.attrs[\"Entry0\"] = _string_to_byte_array(\n            f\"&lt;ZarrNiiExport channels=\\\"{' '.join(['on'] * n_channels)}\\\"/&gt;\"\n        )\n\n        # Create thumbnail group with proper multi-channel thumbnail\n        thumbnail_group = f.create_group(\"Thumbnail\")\n\n        # Create a combined thumbnail (256x1024 for multi-channel as in reference)\n        if n_channels &gt; 1:\n            # Multi-channel thumbnail: concatenate channels horizontally\n            thumb_width = 256 * n_channels\n            thumbnail_data = np.zeros((256, thumb_width), dtype=np.uint8)\n\n            for c in range(n_channels):\n                # Downsample each channel to 256x256\n                channel_data = data[c]\n                # Take MIP (Maximum Intensity Projection) along Z\n                mip = np.max(channel_data, axis=0)\n                # Resize to 256x256 (simple decimation)\n                step_y = max(1, mip.shape[0] // 256)\n                step_x = max(1, mip.shape[1] // 256)\n                thumb_channel = mip[::step_y, ::step_x]\n\n                # Pad or crop to exactly 256x256\n                if thumb_channel.shape[0] &lt; 256 or thumb_channel.shape[1] &lt; 256:\n                    padded = np.zeros((256, 256), dtype=thumb_channel.dtype)\n                    h, w = thumb_channel.shape\n                    padded[:h, :w] = thumb_channel\n                    thumb_channel = padded\n                else:\n                    thumb_channel = thumb_channel[:256, :256]\n\n                # Place in thumbnail\n                thumbnail_data[:, c * 256 : (c + 1) * 256] = thumb_channel\n        else:\n            # Single channel: 256x256 thumbnail\n            channel_data = data[0]\n            mip = np.max(channel_data, axis=0)\n            step_y = max(1, mip.shape[0] // 256)\n            step_x = max(1, mip.shape[1] // 256)\n            thumbnail_data = mip[::step_y, ::step_x]\n\n            if thumbnail_data.shape[0] &lt; 256 or thumbnail_data.shape[1] &lt; 256:\n                padded = np.zeros((256, 256), dtype=thumbnail_data.dtype)\n                h, w = thumbnail_data.shape\n                padded[:h, :w] = thumbnail_data\n                thumbnail_data = padded\n            else:\n                thumbnail_data = thumbnail_data[:256, :256]\n\n        thumbnail_group.create_dataset(\"Data\", data=thumbnail_data.astype(np.uint8))\n\n    return path\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.to_ngff_image","title":"<code>to_ngff_image(name=None)</code>","text":"<p>Convert to NgffImage object.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Optional name for the image</p> <code>None</code> <p>Returns:</p> Type Description <code>NgffImage</code> <p>NgffImage representation</p> Source code in <code>zarrnii/core.py</code> <pre><code>def to_ngff_image(self, name: str = None) -&gt; nz.NgffImage:\n    \"\"\"\n    Convert to NgffImage object.\n\n    Args:\n        name: Optional name for the image\n\n    Returns:\n        NgffImage representation\n    \"\"\"\n    if name is None:\n        name = self.name\n\n    return nz.NgffImage(\n        data=self.data,\n        dims=self.dims,\n        scale=self.scale,\n        translation=self.translation,\n        name=name,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.to_nifti","title":"<code>to_nifti(filename=None)</code>","text":"<p>Convert to NIfTI format with automatic dimension handling.</p> <p>Converts the ZarrNii image to NIfTI-1 format, handling dimension reordering, singleton dimension removal, and spatial transformation conversion. NIfTI files are always written in XYZ axis order.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>Optional[Union[str, bytes]]</code> <p>Output file path for saving. Supported extensions: - .nii: Uncompressed NIfTI - .nii.gz: Compressed NIfTI (recommended) If None, returns nibabel image object without saving</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Nifti1Image, str]</code> <p>If filename is None: nibabel.Nifti1Image object</p> <code>Union[Nifti1Image, str]</code> <p>If filename provided: path to saved file</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data has non-singleton time or channel dimensions (NIfTI doesn't support &gt;4D data)</p> <code>OSError</code> <p>If unable to write to specified filename</p> Notes <ul> <li>Automatically reorders data from ZYX to XYZ if necessary</li> <li>Removes singleton time/channel dimensions automatically</li> <li>Spatial transformations are converted to NIfTI affine format</li> <li>For 5D data (T,C,Z,Y,X), only singleton T/C dimensions are supported</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Save to compressed NIfTI file\n&gt;&gt;&gt; znii.to_nifti(\"output.nii.gz\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Get nibabel object without saving\n&gt;&gt;&gt; nifti_img = znii.to_nifti()\n&gt;&gt;&gt; print(nifti_img.shape)\n</code></pre> <pre><code>&gt;&gt;&gt; # Handle multi-channel data by selecting single channel first\n&gt;&gt;&gt; znii.select_channels([0]).to_nifti(\"channel0.nii.gz\")\n</code></pre> Source code in <code>zarrnii/core.py</code> <pre><code>def to_nifti(\n    self, filename: Optional[Union[str, bytes]] = None\n) -&gt; Union[nib.Nifti1Image, str]:\n    \"\"\"Convert to NIfTI format with automatic dimension handling.\n\n    Converts the ZarrNii image to NIfTI-1 format, handling dimension\n    reordering, singleton dimension removal, and spatial transformation\n    conversion. NIfTI files are always written in XYZ axis order.\n\n    Args:\n        filename: Output file path for saving. Supported extensions:\n            - .nii: Uncompressed NIfTI\n            - .nii.gz: Compressed NIfTI (recommended)\n            If None, returns nibabel image object without saving\n\n    Returns:\n        If filename is None: nibabel.Nifti1Image object\n        If filename provided: path to saved file\n\n    Raises:\n        ValueError: If data has non-singleton time or channel dimensions\n            (NIfTI doesn't support &gt;4D data)\n        OSError: If unable to write to specified filename\n\n    Notes:\n        - Automatically reorders data from ZYX to XYZ if necessary\n        - Removes singleton time/channel dimensions automatically\n        - Spatial transformations are converted to NIfTI affine format\n        - For 5D data (T,C,Z,Y,X), only singleton T/C dimensions are supported\n\n    Examples:\n        &gt;&gt;&gt; # Save to compressed NIfTI file\n        &gt;&gt;&gt; znii.to_nifti(\"output.nii.gz\")\n\n        &gt;&gt;&gt; # Get nibabel object without saving\n        &gt;&gt;&gt; nifti_img = znii.to_nifti()\n        &gt;&gt;&gt; print(nifti_img.shape)\n\n        &gt;&gt;&gt; # Handle multi-channel data by selecting single channel first\n        &gt;&gt;&gt; znii.select_channels([0]).to_nifti(\"channel0.nii.gz\")\n\n    Warnings:\n        Large images will be computed in memory during conversion.\n        Consider downsampling or cropping first for very large datasets.\n    \"\"\"\n    # Get data and dimensions\n    data = self.data.compute()\n\n    dims = self.dims\n\n    # Handle dimensional reduction for NIfTI compatibility\n    # NIfTI supports up to 4D, so we need to remove singleton dimensions\n    squeeze_axes = []\n    remaining_dims = []\n\n    for i, dim in enumerate(dims):\n        if dim in [\"t\", \"c\"] and data.shape[i] == 1:\n            # Remove singleton time or channel dimensions\n            squeeze_axes.append(i)\n        elif dim in [\"t\", \"c\"] and data.shape[i] &gt; 1:\n            # Non-singleton time or channel dimensions - NIfTI can't handle this\n            raise ValueError(\n                f\"NIfTI format doesn't support non-singleton {dim} dimension. \"\n                f\"Dimension '{dim}' has size {data.shape[i]}. \"\n                f\"Consider selecting specific timepoints/channels first.\"\n            )\n        else:\n            remaining_dims.append(dim)\n\n    # Squeeze out singleton dimensions\n    if squeeze_axes:\n        data = np.squeeze(data, axis=tuple(squeeze_axes))\n\n    # Check final dimensionality\n    if data.ndim &gt; 4:\n        raise ValueError(\n            f\"Resulting data has {data.ndim} dimensions, but NIfTI supports maximum 4D\"\n        )\n\n    # Now handle spatial reordering based on axes_order\n    if self.axes_order == \"ZYX\":\n        # Data spatial dimensions are in ZYX order, need to transpose to XYZ\n        if data.ndim == 3:\n            # Pure spatial data: ZYX -&gt; XYZ\n            data = data.transpose(2, 1, 0)\n        elif data.ndim == 4:\n            # 4D data with one non-spatial dimension remaining\n            # Could be (T,Z,Y,X) or (C,Z,Y,X) - spatial part needs ZYX-&gt;XYZ\n            # The non-spatial dimension stays first\n            data = data.transpose(0, 3, 2, 1)\n\n        # Get affine matrix in XYZ order\n        affine_matrix = self.get_affine_matrix(axes_order=\"XYZ\")\n    else:\n        # Data is already in XYZ order\n        affine_matrix = self.get_affine_matrix(axes_order=\"XYZ\")\n\n    # Create NIfTI image\n    nifti_img = nib.Nifti1Image(data, affine_matrix)\n\n    if filename is not None:\n        nib.save(nifti_img, filename)\n        return filename\n    else:\n        return nifti_img\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.to_ome_zarr","title":"<code>to_ome_zarr(store_or_path, max_layer=4, scale_factors=None, **kwargs)</code>","text":"<p>Save to OME-Zarr store with multiscale pyramid.</p> <p>Creates an OME-Zarr dataset with automatic multiscale pyramid generation for efficient visualization and processing at multiple resolutions. Preserves spatial metadata and supports various storage backends.</p> <p>Parameters:</p> Name Type Description Default <code>store_or_path</code> <code>Union[str, Any]</code> <p>Target location for OME-Zarr store. Supports: - Local directory path - Remote URLs (s3://, gs://, etc.) - ZIP files (.zip extension for compressed storage) - Zarr store objects</p> required <code>max_layer</code> <code>int</code> <p>Maximum number of pyramid levels to create (including level 0). Higher values create more downsampled levels</p> <code>4</code> <code>scale_factors</code> <code>Optional[List[int]]</code> <p>Custom downsampling factors for each pyramid level. If None, uses powers of 2: [2, 4, 8, 16, ...]</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to underlying to_ngff_zarr function. May include compression options, chunk sizes, etc.</p> <code>{}</code> <p>Returns:</p> Type Description <code>'ZarrNii'</code> <p>Self for method chaining</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If unable to write to target location</p> <code>ValueError</code> <p>If invalid scale_factors provided</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Save with default pyramid levels\n&gt;&gt;&gt; znii.to_ome_zarr(\"/path/to/output.zarr\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Save to compressed ZIP with custom pyramid\n&gt;&gt;&gt; znii.to_ome_zarr(\n...     \"/path/to/output.zarr.zip\",\n...     max_layer=3,\n...     scale_factors=[2, 4]\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Chain with other operations\n&gt;&gt;&gt; result = (znii.downsample(2)\n...               .crop((0,0,0), (100,100,100))\n...               .to_ome_zarr(\"processed.zarr\"))\n</code></pre> Notes <ul> <li>OME-Zarr files are always saved in ZYX axis order</li> <li>Automatic axis reordering if current order is XYZ</li> <li>Spatial transformations and metadata are preserved</li> <li>Orientation information is stored using the new 'xyz_orientation'   metadata key for consistency and future compatibility</li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def to_ome_zarr(\n    self,\n    store_or_path: Union[str, Any],\n    max_layer: int = 4,\n    scale_factors: Optional[List[int]] = None,\n    **kwargs: Any,\n) -&gt; \"ZarrNii\":\n    \"\"\"Save to OME-Zarr store with multiscale pyramid.\n\n    Creates an OME-Zarr dataset with automatic multiscale pyramid generation\n    for efficient visualization and processing at multiple resolutions.\n    Preserves spatial metadata and supports various storage backends.\n\n    Args:\n        store_or_path: Target location for OME-Zarr store. Supports:\n            - Local directory path\n            - Remote URLs (s3://, gs://, etc.)\n            - ZIP files (.zip extension for compressed storage)\n            - Zarr store objects\n        max_layer: Maximum number of pyramid levels to create (including level 0).\n            Higher values create more downsampled levels\n        scale_factors: Custom downsampling factors for each pyramid level.\n            If None, uses powers of 2: [2, 4, 8, 16, ...]\n        **kwargs: Additional arguments passed to underlying to_ngff_zarr function.\n            May include compression options, chunk sizes, etc.\n\n    Returns:\n        Self for method chaining\n\n    Raises:\n        OSError: If unable to write to target location\n        ValueError: If invalid scale_factors provided\n\n    Examples:\n        &gt;&gt;&gt; # Save with default pyramid levels\n        &gt;&gt;&gt; znii.to_ome_zarr(\"/path/to/output.zarr\")\n\n        &gt;&gt;&gt; # Save to compressed ZIP with custom pyramid\n        &gt;&gt;&gt; znii.to_ome_zarr(\n        ...     \"/path/to/output.zarr.zip\",\n        ...     max_layer=3,\n        ...     scale_factors=[2, 4]\n        ... )\n\n        &gt;&gt;&gt; # Chain with other operations\n        &gt;&gt;&gt; result = (znii.downsample(2)\n        ...               .crop((0,0,0), (100,100,100))\n        ...               .to_ome_zarr(\"processed.zarr\"))\n\n    Notes:\n        - OME-Zarr files are always saved in ZYX axis order\n        - Automatic axis reordering if current order is XYZ\n        - Spatial transformations and metadata are preserved\n        - Orientation information is stored using the new 'xyz_orientation'\n          metadata key for consistency and future compatibility\n    \"\"\"\n    # Determine the image to save\n    if self.axes_order == \"XYZ\":\n        # Need to reorder data from XYZ to ZYX for OME-Zarr\n        ngff_image_to_save = self._create_zyx_ngff_image()\n    else:\n        # Already in ZYX order\n        ngff_image_to_save = self.ngff_image\n\n    save_ngff_image(\n        ngff_image_to_save,\n        store_or_path,\n        max_layer,\n        scale_factors,\n        xyz_orientation=(\n            self.xyz_orientation if hasattr(self, \"xyz_orientation\") else None\n        ),\n        **kwargs,\n    )\n\n    # Add orientation metadata to the zarr store (only for non-ZIP files)\n    # For ZIP files, orientation is handled inside save_ngff_image\n    if not (isinstance(store_or_path, str) and store_or_path.endswith(\".zip\")):\n        try:\n            import zarr\n\n            if isinstance(store_or_path, str):\n                group = zarr.open_group(store_or_path, mode=\"r+\")\n            else:\n                group = zarr.open_group(store_or_path, mode=\"r+\")\n\n            # Add metadata for xyz_orientation (new format)\n            if hasattr(self, \"xyz_orientation\") and self.xyz_orientation:\n                group.attrs[\"xyz_orientation\"] = self.xyz_orientation\n        except Exception:\n            # If we can't write orientation metadata, that's not critical\n            pass\n\n    return self\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.to_tiff_stack","title":"<code>to_tiff_stack(filename_pattern, channel=None, timepoint=None, compress=True, dtype='uint16', rescale=True)</code>","text":"<p>Save data as a stack of 2D TIFF images.</p> <p>Saves the image data as a series of 2D TIFF files, with each Z-slice saved as a separate file. This format is useful for compatibility with tools that don't support OME-Zarr or napari plugins that require individual TIFF files.</p> <p>Parameters:</p> Name Type Description Default <code>filename_pattern</code> <code>str</code> <p>Output filename pattern. Should contain '{z:04d}' or similar format specifier for the Z-slice number. Examples: - \"output_z{z:04d}.tif\" - \"data/slice_{z:03d}.tiff\" If pattern doesn't contain format specifier, '_{z:04d}' is appended before the extension.</p> required <code>channel</code> <code>Optional[int]</code> <p>Channel index to save (0-based). If None and data has multiple channels, all channels will be saved as separate channel dimensions in each TIFF file (multi-channel TIFFs).</p> <code>None</code> <code>timepoint</code> <code>Optional[int]</code> <p>Timepoint index to save (0-based). If None and data has multiple timepoints, raises ValueError (must select single timepoint).</p> <code>None</code> <code>compress</code> <code>bool</code> <p>Whether to use LZW compression (default: True)</p> <code>True</code> <code>dtype</code> <code>Optional[str]</code> <p>Output data type for TIFF files. Options: - 'uint8': 8-bit unsigned integer (0-255) - 'uint16': 16-bit unsigned integer (0-65535) [default] - 'int16': 16-bit signed integer (-32768 to 32767) - 'float32': 32-bit float (preserves original data) Default 'uint16' provides good range and compatibility.</p> <code>'uint16'</code> <code>rescale</code> <code>bool</code> <p>Whether to rescale data to fit the output dtype range. If True, data is linearly scaled from [min, max] to the full range of the output dtype. If False, data is clipped to the output dtype range. Default: True</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Base directory path where files were saved</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data has multiple timepoints but none selected, or if selected channel/timepoint is out of range, or if dtype is not supported</p> <code>OSError</code> <p>If unable to write to specified directory</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Save as 16-bit with auto-rescaling (default, recommended)\n&gt;&gt;&gt; znii.to_tiff_stack(\"output_z{z:04d}.tif\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Save as 8-bit for smaller file sizes\n&gt;&gt;&gt; znii.to_tiff_stack(\"output_z{z:04d}.tif\", dtype='uint8')\n</code></pre> <pre><code>&gt;&gt;&gt; # Save specific channel without rescaling\n&gt;&gt;&gt; znii.to_tiff_stack(\"channel0_z{z:04d}.tif\", channel=0, rescale=False)\n</code></pre> <pre><code>&gt;&gt;&gt; # Save as float32 to preserve original precision\n&gt;&gt;&gt; znii.to_tiff_stack(\"precise_z{z:04d}.tif\", dtype='float32')\n</code></pre> Notes <ul> <li>Z-dimension becomes the stack (file) dimension</li> <li>Time and channel dimensions are handled as specified</li> <li>Spatial transformations are not preserved in TIFF format</li> <li>For 5D data (T,C,Z,Y,X), you must select a single timepoint</li> <li>Multi-channel data can be saved as multi-channel TIFFs or selected</li> <li>Data type conversion helps ensure compatibility with analysis tools</li> <li>uint16 is recommended for most scientific applications (good range + compatibility)</li> </ul> Source code in <code>zarrnii/core.py</code> <pre><code>def to_tiff_stack(\n    self,\n    filename_pattern: str,\n    channel: Optional[int] = None,\n    timepoint: Optional[int] = None,\n    compress: bool = True,\n    dtype: Optional[str] = \"uint16\",\n    rescale: bool = True,\n) -&gt; str:\n    \"\"\"Save data as a stack of 2D TIFF images.\n\n    Saves the image data as a series of 2D TIFF files, with each Z-slice\n    saved as a separate file. This format is useful for compatibility with\n    tools that don't support OME-Zarr or napari plugins that require\n    individual TIFF files.\n\n    Args:\n        filename_pattern: Output filename pattern. Should contain '{z:04d}' or similar\n            format specifier for the Z-slice number. Examples:\n            - \"output_z{z:04d}.tif\"\n            - \"data/slice_{z:03d}.tiff\"\n            If pattern doesn't contain format specifier, '_{z:04d}' is appended\n            before the extension.\n        channel: Channel index to save (0-based). If None and data has multiple\n            channels, all channels will be saved as separate channel dimensions\n            in each TIFF file (multi-channel TIFFs).\n        timepoint: Timepoint index to save (0-based). If None and data has multiple\n            timepoints, raises ValueError (must select single timepoint).\n        compress: Whether to use LZW compression (default: True)\n        dtype: Output data type for TIFF files. Options:\n            - 'uint8': 8-bit unsigned integer (0-255)\n            - 'uint16': 16-bit unsigned integer (0-65535) [default]\n            - 'int16': 16-bit signed integer (-32768 to 32767)\n            - 'float32': 32-bit float (preserves original data)\n            Default 'uint16' provides good range and compatibility.\n        rescale: Whether to rescale data to fit the output dtype range.\n            If True, data is linearly scaled from [min, max] to the full\n            range of the output dtype. If False, data is clipped to the\n            output dtype range. Default: True\n\n    Returns:\n        Base directory path where files were saved\n\n    Raises:\n        ValueError: If data has multiple timepoints but none selected,\n            or if selected channel/timepoint is out of range,\n            or if dtype is not supported\n        OSError: If unable to write to specified directory\n\n    Examples:\n        &gt;&gt;&gt; # Save as 16-bit with auto-rescaling (default, recommended)\n        &gt;&gt;&gt; znii.to_tiff_stack(\"output_z{z:04d}.tif\")\n\n        &gt;&gt;&gt; # Save as 8-bit for smaller file sizes\n        &gt;&gt;&gt; znii.to_tiff_stack(\"output_z{z:04d}.tif\", dtype='uint8')\n\n        &gt;&gt;&gt; # Save specific channel without rescaling\n        &gt;&gt;&gt; znii.to_tiff_stack(\"channel0_z{z:04d}.tif\", channel=0, rescale=False)\n\n        &gt;&gt;&gt; # Save as float32 to preserve original precision\n        &gt;&gt;&gt; znii.to_tiff_stack(\"precise_z{z:04d}.tif\", dtype='float32')\n\n    Warnings:\n        This method loads all data into memory. For large datasets,\n        consider cropping or downsampling first to reduce memory usage.\n        The cellseg3d napari plugin and similar tools work best with\n        cropped regions rather than full-resolution whole-brain images.\n\n    Notes:\n        - Z-dimension becomes the stack (file) dimension\n        - Time and channel dimensions are handled as specified\n        - Spatial transformations are not preserved in TIFF format\n        - For 5D data (T,C,Z,Y,X), you must select a single timepoint\n        - Multi-channel data can be saved as multi-channel TIFFs or selected\n        - Data type conversion helps ensure compatibility with analysis tools\n        - uint16 is recommended for most scientific applications (good range + compatibility)\n    \"\"\"\n    try:\n        import tifffile\n    except ImportError:\n        raise ImportError(\n            \"tifffile is required for TIFF stack support. \"\n            \"Install with: pip install tifffile\"\n        )\n\n    # Get data and dimensions\n    data = self.data.compute()\n    dims = self.dims\n\n    # Create output directory if needed\n    import os\n\n    output_dir = os.path.dirname(filename_pattern)\n    if output_dir and not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Handle dimensional selection and validation\n    # Remove singleton dimensions first, similar to to_nifti\n    squeeze_axes = []\n    remaining_dims = []\n    time_dim_size = 1\n    channel_dim_size = 1\n\n    for i, dim in enumerate(dims):\n        if dim == \"t\":\n            time_dim_size = data.shape[i]\n            if data.shape[i] == 1:\n                squeeze_axes.append(i)\n            elif timepoint is None:\n                raise ValueError(\n                    f\"Data has {data.shape[i]} timepoints. \"\n                    f\"Must specify 'timepoint' parameter to select a single timepoint.\"\n                )\n            elif timepoint &gt;= data.shape[i]:\n                raise ValueError(\n                    f\"Timepoint {timepoint} is out of range (data has {data.shape[i]} timepoints)\"\n                )\n            else:\n                remaining_dims.append(dim)\n        elif dim == \"c\":\n            channel_dim_size = data.shape[i]\n            if data.shape[i] == 1:\n                squeeze_axes.append(i)\n            elif channel is None:\n                raise ValueError(\n                    f\"Data has {data.shape[i]} channels. \"\n                    f\"Must specify 'channel' parameter to select a single channel.\"\n                )\n            elif channel &gt;= data.shape[i]:\n                raise ValueError(\n                    f\"Channel {channel} is out of range (data has {data.shape[i]} channels)\"\n                )\n            else:\n                remaining_dims.append(dim)\n        else:\n            remaining_dims.append(dim)\n\n    # Select specific timepoint if needed\n    if time_dim_size &gt; 1 and timepoint is not None:\n        time_axis = dims.index(\"t\")\n        data = np.take(data, timepoint, axis=time_axis)\n        # Update dims list\n        dims = [d for i, d in enumerate(dims) if i != time_axis]\n\n    # Select specific channel if needed\n    if channel_dim_size &gt; 1 and channel is not None:\n        channel_axis = dims.index(\"c\")\n        data = np.take(data, channel, axis=channel_axis)\n        # Update dims list\n        dims = [d for i, d in enumerate(dims) if i != channel_axis]\n\n    # Squeeze singleton dimensions\n    if squeeze_axes:\n        # Recalculate squeeze axes after potential dimension removal\n        current_squeeze_axes = []\n        for axis in squeeze_axes:\n            # Count how many axes were removed before this one\n            removed_before = sum(\n                1\n                for removed_axis in [\n                    (\n                        dims.index(\"t\")\n                        if time_dim_size &gt; 1 and timepoint is not None\n                        else -1\n                    ),\n                    (\n                        dims.index(\"c\")\n                        if channel_dim_size &gt; 1 and channel is not None\n                        else -1\n                    ),\n                ]\n                if removed_axis != -1 and removed_axis &lt; axis\n            )\n            current_squeeze_axes.append(axis - removed_before)\n\n        data = np.squeeze(data, axis=tuple(current_squeeze_axes))\n        dims = [dim for i, dim in enumerate(dims) if i not in current_squeeze_axes]\n\n    # Find Z dimension for stacking\n    if \"z\" not in dims:\n        raise ValueError(\"Data must have a Z dimension for TIFF stack export\")\n\n    z_axis = dims.index(\"z\")\n    z_size = data.shape[z_axis]\n\n    # Check filename pattern contains format specifier\n    if \"{z\" not in filename_pattern:\n        # Add default z format before extension\n        name, ext = os.path.splitext(filename_pattern)\n        filename_pattern = f\"{name}_{{z:04d}}{ext}\"\n\n    # Move Z axis to first position for easy iteration\n    axes_order = list(range(data.ndim))\n    axes_order[0], axes_order[z_axis] = axes_order[z_axis], axes_order[0]\n    data = data.transpose(axes_order)\n\n    # Handle data type conversion and rescaling\n    supported_dtypes = {\n        \"uint8\": np.uint8,\n        \"uint16\": np.uint16,\n        \"int16\": np.int16,\n        \"float32\": np.float32,\n    }\n\n    if dtype not in supported_dtypes:\n        raise ValueError(\n            f\"Unsupported dtype '{dtype}'. Supported types: {list(supported_dtypes.keys())}\"\n        )\n\n    target_dtype = supported_dtypes[dtype]\n\n    if rescale and dtype != \"float32\":\n        # Get the data range\n        data_min = np.min(data)\n        data_max = np.max(data)\n\n        if data_min == data_max:\n            # Handle constant data case\n            data_scaled = np.zeros_like(data, dtype=target_dtype)\n        else:\n            # Get target range for the dtype\n            if dtype == \"uint8\":\n                target_min, target_max = 0, 255\n            elif dtype == \"uint16\":\n                target_min, target_max = 0, 65535\n            elif dtype == \"int16\":\n                target_min, target_max = -32768, 32767\n\n            # Linear rescaling: new_value = (value - data_min) * (target_max - target_min) / (data_max - data_min) + target_min\n            data_scaled = (\n                (data - data_min)\n                * (target_max - target_min)\n                / (data_max - data_min)\n                + target_min\n            ).astype(target_dtype)\n\n        print(\n            f\"Rescaled data from [{data_min:.3f}, {data_max:.3f}] to {dtype} range\"\n        )\n    else:\n        # No rescaling - just clip and convert\n        if dtype == \"uint8\":\n            data_scaled = np.clip(data, 0, 255).astype(target_dtype)\n        elif dtype == \"uint16\":\n            data_scaled = np.clip(data, 0, 65535).astype(target_dtype)\n        elif dtype == \"int16\":\n            data_scaled = np.clip(data, -32768, 32767).astype(target_dtype)\n        else:  # float32\n            data_scaled = data.astype(target_dtype)\n\n        if dtype != \"float32\":\n            print(f\"Converted data to {dtype} with clipping (no rescaling)\")\n\n    data = data_scaled\n\n    # Save each Z-slice as a separate TIFF file\n    compression = \"lzw\" if compress else None\n    saved_files = []\n\n    for z_idx in range(z_size):\n        slice_data = data[z_idx]\n\n        # Generate filename for this slice\n        filename = filename_pattern.format(z=z_idx)\n\n        # Save the 2D slice\n        tifffile.imwrite(filename, slice_data, compression=compression)\n        saved_files.append(filename)\n\n    print(f\"Saved {len(saved_files)} TIFF files to {output_dir or '.'}\")\n    print(\n        f\"Files: {os.path.basename(saved_files[0])} ... {os.path.basename(saved_files[-1])}\"\n    )\n\n    return output_dir or \".\"\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.upsample","title":"<code>upsample(along_x=1, along_y=1, along_z=1, to_shape=None)</code>","text":"<p>Upsamples the ZarrNii instance using <code>scipy.ndimage.zoom</code>.</p> <p>Parameters:</p> Name Type Description Default <code>along_x</code> <code>int</code> <p>Upsampling factor along the X-axis (default: 1).</p> <code>1</code> <code>along_y</code> <code>int</code> <p>Upsampling factor along the Y-axis (default: 1).</p> <code>1</code> <code>along_z</code> <code>int</code> <p>Upsampling factor along the Z-axis (default: 1).</p> <code>1</code> <code>to_shape</code> <code>tuple</code> <p>Target shape for upsampling. Should include all dimensions                          (e.g., <code>(c, z, y, x)</code> for ZYX or <code>(c, x, y, z)</code> for XYZ).                          If provided, <code>along_x</code>, <code>along_y</code>, and <code>along_z</code> are ignored.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ZarrNii</code> <p>A new ZarrNii instance with the upsampled data and updated affine.</p> Notes <ul> <li>This method supports both direct scaling via <code>along_*</code> factors or target shape via <code>to_shape</code>.</li> <li>If <code>to_shape</code> is provided, chunk sizes and scaling factors are dynamically calculated.</li> <li>The affine matrix is updated to reflect the new voxel size after upsampling.</li> </ul> Example Source code in <code>zarrnii/core.py</code> <pre><code>def upsample(self, along_x=1, along_y=1, along_z=1, to_shape=None):\n    \"\"\"\n    Upsamples the ZarrNii instance using `scipy.ndimage.zoom`.\n\n    Parameters:\n        along_x (int, optional): Upsampling factor along the X-axis (default: 1).\n        along_y (int, optional): Upsampling factor along the Y-axis (default: 1).\n        along_z (int, optional): Upsampling factor along the Z-axis (default: 1).\n        to_shape (tuple, optional): Target shape for upsampling. Should include all dimensions\n                                     (e.g., `(c, z, y, x)` for ZYX or `(c, x, y, z)` for XYZ).\n                                     If provided, `along_x`, `along_y`, and `along_z` are ignored.\n\n    Returns:\n        ZarrNii: A new ZarrNii instance with the upsampled data and updated affine.\n\n    Notes:\n        - This method supports both direct scaling via `along_*` factors or target shape via `to_shape`.\n        - If `to_shape` is provided, chunk sizes and scaling factors are dynamically calculated.\n        - The affine matrix is updated to reflect the new voxel size after upsampling.\n\n    Example:\n        # Upsample with scaling factors\n        upsampled_znimg = znimg.upsample(along_x=2, along_y=2, along_z=2)\n\n        # Upsample to a specific shape\n        upsampled_znimg = znimg.upsample(to_shape=(1, 256, 256, 256))\n    \"\"\"\n    # Determine scaling and chunks based on input parameters\n    if to_shape is None:\n        if self.axes_order == \"XYZ\":\n            scaling = (1, along_x, along_y, along_z)\n        else:\n            scaling = (1, along_z, along_y, along_x)\n\n        chunks_out = tuple(\n            tuple(c * scale for c in chunks_i)\n            for chunks_i, scale in zip(self.data.chunks, scaling)\n        )\n    else:\n        chunks_out, scaling = self.__get_upsampled_chunks(to_shape)\n\n    # Define block-wise upsampling function\n    def zoom_blocks(x, block_info=None):\n        \"\"\"\n        Scales blocks to the desired size using `scipy.ndimage.zoom`.\n\n        Parameters:\n            x (np.ndarray): Input block data.\n            block_info (dict, optional): Metadata about the current block.\n\n        Returns:\n            np.ndarray: The upscaled block.\n        \"\"\"\n        # Calculate scaling factors based on input and output chunk shapes\n        scaling = tuple(\n            out_n / in_n\n            for out_n, in_n in zip(block_info[None][\"chunk-shape\"], x.shape)\n        )\n        return zoom(x, scaling, order=1, prefilter=False)\n\n    # Perform block-wise upsampling\n    darr_scaled = da.map_blocks(\n        zoom_blocks, self.data, dtype=self.data.dtype, chunks=chunks_out\n    )\n\n    # Update the affine matrix to reflect the new voxel size\n    if self.axes_order == \"XYZ\":\n        scaling_matrix = np.diag(\n            (1 / scaling[1], 1 / scaling[2], 1 / scaling[3], 1)\n        )\n    else:\n        scaling_matrix = np.diag(\n            (1 / scaling[-1], 1 / scaling[-2], 1 / scaling[-3], 1)\n        )\n    new_affine = AffineTransform.from_array(scaling_matrix @ self.affine.matrix)\n\n    # Create new NgffImage with upsampled data\n    dims = self.dims\n    if self.axes_order == \"XYZ\":\n        new_scale = {\n            dims[1]: self.scale[dims[1]] / scaling[1],\n            dims[2]: self.scale[dims[2]] / scaling[2],\n            dims[3]: self.scale[dims[3]] / scaling[3],\n        }\n    else:\n        new_scale = {\n            dims[1]: self.scale[dims[1]] / scaling[1],\n            dims[2]: self.scale[dims[2]] / scaling[2],\n            dims[3]: self.scale[dims[3]] / scaling[3],\n        }\n\n    upsampled_ngff = nz.to_ngff_image(\n        darr_scaled,\n        dims=dims,\n        scale=new_scale,\n        translation=self.translation.copy(),\n        name=self.name,\n    )\n\n    # Return a new ZarrNii instance with the upsampled data\n    return ZarrNii.from_ngff_image(\n        upsampled_ngff,\n        axes_order=self.axes_order,\n        xyz_orientation=self.xyz_orientation,\n        omero=self.omero,\n    )\n</code></pre>"},{"location":"reference/#zarrnii.ZarrNii.upsample--upsample-with-scaling-factors","title":"Upsample with scaling factors","text":"<p>upsampled_znimg = znimg.upsample(along_x=2, along_y=2, along_z=2)</p>"},{"location":"reference/#zarrnii.ZarrNii.upsample--upsample-to-a-specific-shape","title":"Upsample to a specific shape","text":"<p>upsampled_znimg = znimg.upsample(to_shape=(1, 256, 256, 256))</p>"},{"location":"reference/#zarrnii.transform.AffineTransform.matrix","title":"<code>matrix = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/#zarrnii.transform.AffineTransform.__array__","title":"<code>__array__()</code>","text":"<p>Convert to numpy array.</p> <p>Defines how the object behaves when converted to a numpy array.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>The 4x4 affine transformation matrix</p> Source code in <code>zarrnii/transform.py</code> <pre><code>def __array__(self) -&gt; np.ndarray:\n    \"\"\"Convert to numpy array.\n\n    Defines how the object behaves when converted to a numpy array.\n\n    Returns:\n        The 4x4 affine transformation matrix\n    \"\"\"\n    return self.matrix\n</code></pre>"},{"location":"reference/#zarrnii.transform.AffineTransform.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Enable array-like indexing on the matrix.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <p>Index or slice for matrix access</p> required <p>Returns:</p> Type Description <code>Union[ndarray, float]</code> <p>Element(s) from the transformation matrix</p> Source code in <code>zarrnii/transform.py</code> <pre><code>def __getitem__(self, key) -&gt; Union[np.ndarray, float]:\n    \"\"\"Enable array-like indexing on the matrix.\n\n    Args:\n        key: Index or slice for matrix access\n\n    Returns:\n        Element(s) from the transformation matrix\n    \"\"\"\n    return self.matrix[key]\n</code></pre>"},{"location":"reference/#zarrnii.transform.AffineTransform.__matmul__","title":"<code>__matmul__(other)</code>","text":"<p>Perform matrix multiplication with another object.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Union[ndarray, 'AffineTransform']</code> <p>The object to multiply with. Supported types: - (3,) or (3, 1): A 3D point or vector (voxel coordinates) - (3, N): A batch of N 3D points or vectors (voxel coordinates) - (4,) or (4, 1): A 4D point/vector in homogeneous coordinates - (4, N): A batch of N 4D points in homogeneous coordinates - (4, 4): Another affine transformation matrix - AffineTransform: Another affine transformation object</p> required <p>Returns:</p> Type Description <code>Union[ndarray, 'AffineTransform']</code> <p>Transformed coordinates as numpy array or new AffineTransform: - For coordinate inputs: transformed coordinates with same shape - For matrix/AffineTransform inputs: new AffineTransform object</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the shape of <code>other</code> is unsupported</p> <code>TypeError</code> <p>If <code>other</code> is not an np.ndarray or AffineTransform</p> Source code in <code>zarrnii/transform.py</code> <pre><code>def __matmul__(\n    self, other: Union[np.ndarray, \"AffineTransform\"]\n) -&gt; Union[np.ndarray, \"AffineTransform\"]:\n    \"\"\"Perform matrix multiplication with another object.\n\n    Args:\n        other: The object to multiply with. Supported types:\n            - (3,) or (3, 1): A 3D point or vector (voxel coordinates)\n            - (3, N): A batch of N 3D points or vectors (voxel coordinates)\n            - (4,) or (4, 1): A 4D point/vector in homogeneous coordinates\n            - (4, N): A batch of N 4D points in homogeneous coordinates\n            - (4, 4): Another affine transformation matrix\n            - AffineTransform: Another affine transformation object\n\n    Returns:\n        Transformed coordinates as numpy array or new AffineTransform:\n            - For coordinate inputs: transformed coordinates with same shape\n            - For matrix/AffineTransform inputs: new AffineTransform object\n\n    Raises:\n        ValueError: If the shape of `other` is unsupported\n        TypeError: If `other` is not an np.ndarray or AffineTransform\n    \"\"\"\n    if isinstance(other, np.ndarray):\n        if other.shape == (3,):\n            # Single 3D point/vector\n            homog_point = np.append(other, 1)  # Convert to homogeneous coordinates\n            result = self.matrix @ homog_point\n            return result[:3] / result[3]  # Convert back to 3D\n        elif len(other.shape) == 2 and other.shape[0] == 3:\n            # Batch of 3D points/vectors (3 x N)\n            homog_points = np.vstack(\n                [other, np.ones((1, other.shape[1]))]\n            )  # Add homogeneous row\n            transformed_points = (\n                self.matrix @ homog_points\n            )  # Apply affine transform\n            return (\n                transformed_points[:3] / transformed_points[3]\n            )  # Convert back to 3D\n        elif other.shape == (4,):\n            # Single 4D point/vector\n            result = self.matrix @ other\n            return result[:3] / result[3]\n        elif len(other.shape) == 2 and other.shape[0] == 4:\n            # Batch of 4D points in homogeneous coordinates (4 x N)\n            transformed_points = self.matrix @ other  # Apply affine transform\n            return transformed_points  # No conversion needed, stays in 4D space\n        elif other.shape == (4, 4):\n            # Matrix multiplication with another affine matrix\n            return AffineTransform.from_array(self.matrix @ other)\n        else:\n            raise ValueError(f\"Unsupported shape for multiplication: {other.shape}\")\n    elif isinstance(other, AffineTransform):\n        # Matrix multiplication with another AffineTransform object\n        return AffineTransform.from_array(self.matrix @ other.matrix)\n    else:\n        raise TypeError(f\"Unsupported type for multiplication: {type(other)}\")\n</code></pre>"},{"location":"reference/#zarrnii.transform.AffineTransform.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Enable array-like assignment to the matrix.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <p>Index or slice for matrix assignment</p> required <code>value</code> <code>Union[float, ndarray]</code> <p>Value(s) to assign to matrix</p> required Source code in <code>zarrnii/transform.py</code> <pre><code>def __setitem__(self, key, value: Union[float, np.ndarray]) -&gt; None:\n    \"\"\"Enable array-like assignment to the matrix.\n\n    Args:\n        key: Index or slice for matrix assignment\n        value: Value(s) to assign to matrix\n    \"\"\"\n    self.matrix[key] = value\n</code></pre>"},{"location":"reference/#zarrnii.transform.AffineTransform.apply_transform","title":"<code>apply_transform(vecs)</code>","text":"<p>Apply transformation to coordinate vectors.</p> <p>Parameters:</p> Name Type Description Default <code>vecs</code> <code>ndarray</code> <p>Input coordinates to transform</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Transformed coordinates</p> Source code in <code>zarrnii/transform.py</code> <pre><code>def apply_transform(self, vecs: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Apply transformation to coordinate vectors.\n\n    Args:\n        vecs: Input coordinates to transform\n\n    Returns:\n        Transformed coordinates\n    \"\"\"\n    return self @ vecs\n</code></pre>"},{"location":"reference/#zarrnii.transform.AffineTransform.from_array","title":"<code>from_array(matrix, invert=False)</code>  <code>classmethod</code>","text":"<p>Create AffineTransform from numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>ndarray</code> <p>4x4 numpy array representing affine transformation</p> required <code>invert</code> <code>bool</code> <p>Whether to invert the matrix</p> <code>False</code> <p>Returns:</p> Type Description <code>'AffineTransform'</code> <p>AffineTransform instance with the matrix</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If matrix is not 4x4</p> Source code in <code>zarrnii/transform.py</code> <pre><code>@classmethod\ndef from_array(cls, matrix: np.ndarray, invert: bool = False) -&gt; \"AffineTransform\":\n    \"\"\"Create AffineTransform from numpy array.\n\n    Args:\n        matrix: 4x4 numpy array representing affine transformation\n        invert: Whether to invert the matrix\n\n    Returns:\n        AffineTransform instance with the matrix\n\n    Raises:\n        ValueError: If matrix is not 4x4\n    \"\"\"\n    if matrix.shape != (4, 4):\n        raise ValueError(f\"Matrix must be 4x4, got shape {matrix.shape}\")\n\n    if invert:\n        matrix = np.linalg.inv(matrix)\n    return cls(matrix=matrix)\n</code></pre>"},{"location":"reference/#zarrnii.transform.AffineTransform.from_txt","title":"<code>from_txt(path, invert=False)</code>  <code>classmethod</code>","text":"<p>Create AffineTransform from text file containing matrix.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, bytes]</code> <p>Path to text file containing 4x4 affine matrix</p> required <code>invert</code> <code>bool</code> <p>Whether to invert the matrix after loading</p> <code>False</code> <p>Returns:</p> Type Description <code>'AffineTransform'</code> <p>AffineTransform instance with loaded matrix</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If file cannot be read</p> <code>ValueError</code> <p>If file does not contain valid 4x4 matrix</p> Source code in <code>zarrnii/transform.py</code> <pre><code>@classmethod\ndef from_txt(\n    cls, path: Union[str, bytes], invert: bool = False\n) -&gt; \"AffineTransform\":\n    \"\"\"Create AffineTransform from text file containing matrix.\n\n    Args:\n        path: Path to text file containing 4x4 affine matrix\n        invert: Whether to invert the matrix after loading\n\n    Returns:\n        AffineTransform instance with loaded matrix\n\n    Raises:\n        OSError: If file cannot be read\n        ValueError: If file does not contain valid 4x4 matrix\n    \"\"\"\n    matrix = np.loadtxt(path)\n    if invert:\n        matrix = np.linalg.inv(matrix)\n    return cls(matrix=matrix)\n</code></pre>"},{"location":"reference/#zarrnii.transform.AffineTransform.identity","title":"<code>identity()</code>  <code>classmethod</code>","text":"<p>Create identity transformation.</p> <p>Returns:</p> Type Description <code>'AffineTransform'</code> <p>AffineTransform representing identity transformation (no change)</p> Source code in <code>zarrnii/transform.py</code> <pre><code>@classmethod\ndef identity(cls) -&gt; \"AffineTransform\":\n    \"\"\"Create identity transformation.\n\n    Returns:\n        AffineTransform representing identity transformation (no change)\n    \"\"\"\n    return cls(matrix=np.eye(4, 4))\n</code></pre>"},{"location":"reference/#zarrnii.transform.AffineTransform.invert","title":"<code>invert()</code>","text":"<p>Return the inverse of the matrix transformation.</p> <p>Returns:</p> Type Description <code>'AffineTransform'</code> <p>New AffineTransform with inverted matrix</p> <p>Raises:</p> Type Description <code>LinAlgError</code> <p>If matrix is singular and cannot be inverted</p> Source code in <code>zarrnii/transform.py</code> <pre><code>def invert(self) -&gt; \"AffineTransform\":\n    \"\"\"Return the inverse of the matrix transformation.\n\n    Returns:\n        New AffineTransform with inverted matrix\n\n    Raises:\n        np.linalg.LinAlgError: If matrix is singular and cannot be inverted\n    \"\"\"\n    return AffineTransform.from_array(np.linalg.inv(self.matrix))\n</code></pre>"},{"location":"reference/#zarrnii.transform.AffineTransform.update_for_orientation","title":"<code>update_for_orientation(input_orientation, output_orientation)</code>","text":"<p>Update the matrix to map from input orientation to output orientation.</p> <p>Parameters:</p> Name Type Description Default <code>input_orientation</code> <code>str</code> <p>Current anatomical orientation (e.g., 'RPI')</p> required <code>output_orientation</code> <code>str</code> <p>Target anatomical orientation (e.g., 'RAS')</p> required <p>Returns:</p> Type Description <code>'AffineTransform'</code> <p>New AffineTransform updated for orientation mapping</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If orientations are invalid or cannot be matched</p> Source code in <code>zarrnii/transform.py</code> <pre><code>def update_for_orientation(\n    self, input_orientation: str, output_orientation: str\n) -&gt; \"AffineTransform\":\n    \"\"\"Update the matrix to map from input orientation to output orientation.\n\n    Args:\n        input_orientation: Current anatomical orientation (e.g., 'RPI')\n        output_orientation: Target anatomical orientation (e.g., 'RAS')\n\n    Returns:\n        New AffineTransform updated for orientation mapping\n\n    Raises:\n        ValueError: If orientations are invalid or cannot be matched\n    \"\"\"\n\n    # Define a mapping of anatomical directions to axis indices and flips\n    axis_map = {\n        \"R\": (0, 1),\n        \"L\": (0, -1),\n        \"A\": (1, 1),\n        \"P\": (1, -1),\n        \"S\": (2, 1),\n        \"I\": (2, -1),\n    }\n\n    # Parse the input and output orientations\n    input_axes = [axis_map[ax] for ax in input_orientation]\n    output_axes = [axis_map[ax] for ax in output_orientation]\n\n    # Create a mapping from input to output\n    reorder_indices = [None] * 3\n    flip_signs = [1] * 3\n\n    for out_idx, (out_axis, out_sign) in enumerate(output_axes):\n        for in_idx, (in_axis, in_sign) in enumerate(input_axes):\n            if out_axis == in_axis:  # Match axis\n                reorder_indices[out_idx] = in_idx\n                flip_signs[out_idx] = out_sign * in_sign\n                break\n\n    # Reorder and flip the affine matrix\n    reordered_matrix = np.zeros_like(self.matrix)\n    for i, (reorder_idx, flip_sign) in enumerate(zip(reorder_indices, flip_signs)):\n        if reorder_idx is None:\n            raise ValueError(\n                f\"Cannot match all axes from {input_orientation} to {output_orientation}.\"\n            )\n        reordered_matrix[i, :3] = flip_sign * self.matrix[reorder_idx, :3]\n        reordered_matrix[i, 3] = flip_sign * self.matrix[reorder_idx, 3]\n    reordered_matrix[3, :] = self.matrix[3, :]  # Preserve the homogeneous row\n\n    return AffineTransform.from_array(reordered_matrix)\n</code></pre>"},{"location":"reference/#zarrnii.transform.DisplacementTransform.disp_affine","title":"<code>disp_affine = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/#zarrnii.transform.DisplacementTransform.disp_grid","title":"<code>disp_grid = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/#zarrnii.transform.DisplacementTransform.disp_xyz","title":"<code>disp_xyz = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/#zarrnii.transform.DisplacementTransform.apply_transform","title":"<code>apply_transform(vecs)</code>","text":"<p>Apply displacement transformation to coordinate vectors.</p> <p>Transforms input coordinates by interpolating displacement vectors from the displacement field and adding them to the input coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>vecs</code> <code>ndarray</code> <p>Input coordinates as numpy array. Shape should be (3, N) for N points or (3,) for single point</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Transformed coordinates with same shape as input</p> Notes <p>Points outside the displacement field domain are filled with zero displacement (no transformation).</p> Source code in <code>zarrnii/transform.py</code> <pre><code>def apply_transform(self, vecs: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Apply displacement transformation to coordinate vectors.\n\n    Transforms input coordinates by interpolating displacement vectors\n    from the displacement field and adding them to the input coordinates.\n\n    Args:\n        vecs: Input coordinates as numpy array. Shape should be (3, N) for\n            N points or (3,) for single point\n\n    Returns:\n        Transformed coordinates with same shape as input\n\n    Notes:\n        Points outside the displacement field domain are filled with\n        zero displacement (no transformation).\n    \"\"\"\n    # Transform points to voxel space of the displacement field\n    vox_vecs = self.disp_affine.invert() @ vecs\n\n    # Initialize displacement vectors\n    disp_vecs = np.zeros(vox_vecs.shape)\n\n    # Interpolate displacement for each spatial dimension (x, y, z)\n    for ax in range(3):\n        disp_vecs[ax, :] = interpn(\n            self.disp_grid,\n            self.disp_xyz[:, :, :, ax].squeeze(),\n            vox_vecs[:3, :].T,\n            method=\"linear\",\n            bounds_error=False,\n            fill_value=0,\n        )\n\n    # Add displacement to original coordinates\n    return vecs + disp_vecs\n</code></pre>"},{"location":"reference/#zarrnii.transform.DisplacementTransform.from_nifti","title":"<code>from_nifti(path)</code>  <code>classmethod</code>","text":"<p>Create DisplacementTransform from NIfTI file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, bytes]</code> <p>Path to NIfTI displacement field file</p> required <p>Returns:</p> Type Description <code>'DisplacementTransform'</code> <p>DisplacementTransform instance loaded from file</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If file cannot be read</p> <code>ValueError</code> <p>If file format is invalid</p> Source code in <code>zarrnii/transform.py</code> <pre><code>@classmethod\ndef from_nifti(cls, path: Union[str, bytes]) -&gt; \"DisplacementTransform\":\n    \"\"\"Create DisplacementTransform from NIfTI file.\n\n    Args:\n        path: Path to NIfTI displacement field file\n\n    Returns:\n        DisplacementTransform instance loaded from file\n\n    Raises:\n        OSError: If file cannot be read\n        ValueError: If file format is invalid\n    \"\"\"\n    disp_nib = nib.load(path)\n    disp_xyz = disp_nib.get_fdata().squeeze()\n    disp_affine = AffineTransform.from_array(disp_nib.affine)\n\n    # Convert from ITK transform convention\n    # ITK uses opposite sign convention for x and y displacements\n    disp_xyz[:, :, :, 0] = -disp_xyz[:, :, :, 0]\n    disp_xyz[:, :, :, 1] = -disp_xyz[:, :, :, 1]\n\n    # Create grid coordinates\n    disp_grid = (\n        np.arange(disp_xyz.shape[0]),\n        np.arange(disp_xyz.shape[1]),\n        np.arange(disp_xyz.shape[2]),\n    )\n\n    return cls(\n        disp_xyz=disp_xyz,\n        disp_grid=disp_grid,\n        disp_affine=disp_affine,\n    )\n</code></pre>"},{"location":"examples/atlas_example/","title":"Atlas Module Example","text":"<p>This example demonstrates how to use the atlas functionality in ZarrNii for working with brain atlases and performing region-of-interest (ROI) analysis.</p>"},{"location":"examples/atlas_example/#atlas-focused-functionality","title":"Atlas-Focused Functionality","text":"<p>ZarrNii functionality is solely for atlases (dseg.nii.gz + dseg.tsv files), providing: - Loading atlases from local files or TemplateFlow - Region analysis by index, name, or abbreviation - ROI aggregation with multiple statistical functions - Feature mapping to assign values back to regions - Format conversion utilities (CSV, ITK-SNAP to TSV)</p>"},{"location":"examples/atlas_example/#loading-atlases-from-files","title":"Loading Atlases from Files","text":"<pre><code>from zarrnii import ZarrNiiAtlas\n\n# Load atlas from BIDS-format files\natlas = ZarrNiiAtlas.from_files(\n    dseg_path=\"atlas_dseg.nii.gz\",\n    labels_path=\"atlas_dseg.tsv\"\n)\n\n# Get basic atlas information\nprint(f\"Atlas shape: {atlas.dseg.shape}\")\nprint(f\"Number of regions: {len(atlas.labels_df)}\")\nprint(f\"Region labels: {atlas.labels_df[atlas.label_column].values}\")\n</code></pre>"},{"location":"examples/atlas_example/#loading-atlases-from-templateflow","title":"Loading Atlases from TemplateFlow","text":"<pre><code>from zarrnii import get_atlas, get_template\n\n# Load atlas directly from TemplateFlow (if available)\n# atlas = get_atlas(\"MNI152NLin2009cAsym\", \"DKT\", resolution=1)\n\n# Or load template first\n# template = get_template(\"MNI152NLin2009cAsym\", \"T1w\", resolution=1)\n# Note: These functions require templateflow to be installed\n</code></pre>"},{"location":"examples/atlas_example/#saving-atlases-to-templateflow","title":"Saving Atlases to TemplateFlow","text":"<pre><code>from zarrnii import save_atlas_to_templateflow\n\n# Save atlas to TemplateFlow directory as BIDS-compliant files\n# Requires templateflow to be installed\n# template_dir = save_atlas_to_templateflow(atlas, \"MyTemplate\", \"MyAtlas\")\nprint(f\"Atlas saved to: {template_dir}\")\n# Creates: tpl-MyTemplate_atlas-MyAtlas_dseg.nii.gz and .tsv files\n</code></pre> <ul> <li>Unified API: Access built-in and remote templates through same TemplateFlow interface</li> </ul> <pre><code># Install templateflow extra for lazy loading\n# pip install zarrnii[templateflow]\n\n# Lazy loading - templates copy to TEMPLATEFLOW_HOME on first access\nfrom zarrnii import get_builtin_template\ntemplate = get_builtin_template(\"placeholder\")  # Copies to TemplateFlow on first call\n\n# After lazy loading, both APIs work identically:\nimport templateflow.api as tflow\nzarrnii_template = tflow.get(\"placeholder\", suffix=\"SPIM\")        # zarrnii built-in\nremote_template = tflow.get(\"MNI152NLin2009cAsym\", suffix=\"T1w\")  # remote template\n\n# Manual installation (if preferred over lazy loading)\nfrom zarrnii import install_zarrnii_templates\nresults = install_zarrnii_templates()  # {'placeholder': True}\n</code></pre>"},{"location":"examples/atlas_example/#template-installation-behavior","title":"Template Installation Behavior","text":"<p>With <code>templateflow</code> extra installed: - \u2705 Lazy loading: Templates copy to <code>$TEMPLATEFLOW_HOME</code> on first <code>get_builtin_template()</code> call - \u2705 Proper setup: Uses TemplateFlow's <code>@requires_layout</code> decorator to ensure directory structure - \u2705 Unified API: Use TemplateFlow API for both zarrnii and remote templates - \u2705 Automatic: No extra steps needed</p> <p>Without <code>templateflow</code> extra: - \u2705 Direct access: <code>get_builtin_template()</code> works normally from zarrnii package - \u274c No lazy loading: Templates stay in zarrnii package only - \u274c No unified API: TemplateFlow functions unavailable</p> <pre><code># Check installation status\ntry:\n    results = install_zarrnii_templates()\n    print(\"TemplateFlow integration:\", results)\nexcept ImportError:\n    print(\"TemplateFlow extra not installed - using direct access only\")\n</code></pre>"},{"location":"examples/atlas_example/#backward-compatibility","title":"Backward Compatibility","text":"<p>The old atlas functions still work for convenience:</p> <pre><code>from zarrnii import get_builtin_atlas, list_builtin_atlases\n\n# These still work (equivalent to template system)\natlases = list_builtin_atlases() \natlas = get_builtin_atlas(\"placeholder\")  # Gets default atlas from default template\n</code></pre>"},{"location":"examples/atlas_example/#loading-an-atlas-from-files","title":"Loading an Atlas from Files","text":"<pre><code>from zarrnii import ZarrNiiAtlas\nimport numpy as np\n\n# Load atlas from BIDS format files\natlas = ZarrNiiAtlas.from_files(\n    dseg_path=\"path/to/atlas_dseg.nii.gz\",\n    labels_path=\"path/to/atlas_dseg.tsv\"\n)\n\nprint(f\"Atlas loaded: {atlas}\")\nprint(f\"Number of regions: {len(atlas.labels_df)}\")\nprint(f\"Region names: {atlas.labels_df[atlas.name_column].tolist()[:5]}...\")  # First 5 regions\n</code></pre>"},{"location":"examples/atlas_example/#creating-an-atlas-from-existing-data","title":"Creating an Atlas from Existing Data","text":"<pre><code>from zarrnii import ZarrNii, ZarrNiiAtlas, AffineTransform\nimport pandas as pd\nimport numpy as np\n\n# Create a simple segmentation image\nshape = (64, 64, 64)\ndseg_data = np.random.randint(0, 5, shape, dtype=np.int32)\ndseg = ZarrNii.from_darr(dseg_data, affine=AffineTransform.identity())\n\n# Create lookup table\nlabels_df = pd.DataFrame({\n    'index': [0, 1, 2, 3, 4],\n    'name': ['Background', 'Cortex', 'Hippocampus', 'Thalamus', 'Cerebellum'],\n    'abbreviation': ['BG', 'CTX', 'HIP', 'THA', 'CB']\n})\n\n# Create Atlas\natlas = ZarrNiiAtlas.create_from_dseg(dseg, labels_df)\n</code></pre>"},{"location":"examples/atlas_example/#region-analysis","title":"Region Analysis","text":""},{"location":"examples/atlas_example/#getting-region-information","title":"Getting Region Information","text":"<pre><code># Get information about a specific region by index (traditional way)\nregion_info = atlas.get_region_info(2)  # Hippocampus\nprint(f\"Region: {region_info['name']}\")\nprint(f\"Abbreviation: {region_info['abbreviation']}\")\n\n# NEW: Get information by region name\nregion_info = atlas.get_region_info(\"Hippocampus\")\nprint(f\"Hippocampus index: {region_info['index']}\")\n\n# NEW: Get information by abbreviation\nregion_info = atlas.get_region_info(\"HIP\")\nprint(f\"HIP full name: {region_info['name']}\")\n\n# Get all region names and labels\nfor label, name in zip(atlas.region_labels, atlas.region_names):\n    print(f\"Label {label}: {name}\")\n</code></pre>"},{"location":"examples/atlas_example/#creating-region-masks","title":"Creating Region Masks","text":"<pre><code># Get binary mask for hippocampus by index\nhippocampus_mask = atlas.get_region_mask(2)\nprint(f\"Hippocampus mask shape: {hippocampus_mask.shape}\")\n\n# NEW: Get mask by region name\nhippocampus_mask = atlas.get_region_mask(\"Hippocampus\")\nprint(f\"Hippocampus mask shape: {hippocampus_mask.shape}\")\n\n# NEW: Get mask by abbreviation\nhippocampus_mask = atlas.get_region_mask(\"HIP\")\nprint(f\"Hippocampus mask shape: {hippocampus_mask.shape}\")\n\n# Save mask as NIfTI\nhippocampus_mask.to_nifti(\"hippocampus_mask.nii.gz\")\n</code></pre>"},{"location":"examples/atlas_example/#calculating-region-volumes","title":"Calculating Region Volumes","text":"<pre><code># Calculate volume for all regions\nvolumes = {}\nfor _, row in atlas.labels_df.iterrows():\n    label = row[atlas.label_column]\n    if label == 0:  # Skip background\n        continue\n    volume = atlas.get_region_volume(label)\n    name = row[atlas.name_column]\n    volumes[name] = volume\n    print(f\"{name}: {volume:.2f} mm\u00b3\")\n\n# Calculate volume by name or abbreviation\ncortex_volume = atlas.get_region_volume(\"Cortex\")\nhippocampus_volume = atlas.get_region_volume(\"HIP\")  # By abbreviation\nprint(f\"Cortex volume: {cortex_volume:.2f} mm\u00b3\")\nprint(f\"Hippocampus volume: {hippocampus_volume:.2f} mm\u00b3\")\n</code></pre>"},{"location":"examples/atlas_example/#region-based-cropping","title":"Region-Based Cropping","text":""},{"location":"examples/atlas_example/#getting-bounding-boxes-for-regions","title":"Getting Bounding Boxes for Regions","text":"<p>The <code>get_region_bounding_box</code> method allows you to extract the spatial extents of one or more atlas regions in physical coordinates, which can then be used to crop images to focus on specific anatomical structures.</p> <pre><code>from zarrnii import ZarrNiiAtlas\n\n# Load an atlas\natlas = ZarrNiiAtlas.from_files(\"atlas_dseg.nii.gz\", \"atlas_dseg.tsv\")\n\n# Get bounding box for a single region by name\nbbox_min, bbox_max = atlas.get_region_bounding_box(\"Hippocampus\")\nprint(f\"Hippocampus extends from {bbox_min} to {bbox_max} mm in physical space\")\n\n# Get bounding box for multiple regions\nbbox_min, bbox_max = atlas.get_region_bounding_box([\"Hippocampus\", \"Amygdala\"])\nprint(f\"Combined bounding box: {bbox_min} to {bbox_max}\")\n\n# Use regex to select regions\nbbox_min, bbox_max = atlas.get_region_bounding_box(regex=\"Hippocampus.*\")\nprint(f\"All hippocampal subfields: {bbox_min} to {bbox_max}\")\n</code></pre>"},{"location":"examples/atlas_example/#cropping-images-to-regions","title":"Cropping Images to Regions","text":"<p>The bounding boxes returned by <code>get_region_bounding_box</code> are in physical coordinates and can be used directly with the <code>crop</code> method:</p> <pre><code>from zarrnii import ZarrNii\n\n# Load a brain image\nimage = ZarrNii.from_nifti(\"brain_image.nii.gz\")\n\n# Get bounding box for hippocampus\nbbox_min, bbox_max = atlas.get_region_bounding_box(\"Hippocampus\")\n\n# Crop the image to the hippocampus region\ncropped_image = image.crop(bbox_min, bbox_max, physical_coords=True)\n\n# Save the cropped region\ncropped_image.to_nifti(\"hippocampus_cropped.nii.gz\")\n</code></pre>"},{"location":"examples/atlas_example/#example-cropping-and-saving-as-tiff-stack","title":"Example: Cropping and Saving as TIFF Stack","text":"<p>Here's a complete example showing how to crop around the hippocampus and save as a z-stack of TIFF images:</p> <pre><code>from zarrnii import ZarrNii, ZarrNiiAtlas\nimport numpy as np\nfrom PIL import Image\nfrom pathlib import Path\n\n# Load atlas and image\natlas = ZarrNiiAtlas.from_files(\"atlas_dseg.nii.gz\", \"atlas_dseg.tsv\")\nbrain_image = ZarrNii.from_nifti(\"brain_mri.nii.gz\")\n\n# Get bounding box for hippocampus (or use regex for bilateral hippocampi)\nbbox_min, bbox_max = atlas.get_region_bounding_box(regex=\"[Hh]ippocampus.*\")\n\n# Crop both the atlas and the image\ncropped_atlas = atlas.crop(bbox_min, bbox_max, physical_coords=True)\ncropped_image = brain_image.crop(bbox_min, bbox_max, physical_coords=True)\n\n# Get the data (compute if dask array)\nimage_data = cropped_image.data\nif hasattr(image_data, \"compute\"):\n    image_data = image_data.compute()\n\n# Create output directory\noutput_dir = Path(\"hippocampus_slices\")\noutput_dir.mkdir(exist_ok=True)\n\n# Save each z-slice as a TIFF\n# Handle different axes orders (CZYX or CXYZ)\nif cropped_image.axes_order == \"ZYX\":\n    # Data is (C, Z, Y, X)\n    channel_idx = 0 if image_data.ndim == 4 else None\n    for z_idx in range(image_data.shape[1] if channel_idx is not None else image_data.shape[0]):\n        if channel_idx is not None:\n            slice_data = image_data[channel_idx, z_idx, :, :]\n        else:\n            slice_data = image_data[z_idx, :, :]\n\n        # Normalize to 0-255 for TIFF\n        slice_normalized = ((slice_data - slice_data.min()) / \n                           (slice_data.max() - slice_data.min()) * 255).astype(np.uint8)\n\n        # Save as TIFF\n        img = Image.fromarray(slice_normalized)\n        img.save(output_dir / f\"hippocampus_z{z_idx:03d}.tif\")\n\nprint(f\"Saved {image_data.shape[1 if channel_idx is not None else 0]} slices to {output_dir}\")\n</code></pre>"},{"location":"examples/atlas_example/#cropping-multiple-regions","title":"Cropping Multiple Regions","text":"<p>You can easily create crops for multiple regions of interest:</p> <pre><code># Define regions of interest\nregions_of_interest = {\n    \"hippocampus\": \"Hippocampus\",\n    \"amygdala\": \"Amygdala\",\n    \"cortical\": \"cortex.*\",  # regex pattern\n}\n\n# Crop and save each region\nfor region_name, region_pattern in regions_of_interest.items():\n    # Get bounding box (use regex if pattern contains special characters)\n    if any(char in region_pattern for char in [\"*\", \".\", \"[\", \"]\"]):\n        bbox_min, bbox_max = atlas.get_region_bounding_box(regex=region_pattern)\n    else:\n        bbox_min, bbox_max = atlas.get_region_bounding_box(region_pattern)\n\n    # Crop image\n    cropped = brain_image.crop(bbox_min, bbox_max, physical_coords=True)\n\n    # Save\n    cropped.to_nifti(f\"{region_name}_cropped.nii.gz\")\n    print(f\"Saved {region_name} crop: shape={cropped.shape}\")\n</code></pre>"},{"location":"examples/atlas_example/#image-analysis-with-atlas","title":"Image Analysis with Atlas","text":""},{"location":"examples/atlas_example/#aggregating-image-values-by-regions","title":"Aggregating Image Values by Regions","text":"<pre><code># Load an image for analysis (e.g., a functional or structural image)\nimage = ZarrNii.from_nifti(\"functional_image.nii.gz\")\n\n# Make sure image and atlas have the same shape and alignment\n# (In practice, you might need to resample/register the image to atlas space)\n\n# Aggregate image values by atlas regions\nresults = atlas.aggregate_image_by_regions(\n    image, \n    aggregation_func=\"mean\"  # Can be \"mean\", \"sum\", \"std\", \"median\", \"min\", \"max\"\n)\n\nprint(\"Mean signal by region:\")\nfor _, row in results.iterrows():\n    print(f\"{row['name']}: {row['mean_value']:.3f}\")\n\n# Save results to CSV\nresults.to_csv(\"roi_analysis_results.csv\", index=False)\n</code></pre>"},{"location":"examples/atlas_example/#different-aggregation-functions","title":"Different Aggregation Functions","text":"<pre><code># Calculate multiple statistics\nstats = {}\nfor func in [\"mean\", \"std\", \"min\", \"max\"]:\n    stats[func] = atlas.aggregate_image_by_regions(image, aggregation_func=func)\n\n# Combine results (note: results have 'label' and 'name' columns by default)\ncombined_results = stats[\"mean\"][[\"label\", \"name\"]].copy()\nfor func in [\"mean\", \"std\", \"min\", \"max\"]:\n    combined_results[f\"{func}_value\"] = stats[func][f\"{func}_value\"]\n\nprint(combined_results.head())\n</code></pre>"},{"location":"examples/atlas_example/#analyzing-specific-regions-only","title":"Analyzing Specific Regions Only","text":"<pre><code># Get results for all regions, then filter\nall_results = atlas.aggregate_image_by_regions(image, aggregation_func=\"mean\")\n\n# Filter to specific regions (e.g., cortical regions with labels 1-3)\ncortical_results = all_results[all_results[\"label\"].isin([1, 2, 3])]\n\nprint(\"Cortical region analysis:\")\nprint(cortical_results)\n</code></pre>"},{"location":"examples/atlas_example/#creating-feature-maps","title":"Creating Feature Maps","text":""},{"location":"examples/atlas_example/#mapping-statistical-values-back-to-image-space","title":"Mapping Statistical Values Back to Image Space","text":"<pre><code># Create a feature DataFrame (e.g., from previous analysis)\nfeature_data = pd.DataFrame({\n    'index': [1, 2, 3, 4],\n    'activation_score': [2.5, 3.1, 1.8, 2.9],\n    'p_value': [0.001, 0.0001, 0.05, 0.002]\n})\n\n# Create feature maps\nactivation_map = atlas.create_feature_map(\n    feature_data, \n    feature_column='activation_score',\n    background_value=0.0\n)\n\np_value_map = atlas.create_feature_map(\n    feature_data, \n    feature_column='p_value',\n    background_value=1.0  # Non-significant p-value for background\n)\n\n# Save feature maps\nactivation_map.to_nifti(\"activation_map.nii.gz\")\np_value_map.to_nifti(\"p_value_map.nii.gz\")\n</code></pre>"},{"location":"examples/atlas_example/#atlas-summary-information","title":"Atlas Summary Information","text":""},{"location":"examples/atlas_example/#getting-atlas-overview","title":"Getting Atlas Overview","text":"<pre><code># Get information about all regions\nprint(\"Atlas Summary:\")\nprint(f\"Total regions: {len(atlas.labels_df)}\")\nprint(f\"\\nRegion list:\")\nfor _, row in atlas.labels_df.iterrows():\n    label = row[atlas.label_column]\n    name = row[atlas.name_column]\n    if label != 0:  # Skip background\n        volume = atlas.get_region_volume(label)\n        print(f\"  {label}: {name} - {volume:.2f} mm\u00b3\")\n\n# Calculate total brain volume (excluding background)\ntotal_volume = sum(\n    atlas.get_region_volume(row[atlas.label_column])\n    for _, row in atlas.labels_df.iterrows()\n    if row[atlas.label_column] != 0\n)\nprint(f\"\\nTotal brain volume: {total_volume:.2f} mm\u00b3\")\n</code></pre>"},{"location":"examples/atlas_example/#lookup-table-conversion-utilities","title":"Lookup Table Conversion Utilities","text":""},{"location":"examples/atlas_example/#convert-csv-to-bids-tsv-format","title":"Convert CSV to BIDS TSV Format","text":"<pre><code>from zarrnii import import_lut_csv_as_tsv\n\n# Convert a CSV lookup table to BIDS-compatible TSV\nimport_lut_csv_as_tsv(\n    csv_path=\"atlas_lookup.csv\",\n    tsv_path=\"atlas_dseg.tsv\",\n    csv_columns=[\"abbreviation\", \"name\", \"index\"]  # Specify column order\n)\n</code></pre>"},{"location":"examples/atlas_example/#convert-itk-snap-label-file-to-tsv","title":"Convert ITK-SNAP Label File to TSV","text":"<pre><code>from zarrnii import import_lut_itksnap_as_tsv\n\n# Convert ITK-SNAP format to BIDS TSV\nimport_lut_itksnap_as_tsv(\n    itksnap_path=\"atlas.txt\",\n    tsv_path=\"atlas_dseg.tsv\"\n)\n</code></pre>"},{"location":"examples/atlas_example/#advanced-usage","title":"Advanced Usage","text":""},{"location":"examples/atlas_example/#working-with-multi-resolution-data","title":"Working with Multi-Resolution Data","text":"<pre><code># Load atlas at different resolution levels (if using OME-Zarr format)\natlas = ZarrNiiAtlas.from_files(\n    dseg_path=\"atlas.ome.zarr\",\n    labels_path=\"atlas_dseg.tsv\",\n    level=2  # Use downsampled level\n)\n\n# This allows processing at different scales for efficiency\n</code></pre>"},{"location":"examples/atlas_example/#custom-column-names","title":"Custom Column Names","text":"<pre><code># Create atlas with custom column naming conventions\n# First load the data\ndseg = ZarrNii.from_nifti(\"custom_atlas.nii.gz\")\nlabels_df = pd.read_csv(\"custom_labels.tsv\", sep=\"\\t\")\n\n# Create atlas with custom column names\natlas = ZarrNiiAtlas.create_from_dseg(\n    dseg, \n    labels_df,\n    label_column=\"region_id\",\n    name_column=\"region_name\",\n    abbrev_column=\"short_name\"\n)\n</code></pre>"},{"location":"examples/atlas_example/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    atlas = ZarrNiiAtlas.from_files(\"nonexistent.nii.gz\", \"nonexistent.tsv\")\nexcept FileNotFoundError as e:\n    print(f\"Atlas files not found: {e}\")\n\ntry:\n    results = atlas.aggregate_image_by_regions(wrong_sized_image)\nexcept ValueError as e:\n    print(f\"Shape mismatch: {e}\")\n</code></pre>"},{"location":"examples/atlas_example/#integration-with-existing-zarrnii-workflows","title":"Integration with Existing ZarrNii Workflows","text":"<pre><code># Complete workflow: load, transform, analyze\nfrom zarrnii import ZarrNii, ZarrNiiAtlas, AffineTransform\n\n# Load atlas and image\natlas = ZarrNiiAtlas.from_files(\"atlas_dseg.nii.gz\", \"atlas_dseg.tsv\")\nimage = ZarrNii.from_ome_zarr(\"microscopy_data.ome.zarr\", level=1)\n\n# Apply spatial transformation to align image to atlas\ntransform = AffineTransform.from_txt(\"subject_to_atlas.txt\")\naligned_image = image.apply_transform(transform, ref_znimg=atlas.dseg)\n\n# Perform ROI analysis\nresults = atlas.aggregate_image_by_regions(aligned_image, aggregation_func=\"mean\")\n\n# Save results\nresults.to_csv(\"roi_quantification.csv\", index=False)\n\n# Create and save feature map\nfeature_map = atlas.create_feature_map(results, \"mean_value\")\nfeature_map.to_ome_zarr(\"quantification_map.ome.zarr\")\n</code></pre> <p>This atlas functionality provides a complete toolkit for working with brain atlases in the ZarrNii ecosystem, enabling sophisticated region-based analysis workflows that can scale from microscopy data to MRI volumes.</p>"},{"location":"examples/downsampling/","title":"Downsampling and Upsampling","text":"<p>This section covers resolution changes in ZarrNii, including downsampling for efficient processing and upsampling for analysis.</p>"},{"location":"examples/downsampling/#overview","title":"Overview","text":"<p>ZarrNii provides methods for changing image resolution through downsampling and upsampling operations. These are essential for creating multi-resolution datasets and efficient processing workflows.</p>"},{"location":"examples/downsampling/#downsampling","title":"Downsampling","text":""},{"location":"examples/downsampling/#basic-downsampling","title":"Basic Downsampling","text":"<pre><code>from zarrnii import ZarrNii\n\n# Load a high-resolution dataset\nznimg = ZarrNii.from_nifti(\"path/to/highres.nii\")\nprint(\"Original shape:\", znimg.darr.shape)\n\n# Downsample by level (2^level reduction)\ndownsampled = znimg.downsample(level=2)\nprint(\"Downsampled shape:\", downsampled.darr.shape)\n</code></pre>"},{"location":"examples/downsampling/#custom-downsampling-factors","title":"Custom Downsampling Factors","text":"<pre><code># Downsample with specific factors for each axis\ndownsampled_custom = znimg.downsample(\n    along_x=2, \n    along_y=2, \n    along_z=1  # 2x in X,Y; no change in Z\n)\n</code></pre>"},{"location":"examples/downsampling/#upsampling","title":"Upsampling","text":""},{"location":"examples/downsampling/#basic-upsampling","title":"Basic Upsampling","text":"<pre><code># Upsample by factors\nupsampled = znimg.upsample(\n    along_x=2, \n    along_y=2, \n    along_z=1  # 2x in X,Y; no change in Z\n)\n\n# Upsample to specific target shape\ntarget_shape = (100, 200, 300)\nupsampled_to_shape = znimg.upsample(to_shape=target_shape)\n</code></pre>"},{"location":"examples/downsampling/#multi-resolution-workflows","title":"Multi-Resolution Workflows","text":""},{"location":"examples/downsampling/#creating-image-pyramids","title":"Creating Image Pyramids","text":"<pre><code># Create multiple resolution levels\npyramid_levels = []\ncurrent = znimg\n\nfor level in range(4):\n    pyramid_levels.append(current)\n    current = current.downsample(level=1)\n    print(f\"Level {level}: {current.darr.shape}\")\n</code></pre>"},{"location":"examples/downsampling/#working-with-ome-zarr-multi-resolution","title":"Working with OME-Zarr Multi-Resolution","text":"<pre><code># Load multi-resolution OME-Zarr at specific level\nznimg_level0 = ZarrNii.from_ome_zarr(\"path/to/multires.zarr\", level=0)\nznimg_level2 = ZarrNii.from_ome_zarr(\"path/to/multires.zarr\", level=2)\n\n# Create new multi-resolution OME-Zarr\nznimg.to_ome_zarr(\n    \"output_multires.zarr\",\n    max_layer=4\n)\n</code></pre>"},{"location":"examples/downsampling/#automatic-near-isotropic-downsampling","title":"Automatic Near-Isotropic Downsampling","text":"<p>For datasets with anisotropic voxels (e.g., lightsheet microscopy with fine Z resolution), you can automatically downsample to create more isotropic voxels:</p> <pre><code># Load with automatic near-isotropic downsampling\nznimg_isotropic = ZarrNii.from_ome_zarr(\n    \"path/to/anisotropic_data.ome.zarr\", \n    downsample_near_isotropic=True\n)\n\n# Example: if original scales are z=0.25\u03bcm, y=1.0\u03bcm, x=1.0\u03bcm\n# Z dimension gets downsampled by 4x to make scales isotropic:\n# Result: z=1.0\u03bcm, y=1.0\u03bcm, x=1.0\u03bcm (isotropic voxels)\nprint(\"Original anisotropic scales:\", znimg_normal.scale)\nprint(\"Near-isotropic scales:\", znimg_isotropic.scale)\n</code></pre> <p>How it works: - Identifies dimensions with finer resolution (smaller scale values) - Downsamples by powers of 2 to match the coarsest resolution - Works for any spatial dimension (X, Y, or Z) - Only applies when significant anisotropy is detected</p>"},{"location":"examples/downsampling/#memory-efficient-processing","title":"Memory-Efficient Processing","text":"<pre><code># Work with large images efficiently using Dask\n# Downsampling is lazy and computed only when needed\ndownsampled = znimg.downsample(level=2)\n\n# Compute result when needed\nresult = downsampled.darr.compute()\n</code></pre>"},{"location":"examples/downsampling/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use appropriate chunk sizes: For Dask arrays, ensure chunks are well-sized for your operations</li> <li>Lazy evaluation: Downsampling operations are lazy and computed only when <code>.compute()</code> is called</li> <li>Memory management: Use <code>.rechunk()</code> if needed to optimize chunk sizes for your workflow</li> <li>Level-based downsampling: Use <code>level</code> parameter for consistent 2^level reductions</li> </ol>"},{"location":"examples/downsampling/#see-also","title":"See Also","text":"<ul> <li>Multiscale Processing for advanced multi-resolution workflows  </li> <li>Working with Zarr and NIfTI for basic format operations</li> <li>API Reference for detailed method documentation</li> </ul>"},{"location":"examples/imaris_usage/","title":"Working with Imaris Files","text":"<p>ZarrNii provides seamless support for reading and writing Imaris (.ims) files, enabling integration with microscopy workflows that use Imaris for image analysis and visualization.</p> <p>!!! note \"Optional Dependency\"     Imaris support requires the optional <code>imaris</code> dependency. Install it with:     <code>bash     pip install zarrnii[imaris]</code></p> <p>!!! info \"HDF5-Based Imaris Support\"     ZarrNii provides robust Imaris (.ims) file support through a carefully crafted HDF5 implementation. The implementation follows the exact structure of correctly-formed Imaris files to ensure maximum compatibility with Imaris software.</p> <pre><code>**Key Features:**\n- Reads all standard Imaris files with multiple channels, timepoints, and resolution levels\n- Creates Imaris-compatible files using the correct HDF5 structure and metadata\n- Handles both single and multi-channel data automatically\n- Preserves spatial metadata and supports histogram generation\n</code></pre>"},{"location":"examples/imaris_usage/#loading-imaris-files","title":"Loading Imaris Files","text":""},{"location":"examples/imaris_usage/#basic-loading","title":"Basic Loading","text":"<pre><code>from zarrnii import ZarrNii\n\n# Load an Imaris file\nznimg = ZarrNii.from_imaris(\"microscopy_data.ims\")\n\nprint(f\"Data shape: {znimg.darr.shape}\")\nprint(f\"Spacing: {znimg.get_zooms()}\")\n</code></pre>"},{"location":"examples/imaris_usage/#selecting-specific-data","title":"Selecting Specific Data","text":"<p>Imaris files can contain multiple resolution levels, timepoints, and channels. You can specify which to load:</p> <pre><code># Load specific resolution level, timepoint, and channel\nznimg = ZarrNii.from_imaris(\n    \"microscopy_data.ims\",\n    level=1,        # Resolution level (0 = full resolution)\n    timepoint=5,    # Time point\n    channel=0       # Channel index\n)\n\n# Specify axes order and orientation\nznimg = ZarrNii.from_imaris(\n    \"microscopy_data.ims\",\n    axes_order=\"ZYX\",     # Spatial axes order\n    orientation=\"RAS\"     # Coordinate system orientation\n)\n</code></pre>"},{"location":"examples/imaris_usage/#saving-to-imaris-format","title":"Saving to Imaris Format","text":""},{"location":"examples/imaris_usage/#basic-saving","title":"Basic Saving","text":"<pre><code>import numpy as np\nimport dask.array as da\n\n# Create or load data\ndata = np.random.rand(64, 128, 96).astype(np.float32)\ndarr = da.from_array(data[np.newaxis, ...], chunks=\"auto\")\nznimg = ZarrNii.from_darr(darr, spacing=[2.0, 1.0, 1.0])\n\n# Save to Imaris format\noutput_path = znimg.to_imaris(\"output_data.ims\")\nprint(f\"Saved to: {output_path}\")\n</code></pre>"},{"location":"examples/imaris_usage/#compression-options","title":"Compression Options","text":"<pre><code># Save with specific compression settings\nznimg.to_imaris(\n    \"compressed_data.ims\",\n    compression=\"gzip\",      # Compression method\n    compression_opts=6       # Compression level (0-9)\n)\n</code></pre>"},{"location":"examples/imaris_usage/#format-conversions","title":"Format Conversions","text":""},{"location":"examples/imaris_usage/#imaris-to-nifti","title":"Imaris to NIfTI","text":"<pre><code># Load from Imaris and save as NIfTI\nznimg = ZarrNii.from_imaris(\"microscopy_data.ims\")\nznimg.to_nifti(\"converted_data.nii.gz\")\n</code></pre>"},{"location":"examples/imaris_usage/#nifti-to-imaris","title":"NIfTI to Imaris","text":"<pre><code># Load from NIfTI and save as Imaris\nznimg = ZarrNii.from_nifti(\"brain_scan.nii.gz\")\nznimg.to_imaris(\"brain_scan.ims\")\n</code></pre>"},{"location":"examples/imaris_usage/#round-trip-processing","title":"Round-trip Processing","text":"<pre><code># Load, process, and save back to Imaris\nznimg = ZarrNii.from_imaris(\"original_data.ims\")\n\n# Apply transformations\ncropped = znimg.crop((10, 10, 10), (100, 100, 100))\ndownsampled = cropped.downsample(level=2)\n\n# Save processed result\ndownsampled.to_imaris(\"processed_data.ims\")\n</code></pre>"},{"location":"examples/imaris_usage/#integration-with-other-formats","title":"Integration with Other Formats","text":""},{"location":"examples/imaris_usage/#multi-format-workflow","title":"Multi-format Workflow","text":"<pre><code># Load from Imaris\nmicroscopy_data = ZarrNii.from_imaris(\"confocal_stack.ims\")\n\n# Convert to OME-Zarr for analysis\nmicroscopy_data.to_ome_zarr(\"analysis_data.ome.zarr\")\n\n# Load analysis results and convert back\nresults = ZarrNii.from_ome_zarr(\"analysis_results.ome.zarr\")\nresults.to_imaris(\"final_results.ims\")\n</code></pre>"},{"location":"examples/imaris_usage/#understanding-imaris-file-structure","title":"Understanding Imaris File Structure","text":"<p>Imaris files use HDF5 format with a specific hierarchy:</p> <pre><code>MyData.ims\n\u251c\u2500\u2500 DataSet/\n\u2502   \u2514\u2500\u2500 ResolutionLevel 0/\n\u2502       \u2514\u2500\u2500 TimePoint 0/\n\u2502           \u2514\u2500\u2500 Channel 0/\n\u2502               \u2514\u2500\u2500 Data          # The actual image data\n\u251c\u2500\u2500 DataSetInfo/\n\u2502   \u251c\u2500\u2500 Image/                   # Image metadata\n\u2502   \u2514\u2500\u2500 Channel 0/               # Channel information\n\u2514\u2500\u2500 DataSetTimes/\n    \u2514\u2500\u2500 Time                     # Temporal information\n</code></pre> <p>ZarrNii handles this structure automatically, extracting spatial metadata and presenting a unified interface consistent with other supported formats.</p>"},{"location":"examples/imaris_usage/#imaris-file-format-support","title":"Imaris File Format Support","text":"<p>ZarrNii provides comprehensive Imaris (.ims) file support through a robust HDF5-based implementation:</p>"},{"location":"examples/imaris_usage/#file-creation-and-compatibility","title":"File Creation and Compatibility","text":"<p>ZarrNii creates Imaris files that are compatible with Imaris software by following the exact structure found in correctly-formed Imaris files:</p> <pre><code># Create Imaris-compatible files\nznimg.to_imaris(\"output.ims\", compression=\"gzip\")\n</code></pre> <p>Key Features: - Correct HDF5 structure: Follows the exact directory hierarchy used by Imaris - Proper metadata: Includes all necessary attributes for Imaris compatibility - Multi-channel support: Automatically handles single and multi-channel data - Histogram generation: Creates proper histograms for each channel - Compression support: Supports various HDF5 compression options</p> <p>File Structure Created: - Top-level attributes matching Imaris format (ImarisVersion, DataSetDirectoryName, etc.) - <code>DataSet/ResolutionLevel 0/TimePoint 0/Channel X/Data</code> hierarchy - <code>DataSetInfo</code> group with channel metadata - Proper histogram data for each channel</p>"},{"location":"examples/imaris_usage/#best-practices","title":"Best Practices","text":""},{"location":"examples/imaris_usage/#memory-management","title":"Memory Management","text":"<p>For large Imaris files, consider using chunked processing:</p> <pre><code># Load with specific chunking strategy\nznimg = ZarrNii.from_imaris(\"large_dataset.ims\", chunks=(1, 64, 64, 64))\n\n# Process in chunks to avoid memory issues\ncropped = znimg.crop((0, 0, 0), (500, 500, 500))  # Crop first\ndownsampled = cropped.downsample(level=2)          # Then downsample\n</code></pre>"},{"location":"examples/imaris_usage/#metadata-preservation","title":"Metadata Preservation","text":"<p>ZarrNii automatically extracts and preserves spatial metadata from Imaris files:</p> <pre><code>znimg = ZarrNii.from_imaris(\"calibrated_data.ims\")\n\n# Access spatial information\nprint(f\"Voxel spacing: {znimg.get_zooms()}\")\nprint(f\"Origin: {znimg.get_origin()}\")\nprint(f\"Orientation: {znimg.orientation}\")\n</code></pre>"},{"location":"examples/imaris_usage/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    znimg = ZarrNii.from_imaris(\"data.ims\", level=5)\nexcept ValueError as e:\n    print(f\"Invalid parameters: {e}\")\nexcept ImportError as e:\n    print(f\"Missing dependency: {e}\")\n    print(\"Install with: pip install zarrnii[imaris]\")\n</code></pre>"},{"location":"examples/imaris_usage/#example-complete-microscopy-processing-pipeline","title":"Example: Complete Microscopy Processing Pipeline","text":"<pre><code>from zarrnii import ZarrNii\n\ndef process_microscopy_data(input_path, output_path):\n    \"\"\"Complete processing pipeline for microscopy data.\"\"\"\n\n    # Load original Imaris data\n    print(\"Loading Imaris data...\")\n    znimg = ZarrNii.from_imaris(input_path)\n\n    # Apply processing steps\n    print(\"Processing data...\")\n\n    # 1. Crop to region of interest\n    cropped = znimg.crop((50, 50, 50), (400, 400, 300))\n\n    # 2. Downsample for faster processing\n    downsampled = cropped.downsample(level=1)\n\n    # 3. Save intermediate result as OME-Zarr for analysis\n    downsampled.to_ome_zarr(\"intermediate_analysis.ome.zarr\")\n\n    # 4. Save final result as Imaris\n    print(\"Saving processed data...\")\n    downsampled.to_imaris(output_path)\n\n    print(f\"Processing complete. Output saved to: {output_path}\")\n    return downsampled\n\n# Run the pipeline\nresult = process_microscopy_data(\"raw_confocal.ims\", \"processed_confocal.ims\")\n</code></pre> <p>This example demonstrates ZarrNii's ability to seamlessly integrate Imaris files into broader image processing workflows while maintaining spatial accuracy and metadata consistency.</p>"},{"location":"examples/multiscale/","title":"Multiscale OME-Zarr","text":"<p>This section covers working with multi-resolution OME-Zarr datasets, including creation, manipulation, and optimization strategies.</p>"},{"location":"examples/multiscale/#overview","title":"Overview","text":"<p>OME-Zarr supports multi-resolution image pyramids that enable efficient visualization and analysis at different scales. ZarrNii provides support for creating and reading multiscale datasets.</p>"},{"location":"examples/multiscale/#understanding-ome-zarr-multiscale","title":"Understanding OME-Zarr Multiscale","text":""},{"location":"examples/multiscale/#loading-multiscale-data","title":"Loading Multiscale Data","text":"<pre><code>from zarrnii import ZarrNii\nimport zarr\n\n# Load different resolution levels of a multiscale OME-Zarr dataset\nznimg_level0 = ZarrNii.from_ome_zarr(\"path/to/multiscale.zarr\", level=0)  # Full resolution\nznimg_level1 = ZarrNii.from_ome_zarr(\"path/to/multiscale.zarr\", level=1)  # Half resolution\nznimg_level2 = ZarrNii.from_ome_zarr(\"path/to/multiscale.zarr\", level=2)  # Quarter resolution\n\nprint(\"Level 0 shape:\", znimg_level0.darr.shape)\nprint(\"Level 1 shape:\", znimg_level1.darr.shape)\nprint(\"Level 2 shape:\", znimg_level2.darr.shape)\n</code></pre>"},{"location":"examples/multiscale/#inspecting-available-levels","title":"Inspecting Available Levels","text":"<pre><code># Use zarr directly to inspect the structure\nstore = zarr.open_group(\"path/to/multiscale.zarr\", mode='r')\nprint(\"Available arrays:\", list(store.keys()))\n\n# Check shapes at each level\nfor key in sorted(store.keys()):\n    if key.isdigit():\n        array = store[key]\n        print(f\"Level {key}: shape {array.shape}, chunks {array.chunks}\")\n</code></pre>"},{"location":"examples/multiscale/#creating-multiscale-ome-zarr","title":"Creating Multiscale OME-Zarr","text":""},{"location":"examples/multiscale/#basic-multiscale-creation","title":"Basic Multiscale Creation","text":"<pre><code># Load a single-resolution image\nznimg = ZarrNii.from_nifti(\"path/to/highres.nii\")\n\n# Create multiscale OME-Zarr with default parameters\nznimg.to_ome_zarr(\n    \"output_multiscale.zarr\",\n    max_layer=4  # Creates 4 downsampling levels\n)\n</code></pre>"},{"location":"examples/multiscale/#custom-multiscale-parameters","title":"Custom Multiscale Parameters","text":"<pre><code># Create multiscale with custom settings\nznimg.to_ome_zarr(\n    \"custom_multiscale.zarr\",\n    max_layer=6,  # More downsampling levels\n    scaling_method=None  # Use default downsampling\n)\n</code></pre>"},{"location":"examples/multiscale/#working-with-different-resolution-levels","title":"Working with Different Resolution Levels","text":""},{"location":"examples/multiscale/#processing-at-different-scales","title":"Processing at Different Scales","text":"<pre><code># Load and process at different resolutions for different tasks\n\n# Use low resolution for quick overview\nthumbnail = ZarrNii.from_ome_zarr(\"data.zarr\", level=3)\noverview_stats = compute_statistics(thumbnail)\n\n# Use medium resolution for analysis  \nanalysis_res = ZarrNii.from_ome_zarr(\"data.zarr\", level=1)\nfeature_map = extract_features(analysis_res)\n\n# Use full resolution for final processing\nfull_res = ZarrNii.from_ome_zarr(\"data.zarr\", level=0)\nfinal_result = apply_detailed_processing(full_res)\n</code></pre>"},{"location":"examples/multiscale/#multi-resolution-workflow","title":"Multi-Resolution Workflow","text":"<pre><code># Progressive processing workflow\ndef progressive_analysis(zarr_path):\n    # Start with thumbnail for parameter estimation\n    low_res = ZarrNii.from_ome_zarr(zarr_path, level=3)\n    parameters = estimate_parameters(low_res)\n\n    # Refine on medium resolution\n    med_res = ZarrNii.from_ome_zarr(zarr_path, level=1) \n    refined_params = refine_parameters(med_res, parameters)\n\n    # Apply to full resolution\n    full_res = ZarrNii.from_ome_zarr(zarr_path, level=0)\n    final_result = process_with_params(full_res, refined_params)\n\n    return final_result\n\nresult = progressive_analysis(\"multiscale_data.zarr\")\n</code></pre>"},{"location":"examples/multiscale/#channel-and-time-series-support","title":"Channel and Time Series Support","text":""},{"location":"examples/multiscale/#multi-channel-multiscale","title":"Multi-Channel Multiscale","text":"<pre><code># Load specific channels from multiscale data\n# Channel selection works with any resolution level\ndapi_full = ZarrNii.from_ome_zarr(\"multi_channel.zarr\", level=0, channels=[0])\ngfp_thumbnail = ZarrNii.from_ome_zarr(\"multi_channel.zarr\", level=3, channels=[1])\n\nprint(\"DAPI full resolution:\", dapi_full.darr.shape)\nprint(\"GFP thumbnail:\", gfp_thumbnail.darr.shape)\n</code></pre>"},{"location":"examples/multiscale/#channel-labels","title":"Channel labels","text":"<pre><code># If channel labels are present from OME metadata (e.g. for data from SPIMprep)\n# You can select channels based on label\nabeta_full = ZarrNii.from_ome_zarr(\"multi_channel.zarr\", level=3, channel_labels=[\"Abeta\"])\n</code></pre>"},{"location":"examples/multiscale/#memory-management","title":"Memory Management","text":""},{"location":"examples/multiscale/#efficient-loading","title":"Efficient Loading","text":"<pre><code># Load only what you need\n# Zarr and Dask handle lazy loading automatically\n\n# Load with appropriate chunking\nznimg = ZarrNii.from_ome_zarr(\"large_dataset.zarr\", level=1)\n\n# Process in blocks to manage memory\ndef process_large_dataset(znimg):\n    # Process data block by block\n    result = znimg.darr.map_blocks(\n        process_block,\n        dtype=np.float32,\n        drop_axis=None\n    )\n    return result\n\nprocessed = process_large_dataset(znimg)\n</code></pre>"},{"location":"examples/multiscale/#best-practices","title":"Best Practices","text":""},{"location":"examples/multiscale/#choosing-resolution-levels","title":"Choosing Resolution Levels","text":"<pre><code># Guidelines for choosing appropriate resolution levels\n\ndef choose_resolution_level(task_type, data_size):\n    \"\"\"Choose optimal resolution level based on task and data size\"\"\"\n    if task_type == \"thumbnail\":\n        return 4  # Very low resolution for quick preview\n    elif task_type == \"segmentation\":\n        return 1  # Medium resolution for segmentation\n    elif task_type == \"measurement\":\n        return 0  # Full resolution for accurate measurements\n    else:\n        # Default to medium resolution\n        return 2\n\n# Use the function\nlevel = choose_resolution_level(\"segmentation\", data.shape)\nznimg = ZarrNii.from_ome_zarr(\"data.zarr\", level=level)\n</code></pre>"},{"location":"examples/multiscale/#performance-tips","title":"Performance Tips","text":"<ol> <li>Choose appropriate levels: Use lower resolution levels for exploratory analysis</li> <li>Minimize data loading: Only load the resolution level you actually need</li> <li>Leverage lazy evaluation: Let Dask handle memory management automatically</li> </ol>"},{"location":"examples/multiscale/#see-also","title":"See Also","text":"<ul> <li>Downsampling and Upsampling for resolution change operations</li> <li>Working with Zarr and NIfTI for basic format operations  </li> <li>API Reference for detailed method documentation</li> </ul>"},{"location":"examples/near_isotropic/","title":"Near-Isotropic Downsampling","text":"<p>This example demonstrates the automatic near-isotropic downsampling feature, which is particularly useful for lightsheet microscopy data where Z resolution is often much finer than X/Y resolution.</p>"},{"location":"examples/near_isotropic/#problem-anisotropic-voxels","title":"Problem: Anisotropic Voxels","text":"<p>Many biomedical imaging modalities produce datasets with anisotropic voxels where one dimension has much finer resolution than others:</p> <pre><code>from zarrnii import ZarrNii\n\n# Load anisotropic lightsheet data\nznimg = ZarrNii.from_ome_zarr(\"lightsheet_data.ome.zarr\")\n\nprint(\"Original scales:\", znimg.scale)\n# Output might show: {'z': 0.25, 'y': 1.0, 'x': 1.0}\n# Z has 4x finer resolution than X/Y\n</code></pre>"},{"location":"examples/near_isotropic/#solution-automatic-downsampling","title":"Solution: Automatic Downsampling","text":"<p>The <code>downsample_near_isotropic</code> parameter automatically identifies and corrects anisotropic voxels:</p> <pre><code># Load with automatic near-isotropic downsampling\nznimg_isotropic = ZarrNii.from_ome_zarr(\n    \"lightsheet_data.ome.zarr\", \n    downsample_near_isotropic=True\n)\n\nprint(\"Isotropic scales:\", znimg_isotropic.scale)\n# Output: {'z': 1.0, 'y': 1.0, 'x': 1.0}\n# All dimensions now have the same resolution\n\nprint(\"Shape comparison:\")\nprint(f\"Original:  {znimg.darr.shape}\")\nprint(f\"Isotropic: {znimg_isotropic.darr.shape}\")\n# Z dimension is reduced by the downsampling factor\n</code></pre>"},{"location":"examples/near_isotropic/#how-it-works","title":"How It Works","text":"<p>The algorithm:</p> <ol> <li>Identifies the coarsest resolution (largest scale value) among spatial dimensions</li> <li>Calculates downsampling factors as powers of 2 for finer resolution dimensions  </li> <li>Applies selective downsampling using the existing downsample method</li> </ol> <pre><code># For scales z=0.25, y=1.0, x=1.0:\n# - Max scale = 1.0 (coarsest resolution)\n# - Z ratio = 1.0 / 0.25 = 4.0\n# - Z downsampling factor = 2^2 = 4\n# - Result: z=1.0, y=1.0, x=1.0 (isotropic)\n</code></pre>"},{"location":"examples/near_isotropic/#benefits","title":"Benefits","text":"<ul> <li>Improved processing efficiency: Isotropic voxels work better with many algorithms</li> <li>Consistent visualization: Equal sampling in all dimensions for 3D rendering</li> <li>Memory reduction: Removes unnecessary oversampling in fine dimensions</li> <li>Algorithm compatibility: Many image processing algorithms assume isotropic voxels</li> </ul>"},{"location":"examples/near_isotropic/#when-to-use","title":"When to Use","text":"<p>This feature is most beneficial for: - Lightsheet microscopy data with fine Z-sampling - High-resolution imaging with anisotropic acquisition - Preprocessing before analysis algorithms that assume isotropy - Visualization where consistent sampling is desired</p>"},{"location":"examples/near_isotropic/#manual-control","title":"Manual Control","text":"<p>For fine-grained control, you can still use manual downsampling:</p> <pre><code># Manual approach - downsample Z dimension by specific factor\nznimg_manual = znimg.downsample(along_z=4, along_y=1, along_x=1)\n\n# This gives the same result as downsample_near_isotropic=True\n# but allows custom control over factors\n</code></pre>"},{"location":"examples/near_isotropic/#performance-impact","title":"Performance Impact","text":"<p>The automatic downsampling: - Uses the same efficient downsampling algorithm as manual methods - Is applied lazily with Dask for memory efficiency - Reduces data size and subsequent processing time - Preserves all metadata and coordinate system information</p>"},{"location":"examples/near_isotropic/#see-also","title":"See Also","text":"<ul> <li>Downsampling and Upsampling for general resolution operations</li> <li>Basic Tasks for getting started with transformations</li> <li>API Reference for detailed parameter documentation</li> </ul>"},{"location":"examples/scaled_processing/","title":"Multi-Resolution Plugin Architecture","text":"<p>This section covers the multi-resolution plugin architecture that enables efficient processing where algorithms are computed at low resolution and applied to full resolution data.</p>"},{"location":"examples/scaled_processing/#overview","title":"Overview","text":"<p>The scaled processing plugin architecture in ZarrNii allows for efficient multi-resolution operations. This is particularly useful for algorithms that can be computed efficiently at lower resolution and then applied to the full-resolution data. Common use cases include:</p> <ul> <li>Bias field correction</li> <li>Background estimation</li> <li>Denoising operations</li> <li>Global intensity normalization</li> </ul>"},{"location":"examples/scaled_processing/#plugin-interface","title":"Plugin Interface","text":"<p>All scaled processing plugins must inherit from <code>ScaledProcessingPlugin</code> and implement two key methods:</p>"},{"location":"examples/scaled_processing/#lowres_funclowres_array-npndarray-npndarray","title":"<code>lowres_func(lowres_array: np.ndarray) -&gt; np.ndarray</code>","text":"<p>This function processes the downsampled data and returns a result that will be upsampled and applied to the full-resolution data.</p>"},{"location":"examples/scaled_processing/#highres_funcfullres_array-daskarray-upsampled_output-daskarray-daskarray","title":"<code>highres_func(fullres_array: dask.array, upsampled_output: dask.array) -&gt; dask.array</code>","text":"<p>This function receives the full-resolution dask array and the upsampled output (already upsampled to match the full-resolution shape), and applies the operation blockwise. The upsampling is handled internally by the <code>apply_scaled_processing</code> method.</p>"},{"location":"examples/scaled_processing/#basic-usage","title":"Basic Usage","text":"<pre><code>from zarrnii import ZarrNii, GaussianBiasFieldCorrection\n\n# Load your data\nznimg = ZarrNii.from_nifti(\"path/to/image.nii\")\n\n# Apply bias field correction\ncorrected = znimg.apply_scaled_processing(\n    GaussianBiasFieldCorrection(sigma=5.0),\n    downsample_factor=4\n)\n\n# Save result\ncorrected.to_nifti(\"corrected_image.nii\")\n</code></pre>"},{"location":"examples/scaled_processing/#built-in-plugins","title":"Built-in Plugins","text":""},{"location":"examples/scaled_processing/#gaussianbiasfieldcorrection","title":"GaussianBiasFieldCorrection","text":"<p>A simple bias field correction plugin that estimates smooth bias fields using Gaussian smoothing at low resolution and applies correction by division.</p> <pre><code># Basic usage with default parameters\ncorrected = znimg.apply_scaled_processing(GaussianBiasFieldCorrection())\n\n# Custom parameters\ncorrected = znimg.apply_scaled_processing(\n    GaussianBiasFieldCorrection(sigma=3.0, mode='constant'),\n    downsample_factor=8\n)\n</code></pre> <p>Parameters: - <code>sigma</code>: Standard deviation for Gaussian smoothing (default: 5.0) - <code>mode</code>: Boundary condition for smoothing (default: 'reflect')</p>"},{"location":"examples/scaled_processing/#n4biasfieldcorrection","title":"N4BiasFieldCorrection","text":"<p>A more sophisticated bias field correction plugin that uses the N4 algorithm from ANTsPy for superior bias field estimation at low resolution, then applies correction by division.</p> <p>Installation: Requires <code>antspyx</code> to be installed:</p> <pre><code># Install with N4 support\npip install 'zarrnii[n4]'\n# or install antspyx directly\npip install antspyx\n</code></pre> <pre><code>from zarrnii import N4BiasFieldCorrection\n\n# Basic usage with default parameters\ncorrected = znimg.apply_scaled_processing(N4BiasFieldCorrection())\n\n# Custom parameters for more control\ncorrected = znimg.apply_scaled_processing(\n    N4BiasFieldCorrection(\n        spline_spacing=150.0,\n        convergence={'iters': [25, 25], 'tol': 0.001},\n        shrink_factor=2\n    ),\n    downsample_factor=4\n)\n</code></pre> <p>Parameters: - <code>spline_spacing</code>: Spacing between knots for spline fitting (default: 200.0) - <code>convergence</code>: Dictionary with 'iters' (list) and 'tol' (float) for convergence criteria (default: {'iters': [50], 'tol': 0.001}) - <code>shrink_factor</code>: Shrink factor for processing (default: 1)</p>"},{"location":"examples/scaled_processing/#creating-custom-plugins","title":"Creating Custom Plugins","text":"<p>You can create custom plugins by inheriting from <code>ScaledProcessingPlugin</code>:</p> <pre><code>from zarrnii import ScaledProcessingPlugin\nimport numpy as np\nimport dask.array as da\nfrom scipy import ndimage\n\nclass CustomPlugin(ScaledProcessingPlugin):\n    def __init__(self, param1=1.0, **kwargs):\n        super().__init__(param1=param1, **kwargs)\n        self.param1 = param1\n\n    def lowres_func(self, lowres_array: np.ndarray) -&gt; np.ndarray:\n        # Your low-resolution algorithm here\n        # Example: compute some correction map\n        correction_map = np.ones_like(lowres_array) * self.param1\n        return correction_map\n\n    def highres_func(self, fullres_array: da.Array, upsampled_output: da.Array) -&gt; da.Array:\n        # The upsampling is handled internally by apply_scaled_processing\n        # This example shows a simple multiplication operation\n\n        # Apply correction directly (both arrays are same size)\n        result = fullres_array * upsampled_output\n\n        return result\n\n    @property\n    def name(self) -&gt; str:\n        return \"Custom Plugin\"\n\n    @property\n    def description(self) -&gt; str:\n        return \"A custom multi-resolution processing plugin\"\n\n# Use your custom plugin\nresult = znimg.apply_scaled_processing(CustomPlugin(param1=2.0))\n</code></pre>"},{"location":"examples/scaled_processing/#advanced-usage","title":"Advanced Usage","text":""},{"location":"examples/scaled_processing/#custom-downsampling-factors","title":"Custom Downsampling Factors","text":"<pre><code># Use different downsampling factors\nresult = znimg.apply_scaled_processing(\n    GaussianBiasFieldCorrection(),\n    downsample_factor=8  # 8x downsampling\n)\n</code></pre>"},{"location":"examples/scaled_processing/#custom-chunk-sizes","title":"Custom Chunk Sizes","text":"<pre><code># Specify custom chunk sizes for low-resolution processing\n# The chunk_size parameter controls the chunking of low-resolution intermediate results\nresult = znimg.apply_scaled_processing(\n    GaussianBiasFieldCorrection(),\n    chunk_size=(1, 32, 32, 32)  # Used for low-res processing chunks\n)\n</code></pre>"},{"location":"examples/scaled_processing/#temporary-file-options","title":"Temporary File Options","text":"<p>The framework uses temporary OME-Zarr files to break up the dask computation graph for better performance. You can control this behavior:</p> <pre><code># Disable temporary file usage (may impact performance on large datasets)\nresult = znimg.apply_scaled_processing(\n    GaussianBiasFieldCorrection(),\n    use_temp_zarr=False\n)\n\n# Use custom temporary file location\nresult = znimg.apply_scaled_processing(\n    GaussianBiasFieldCorrection(),\n    temp_zarr_path=\"/custom/path/temp_processing.ome.zarr\"\n)\n</code></pre>"},{"location":"examples/scaled_processing/#plugin-class-vs-instance","title":"Plugin Class vs Instance","text":"<pre><code># Using plugin class (parameters passed as kwargs)\nresult1 = znimg.apply_scaled_processing(GaussianBiasFieldCorrection, sigma=3.0)\n\n# Using plugin instance (parameters set during initialization)\nplugin = GaussianBiasFieldCorrection(sigma=3.0)\nresult2 = znimg.apply_scaled_processing(plugin)\n</code></pre>"},{"location":"examples/scaled_processing/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Downsampling Factor: Higher factors reduce computation time but may reduce accuracy</li> <li>Chunk Sizes: Optimize for your memory constraints and processing requirements</li> <li>Algorithm Complexity: The <code>lowres_func</code> runs on small numpy arrays, while <code>highres_func</code> uses dask for scalability</li> <li>Temporary Files: The default temporary OME-Zarr approach breaks up dask computation graphs for better performance on large datasets. Disable only if you have specific memory/disk constraints</li> <li>Dask-based Upsampling: Uses ZarrNii's <code>.upsample()</code> method which leverages dask for efficient parallel upsampling</li> </ol>"},{"location":"examples/scaled_processing/#integration-with-other-operations","title":"Integration with Other Operations","text":"<p>The scaled processing plugins integrate seamlessly with other ZarrNii operations:</p> <pre><code># Chain operations\nresult = (znimg\n    .apply_scaled_processing(GaussianBiasFieldCorrection())\n    .downsample(level=1)\n    .segment_otsu())\n\n# Save to different formats\nresult.to_nifti(\"processed.nii\")\nresult.to_ome_zarr(\"processed.ome.zarr\")\n</code></pre>"},{"location":"examples/scaled_processing/#see-also","title":"See Also","text":"<ul> <li>Segmentation Plugins for other plugin architectures</li> <li>Downsampling and Upsampling for resolution operations</li> <li>API Reference for detailed method documentation</li> </ul>"},{"location":"examples/segmentation_example/","title":"Segmentation Plugin Example","text":"<p>This example demonstrates how to use the segmentation plugin system in ZarrNii.</p>"},{"location":"examples/segmentation_example/#basic-otsu-segmentation","title":"Basic Otsu Segmentation","text":"<pre><code>import numpy as np\nimport dask.array as da\nfrom zarrnii import ZarrNii, OtsuSegmentation\n\n# Load or create your image data\n# For this example, we'll create synthetic bimodal data\nnp.random.seed(42)\nimage_data = np.random.normal(0.2, 0.05, (1, 50, 100, 100))  # Background\nimage_data[0, 20:30, 40:60, 40:60] = np.random.normal(0.8, 0.05, (10, 20, 20))  # Foreground\n\n# Create ZarrNii instance\ndarr = da.from_array(image_data, chunks=(1, 25, 50, 50))\nznimg = ZarrNii.from_darr(darr, axes_order=\"ZYX\", orientation=\"RAS\")\n\n# Method 1: Using the convenience method\nsegmented = znimg.segment_otsu(nbins=256)\n\n# Method 2: Using the plugin directly\nplugin = OtsuSegmentation(nbins=256)\nsegmented = znimg.segment(plugin)\n\n# Method 3: Using plugin class with parameters\nsegmented = znimg.segment(OtsuSegmentation, nbins=128)\n\n# The result is a new ZarrNii instance with binary segmentation\nprint(f\"Original shape: {znimg.shape}\")\nprint(f\"Segmented shape: {segmented.shape}\")\nprint(f\"Segmented dtype: {segmented.data.dtype}\")\nprint(f\"Unique values: {np.unique(segmented.data.compute())}\")\n\n# Save segmented result as OME-Zarr\nsegmented.to_ome_zarr(\"segmented_image.ome.zarr\")\n</code></pre>"},{"location":"examples/segmentation_example/#custom-chunk-processing","title":"Custom Chunk Processing","text":"<p>For large datasets, you can control the chunk size for blockwise processing:</p> <pre><code># Segment with custom chunk size for memory efficiency\ncustom_chunks = (1, 10, 25, 25)\nsegmented = znimg.segment_otsu(chunk_size=custom_chunks)\n\n# The segmentation will be applied block-wise using dask\nresult_data = segmented.data.compute()\n</code></pre>"},{"location":"examples/segmentation_example/#creating-custom-segmentation-plugins","title":"Creating Custom Segmentation Plugins","text":"<p>You can create your own segmentation plugins by inheriting from <code>SegmentationPlugin</code>:</p> <pre><code>from zarrnii.plugins.segmentation import SegmentationPlugin\nimport numpy as np\n\nclass ThresholdSegmentation(SegmentationPlugin):\n    \"\"\"Simple threshold-based segmentation plugin.\"\"\"\n\n    def __init__(self, threshold: float = 0.5, **kwargs):\n        super().__init__(threshold=threshold, **kwargs)\n        self.threshold = threshold\n\n    def segment(self, image: np.ndarray, metadata=None) -&gt; np.ndarray:\n        \"\"\"Apply threshold segmentation.\"\"\"\n        binary_mask = image &gt; self.threshold\n        return binary_mask.astype(np.uint8)\n\n    @property\n    def name(self) -&gt; str:\n        return \"Threshold Segmentation\"\n\n    @property\n    def description(self) -&gt; str:\n        return f\"Simple thresholding at value {self.threshold}\"\n\n# Use your custom plugin\ncustom_plugin = ThresholdSegmentation(threshold=0.3)\nsegmented = znimg.segment(custom_plugin)\n</code></pre>"},{"location":"examples/segmentation_example/#working-with-multi-channel-images","title":"Working with Multi-channel Images","text":"<p>The segmentation plugins automatically handle multi-channel images:</p> <pre><code># Create multi-channel test data\nmultichannel_data = np.random.rand(3, 50, 100, 100)  # 3 channels\nmultichannel_data[0, 20:30, 40:60, 40:60] += 0.5  # Add signal to first channel\n\ndarr = da.from_array(multichannel_data, chunks=(1, 25, 50, 50))\nznimg = ZarrNii.from_darr(darr, axes_order=\"ZYX\", orientation=\"RAS\")\n\n# Segment - will use first channel for threshold calculation\n# but preserve all channel dimensions in output\nsegmented = znimg.segment_otsu()\nprint(f\"Input shape: {znimg.shape}\")       # (3, 50, 100, 100)\nprint(f\"Output shape: {segmented.shape}\")   # (3, 50, 100, 100)\n</code></pre>"},{"location":"examples/segmentation_example/#integration-with-existing-workflows","title":"Integration with Existing Workflows","text":"<p>The segmentation plugins integrate seamlessly with other ZarrNii operations:</p> <pre><code># Complete workflow: load, downsample, segment, save\nznimg = ZarrNii.from_ome_zarr(\"input_image.ome.zarr\", level=1)\n\n# Downsample for faster processing\ndownsampled = znimg.downsample(factors=2, spatial_dims=[\"z\", \"y\", \"x\"])\n\n# Apply segmentation\nsegmented = downsampled.segment_otsu(nbins=128)\n\n# Crop to region of interest\nbbox_min = (10, 20, 20)\nbbox_max = (40, 80, 80)\ncropped = segmented.crop_with_bounding_box(bbox_min, bbox_max)\n\n# Save final result\ncropped.to_ome_zarr(\"processed_segmentation.ome.zarr\")\n</code></pre>"},{"location":"examples/transformations/","title":"Transformations","text":"<p>This section covers spatial transformations in ZarrNii, including affine transforms and displacement fields.</p>"},{"location":"examples/transformations/#overview","title":"Overview","text":"<p>ZarrNii provides support for spatial transformations through the <code>AffineTransform</code> and <code>DisplacementTransform</code> classes. These can be used to apply transformations derived from performing ANTS registration on downsampled (e.g. level &gt; 3)  images, then applied to higher resolution (e.g. level &lt; 3) images. ZarrNii performs these computations in a block-wise manner using Dask, upsampling the displacement field for a block before applying it. You can also provide a sequence of transforms to perform composition of transformations to resample in a single step.</p>"},{"location":"examples/transformations/#affine-transformations","title":"Affine Transformations","text":""},{"location":"examples/transformations/#creating-affine-transformations","title":"Creating Affine Transformations","text":"<pre><code>from zarrnii import ZarrNii\nfrom zarrnii.transform import AffineTransform\nimport numpy as np\n\n# Load a dataset\nznimg = ZarrNii.from_nifti(\"path/to/image.nii\")\n\n# Create identity transformation\nidentity_transform = AffineTransform.identity()\n\n# Create transformation from a matrix\nmatrix = np.array([\n    [2.0, 0.0, 0.0, 10.0],  # Scale X by 2, translate by 10\n    [0.0, 2.0, 0.0, -5.0],  # Scale Y by 2, translate by -5\n    [0.0, 0.0, 1.0, 0.0],   # No change in Z\n    [0.0, 0.0, 0.0, 1.0]    # Homogeneous coordinates\n])\ntransform = AffineTransform.from_array(matrix)\n\n# Load transformation from text file\ntransform_from_file = AffineTransform.from_txt(\"transform.txt\")\n</code></pre>"},{"location":"examples/transformations/#applying-transformations","title":"Applying Transformations","text":"<pre><code># Apply transformation (requires reference image)\nref_znimg = ZarrNii.from_nifti(\"path/to/reference.nii\")\ntransformed = znimg.apply_transform(transform, ref_znimg=ref_znimg)\n</code></pre>"},{"location":"examples/transformations/#working-with-displacement-transformations","title":"Working with Displacement Transformations","text":"<pre><code>from zarrnii.transform import DisplacementTransform\n\n# Load displacement field from NIfTI file\ndisp_transform = DisplacementTransform.from_nifti(\"displacement_field.nii\")\n\n# Apply displacement transformation\ndeformed = znimg.apply_transform(disp_transform, ref_znimg=ref_znimg)\n</code></pre>"},{"location":"examples/transformations/#multiple-transformations","title":"Multiple Transformations","text":"<pre><code># Apply multiple transformations in sequence\n# Each transformation is applied sequentially\nresult = znimg.apply_transform(transform1, transform2, ref_znimg=ref_znimg)\n</code></pre>"},{"location":"examples/transformations/#coordinate-transformations","title":"Coordinate Transformations","text":"<pre><code># Transform coordinates using the matrix multiplication operator\nvoxel_coords = np.array([50, 60, 30])\nras_coords = transform @ voxel_coords\n\n# Transform multiple points\npoints = np.array([[50, 60, 30], [100, 120, 60]]).T  # 3xN array\ntransformed_points = transform @ points\n</code></pre>"},{"location":"examples/transformations/#inverting-transformations","title":"Inverting Transformations","text":"<pre><code># Get the inverse of a transformation\ninverse_transform = transform.invert()\n\n# Apply inverse transformation\nrestored = transformed_znimg.apply_transform(inverse_transform, ref_znimg=znimg)\n</code></pre>"},{"location":"examples/transformations/#coordinate-system-handling","title":"Coordinate System Handling","text":"<pre><code># Update transformation for different orientations\nupdated_transform = transform.update_for_orientation(\"RPI\", \"RAS\")\n</code></pre>"},{"location":"examples/transformations/#best-practices","title":"Best Practices","text":"<ol> <li>Use ref_znimg parameter: Always provide a reference image when applying transformations</li> <li>Consider coordinate systems: Be aware of voxel vs RAS coordinate conventions  </li> <li>Memory efficiency: Use lazy evaluation with Dask arrays for large datasets</li> <li>Transformation order: Remember that multiple transforms are applied sequentially</li> </ol>"},{"location":"examples/transformations/#see-also","title":"See Also","text":"<ul> <li>API Reference for detailed method documentation</li> <li>Downsampling and Upsampling for resolution change operations</li> <li>Working with Zarr and NIfTI for basic format operations</li> </ul>"},{"location":"examples/zarr_nifti/","title":"Examples: Working with Zarr and NIfTI","text":"<p>This section provides practical workflows for using ZarrNii with OME-Zarr and NIfTI datasets.</p>"},{"location":"examples/zarr_nifti/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Loading Datasets<ul> <li>From OME-Zarr</li> <li>From NIfTI</li> </ul> </li> <li>Performing Transformations<ul> <li>Downsampling</li> <li>Cropping</li> <li>Combining Affine Transformations</li> </ul> </li> <li>Saving Results<ul> <li>To OME-Zarr</li> <li>To NIfTI</li> </ul> </li> <li>Advanced Example: Full Workflow</li> </ol>"},{"location":"examples/zarr_nifti/#loading-datasets","title":"Loading Datasets","text":""},{"location":"examples/zarr_nifti/#from-ome-zarr","title":"From OME-Zarr","text":"<p>Load a dataset from an OME-Zarr file and inspect its metadata:</p> <pre><code>from zarrnii import ZarrNii\n\n# Load OME-Zarr dataset\nznimg = ZarrNii.from_ome_zarr(\"path/to/dataset.zarr\")\n\n# Inspect data\nprint(\"Shape:\", znimg.darr.shape)\nprint(\"Affine matrix:\\n\", znimg.affine.matrix)\n</code></pre>"},{"location":"examples/zarr_nifti/#from-nifti","title":"From NIfTI","text":"<p>Load a NIfTI dataset and inspect its attributes:</p> <pre><code># Load NIfTI dataset\nznimg = ZarrNii.from_nifti(\"path/to/dataset.nii\")\n\n# Inspect data\nprint(\"Shape:\", znimg.darr.shape)\nprint(\"Affine matrix:\\n\", znimg.affine.matrix)\n</code></pre>"},{"location":"examples/zarr_nifti/#performing-transformations","title":"Performing Transformations","text":""},{"location":"examples/zarr_nifti/#downsampling","title":"Downsampling","text":"<p>Reduce the resolution of the dataset using the <code>downsample</code> method:</p> <pre><code># Downsample by level\ndownsampled = znimg.downsample(level=2)\nprint(\"Downsampled shape:\", downsampled.darr.shape)\n</code></pre>"},{"location":"examples/zarr_nifti/#cropping","title":"Cropping","text":"<p>Extract a specific region from the dataset using bounding boxes:</p>"},{"location":"examples/zarr_nifti/#voxel-space","title":"Voxel Space:","text":"<pre><code>cropped = znimg.crop((10, 10, 10), (50, 50, 50))\nprint(\"Cropped shape:\", cropped.darr.shape)\n</code></pre>"},{"location":"examples/zarr_nifti/#with-ras-coordinates","title":"With RAS Coordinates:","text":"<pre><code># Note: crop_with_bounding_box is a legacy method that still supports RAS coords\ncropped_ras = znimg.crop_with_bounding_box(\n    (-20, -20, -20), (20, 20, 20), ras_coords=True\n)\nprint(\"Cropped shape:\", cropped_ras.darr.shape)\n</code></pre>"},{"location":"examples/zarr_nifti/#combining-affine-transformations","title":"Combining Affine Transformations","text":"<p>Apply multiple transformations to the dataset in sequence:</p> <pre><code>from zarrnii.transform import AffineTransform\nimport numpy as np\n\n# Define transformations using matrices\nscale_matrix = np.array([\n    [2.0, 0.0, 0.0, 0.0],\n    [0.0, 2.0, 0.0, 0.0], \n    [0.0, 0.0, 1.0, 0.0],\n    [0.0, 0.0, 0.0, 1.0]\n])\nscale = AffineTransform.from_array(scale_matrix)\n\ntranslate_matrix = np.array([\n    [1.0, 0.0, 0.0, 10.0],\n    [0.0, 1.0, 0.0, -5.0],\n    [0.0, 0.0, 1.0, 0.0],\n    [0.0, 0.0, 0.0, 1.0]\n])\ntranslate = AffineTransform.from_array(translate_matrix)\n\n# Apply transformations\ntransformed = znimg.apply_transform(scale, translate, ref_znimg=znimg)\nprint(\"Transformed affine matrix:\\n\", transformed.affine.matrix)\n</code></pre>"},{"location":"examples/zarr_nifti/#saving-results","title":"Saving Results","text":""},{"location":"examples/zarr_nifti/#to-ome-zarr","title":"To OME-Zarr","text":"<p>Save the dataset to OME-Zarr format:</p> <pre><code>znimg.to_ome_zarr(\"output.zarr\", max_layer=3, scaling_method=\"local_mean\")\n</code></pre>"},{"location":"examples/zarr_nifti/#to-nifti","title":"To NIfTI","text":"<p>Save the dataset to NIfTI format:</p> <pre><code>znimg.to_nifti(\"output.nii\")\n</code></pre>"},{"location":"examples/zarr_nifti/#advanced-example-full-workflow","title":"Advanced Example: Full Workflow","text":"<p>Combine multiple operations in a single workflow:</p> <pre><code>from zarrnii import ZarrNii\nfrom zarrnii.transform import AffineTransform\nimport numpy as np\n\n# Load an OME-Zarr dataset\nznimg = ZarrNii.from_ome_zarr(\"path/to/dataset.zarr\")\n\n# Crop the dataset\ncropped = znimg.crop((10, 10, 10), (100, 100, 100))\n\n# Downsample the dataset\ndownsampled = cropped.downsample(level=2)\n\n# Apply an affine transformation\nscale_matrix = np.array([\n    [1.5, 0.0, 0.0, 0.0],\n    [0.0, 1.5, 0.0, 0.0],\n    [0.0, 0.0, 1.0, 0.0],\n    [0.0, 0.0, 0.0, 1.0]\n])\nscale = AffineTransform.from_array(scale_matrix)\ntransformed = downsampled.apply_transform(scale, ref_znimg=downsampled)\n\n# Save the result as a NIfTI file\ntransformed.to_nifti(\"final_output.nii\")\n</code></pre>"},{"location":"examples/zarr_nifti/#summary","title":"Summary","text":"<p>In this section, you learned how to: - Load datasets from OME-Zarr and NIfTI formats. - Perform transformations like downsampling, cropping, and affine transformations. - Save results back to OME-Zarr or NIfTI.</p> <p>Next: - Explore the API Reference for in-depth details about ZarrNii's classes and methods. - Check the FAQ for answers to common questions.</p>"},{"location":"walkthrough/advanced_use_cases/","title":"Walkthrough: Advanced Use Cases","text":"<p>This guide explores advanced workflows with ZarrNii, including metadata preservation, handling multiscale OME-Zarr pyramids, and combining multiple transformations.</p>"},{"location":"walkthrough/advanced_use_cases/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Preserving Metadata</li> <li>Working with Multiscale Pyramids</li> <li>Combining Transformations</li> <li>Handling Large Datasets</li> </ol>"},{"location":"walkthrough/advanced_use_cases/#preserving-metadata","title":"Preserving Metadata","text":"<p>ZarrNii is designed to handle and preserve metadata when converting between formats or applying transformations.</p>"},{"location":"walkthrough/advanced_use_cases/#accessing-metadata","title":"Accessing Metadata","text":"<p>OME-Zarr metadata is automatically extracted and stored in the <code>axes</code>, <code>coordinate_transformations</code>, and <code>omero</code> attributes of a <code>ZarrNii</code> instance.</p> <pre><code>znimg = ZarrNii.from_ome_zarr(\"path/to/dataset.zarr\")\n\n# Access axes metadata\nprint(\"Axes metadata:\", znimg.axes)\n\n# Access coordinate transformations\nprint(\"Coordinate transformations:\", znimg.coordinate_transformations)\n\n# Access Omero metadata\nprint(\"Omero metadata:\", znimg.omero)\n</code></pre>"},{"location":"walkthrough/advanced_use_cases/#preserving-metadata-during-transformations","title":"Preserving Metadata During Transformations","text":"<p>When you perform transformations like cropping or downsampling, ZarrNii ensures metadata remains consistent.</p> <pre><code>cropped = znimg.crop((10, 10, 10), (50, 50, 50))\nprint(\"Updated metadata:\", cropped.coordinate_transformations)\n</code></pre>"},{"location":"walkthrough/advanced_use_cases/#working-with-multiscale-pyramids","title":"Working with Multiscale Pyramids","text":"<p>OME-Zarr datasets often include multiscale pyramids, where each level represents a progressively downsampled version of the data.</p>"},{"location":"walkthrough/advanced_use_cases/#loading-a-specific-level","title":"Loading a Specific Level","text":"<p>You can load a specific pyramid level using the <code>level</code> argument in <code>from_ome_zarr</code>:</p> <pre><code>znimg = ZarrNii.from_ome_zarr(\"path/to/dataset.zarr\", level=2)\nprint(\"Loaded shape:\", znimg.darr.shape)\n</code></pre>"},{"location":"walkthrough/advanced_use_cases/#handling-custom-downsampling","title":"Handling Custom Downsampling","text":"<p>If the desired level isn't available in the pyramid, ZarrNii computes additional downsampling lazily:</p> <pre><code>level, do_downsample, ds_kwargs = ZarrNii.get_level_and_downsampling_kwargs(\n    \"path/to/dataset.zarr\", level=5\n)\nif do_downsample:\n    znimg = znimg.downsample(**ds_kwargs)\n</code></pre>"},{"location":"walkthrough/advanced_use_cases/#combining-transformations","title":"Combining Transformations","text":"<p>ZarrNii allows you to chain multiple transformations into a single workflow. This is useful when applying affine transformations, interpolations, or warping.</p>"},{"location":"walkthrough/advanced_use_cases/#chaining-affine-transformations","title":"Chaining Affine Transformations","text":"<pre><code>from zarrnii.transform import AffineTransform\nimport numpy as np\n\n# Create transformations using matrices\nscaling_matrix = np.array([\n    [2.0, 0.0, 0.0, 0.0],\n    [0.0, 2.0, 0.0, 0.0], \n    [0.0, 0.0, 1.0, 0.0],\n    [0.0, 0.0, 0.0, 1.0]\n])\nscaling = AffineTransform.from_array(scaling_matrix)\n\ntranslation_matrix = np.array([\n    [1.0, 0.0, 0.0, 10.0],\n    [0.0, 1.0, 0.0, -5.0],\n    [0.0, 0.0, 1.0, 0.0],\n    [0.0, 0.0, 0.0, 1.0]\n])\ntranslation = AffineTransform.from_array(translation_matrix)\n\n# Apply multiple transformations sequentially\ncombined = znimg.apply_transform(scaling, translation, ref_znimg=znimg)\nprint(\"New affine matrix:\\n\", combined.affine.matrix)\n</code></pre>"},{"location":"walkthrough/advanced_use_cases/#handling-large-datasets","title":"Handling Large Datasets","text":"<p>ZarrNii leverages Dask to handle datasets that don't fit into memory.</p>"},{"location":"walkthrough/advanced_use_cases/#optimizing-chunking","title":"Optimizing Chunking","text":"<p>Ensure the dataset is chunked appropriately for operations like downsampling or interpolation:</p> <pre><code># Rechunk for efficient processing\nrechunked = znimg.darr.rechunk((1, 64, 64, 64))\nprint(\"Rechunked shape:\", rechunked.shape)\n</code></pre>"},{"location":"walkthrough/advanced_use_cases/#lazy-evaluation","title":"Lazy Evaluation","text":"<p>Most transformations in ZarrNii are lazy, meaning computations are only triggered when necessary. Use <code>.compute()</code> to materialize results.</p> <pre><code># Trigger computation\ncropped = znimg.crop((10, 10, 10), (50, 50, 50))\ncropped.darr.compute()\n</code></pre>"},{"location":"walkthrough/advanced_use_cases/#summary","title":"Summary","text":"<p>This guide covered: - Preserving metadata across transformations and format conversions. - Working with multiscale pyramids in OME-Zarr. - Combining transformations for complex workflows. - Handling large datasets efficiently with Dask.</p> <p>Next, explore: - Examples: Detailed workflows and practical use cases. - API Reference: Technical details for ZarrNii classes and methods.</p>"},{"location":"walkthrough/basic_tasks/","title":"Walkthrough: Basic Tasks","text":"<p>This guide covers the most common tasks you'll perform with ZarrNii, including reading data, performing transformations, and saving results.</p>"},{"location":"walkthrough/basic_tasks/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Reading Data<ul> <li>From OME-Zarr</li> <li>From NIfTI</li> <li>Working with 5D Data</li> </ul> </li> <li>Transforming Data<ul> <li>Cropping</li> <li>Downsampling</li> <li>Upsampling</li> <li>Applying Affine Transformations</li> </ul> </li> <li>Saving Data<ul> <li>To NIfTI</li> <li>To OME-Zarr</li> </ul> </li> </ol>"},{"location":"walkthrough/basic_tasks/#reading-data","title":"Reading Data","text":""},{"location":"walkthrough/basic_tasks/#from-ome-zarr","title":"From OME-Zarr","text":"<p>Load a dataset from an OME-Zarr file using <code>from_ome_zarr</code>:</p> <pre><code>from zarrnii import ZarrNii\n\n# Load the dataset\nznimg = ZarrNii.from_ome_zarr(\"path/to/dataset.ome.zarr\")\n\n# Inspect the data\nprint(\"Data shape:\", znimg.darr.shape)\nprint(\"Affine matrix:\\n\", znimg.affine.matrix)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#automatic-anisotropy-correction","title":"Automatic Anisotropy Correction","text":"<p>For datasets with anisotropic voxels (common in lightsheet microscopy), you can automatically downsample to create more isotropic voxels:</p> <pre><code># Load with near-isotropic downsampling\nznimg_isotropic = ZarrNii.from_ome_zarr(\n    \"path/to/anisotropic_data.ome.zarr\", \n    downsample_near_isotropic=True\n)\n\nprint(\"Isotropic data shape:\", znimg_isotropic.darr.shape)\nprint(\"Isotropic scales:\", znimg_isotropic.scale)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#from-nifti","title":"From NIfTI","text":"<p>Load a dataset from a NIfTI file using <code>from_nifti</code>:</p> <pre><code># Load the dataset\nznimg = ZarrNii.from_nifti(\"path/to/dataset.nii\")\n\n# Inspect the data\nprint(\"Data shape:\", znimg.darr.shape)\nprint(\"Affine matrix:\\n\", znimg.affine.matrix)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#working-with-5d-data","title":"Working with 5D Data","text":"<p>ZarrNii supports 5D images with time and channel dimensions (T,C,Z,Y,X). You can select specific timepoints and channels during loading or after loading.</p>"},{"location":"walkthrough/basic_tasks/#loading-with-timepoint-selection","title":"Loading with Timepoint Selection:","text":"<pre><code># Load specific timepoints\nznimg_time = ZarrNii.from_ome_zarr(\"timeseries.zarr\", timepoints=[0, 2, 4])\nprint(\"Timepoint subset shape:\", znimg_time.darr.shape)\n\n# Load specific channels by index\nznimg_channels = ZarrNii.from_ome_zarr(\"multichannel.zarr\", channels=[0, 2])\n\n# Load specific channels by label\nznimg_labels = ZarrNii.from_ome_zarr(\"labeled.zarr\", channel_labels=[\"DAPI\", \"GFP\"])\n\n# Combine timepoint and channel selection\nznimg_subset = ZarrNii.from_ome_zarr(\"data.zarr\", timepoints=[1, 3], channels=[0])\n</code></pre>"},{"location":"walkthrough/basic_tasks/#post-loading-selection","title":"Post-loading Selection:","text":"<pre><code># Load full dataset first\nznimg = ZarrNii.from_ome_zarr(\"timeseries.zarr\")\n\n# Select timepoints after loading\nselected_time = znimg.select_timepoints([0, 2])\n\n# Select channels after loading\nselected_channels = znimg.select_channels([1, 2])\n\n# Chain selections\nsubset = znimg.select_timepoints([0, 1]).select_channels([0])\n</code></pre>"},{"location":"walkthrough/basic_tasks/#transforming-data","title":"Transforming Data","text":""},{"location":"walkthrough/basic_tasks/#cropping","title":"Cropping","text":"<p>Crop the dataset to a specific bounding box. You can define the bounding box in either voxel space or RAS (real-world) coordinates. For 5D data, cropping operates only on spatial dimensions, preserving time and channel dimensions.</p>"},{"location":"walkthrough/basic_tasks/#voxel-space-cropping","title":"Voxel Space Cropping:","text":"<pre><code># Preferred method using crop()\ncropped = znimg.crop((10, 10, 10), (50, 50, 50))\nprint(\"Cropped shape:\", cropped.darr.shape)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#ras-space-cropping","title":"RAS Space Cropping:","text":"<pre><code># Use legacy method for RAS coordinates\ncropped_ras = znimg.crop_with_bounding_box(\n    (-20, -20, -20), (20, 20, 20), ras_coords=True\n)\nprint(\"Cropped shape:\", cropped_ras.darr.shape)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#downsampling","title":"Downsampling","text":"<p>Downsample the dataset to reduce its resolution. You can specify either a downsampling level or individual scaling factors for each axis. For 5D data, downsampling operates only on spatial dimensions, preserving time and channel dimensions.</p>"},{"location":"walkthrough/basic_tasks/#by-level","title":"By Level:","text":"<pre><code>downsampled = znimg.downsample(level=2)\nprint(\"Downsampled shape:\", downsampled.darr.shape)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#by-scaling-factors","title":"By Scaling Factors:","text":"<pre><code>downsampled_manual = znimg.downsample(along_x=2, along_y=2, along_z=1)\nprint(\"Downsampled shape:\", downsampled_manual.darr.shape)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#upsampling","title":"Upsampling","text":"<p>Increase the resolution of the dataset by upsampling.</p>"},{"location":"walkthrough/basic_tasks/#by-scaling-factors_1","title":"By Scaling Factors:","text":"<pre><code>upsampled = znimg.upsample(along_x=2, along_y=2, along_z=2)\nprint(\"Upsampled shape:\", upsampled.darr.shape)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#to-target-shape","title":"To Target Shape:","text":"<pre><code>upsampled_target = znimg.upsample(to_shape=(1, 256, 256, 256))\nprint(\"Upsampled shape:\", upsampled_target.darr.shape)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#applying-affine-transformations","title":"Applying Affine Transformations","text":"<p>Apply a custom affine transformation to the dataset.</p> <pre><code>from zarrnii.transform import AffineTransform\nimport numpy as np\n\n# Define a scaling transformation using a matrix\nscaling_matrix = np.array([\n    [2.0, 0.0, 0.0, 0.0],\n    [0.0, 2.0, 0.0, 0.0], \n    [0.0, 0.0, 1.0, 0.0],\n    [0.0, 0.0, 0.0, 1.0]\n])\nscaling_transform = AffineTransform.from_array(scaling_matrix)\n\n# Apply the transformation  \ntransformed = znimg.apply_transform(scaling_transform, ref_znimg=znimg)\nprint(\"Transformed affine matrix:\\n\", transformed.affine.matrix)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#saving-data","title":"Saving Data","text":""},{"location":"walkthrough/basic_tasks/#to-nifti","title":"To NIfTI","text":"<p>Save the dataset as a NIfTI file using <code>to_nifti</code>:</p> <pre><code>znimg.to_nifti(\"output_dataset.nii\")\n</code></pre>"},{"location":"walkthrough/basic_tasks/#to-ome-zarr","title":"To OME-Zarr","text":"<p>Save the dataset as an OME-Zarr file using <code>to_ome_zarr</code>:</p> <pre><code>znimg.to_ome_zarr(\"output_dataset.ome.zarr\")\n</code></pre> <p>You can also save additional metadata during the process:</p> <pre><code>znimg.to_ome_zarr(\n    \"output_dataset.ome.zarr\",\n    max_layer=3,\n    scaling_method=\"local_mean\"\n)\n</code></pre>"},{"location":"walkthrough/basic_tasks/#summary","title":"Summary","text":"<p>This guide covered the essential operations you can perform with ZarrNii: - Reading datasets from OME-Zarr and NIfTI formats. - Transforming datasets through cropping, downsampling, upsampling, and affine transformations. - Saving datasets back to either format.</p> <p>Next, explore Advanced Use Cases or dive into the API Reference for detailed technical documentation.</p>"},{"location":"walkthrough/getting_started/","title":"Getting Started","text":"<p>This guide helps you set up ZarrNii and get started with its basic functionality. By the end of this guide, you'll be able to read OME-Zarr and NIfTI datasets, perform basic transformations, and save your results.</p>"},{"location":"walkthrough/getting_started/#installation","title":"Installation","text":"<p>ZarrNii requires Python 3.11 or later. Install it using uv, a modern, fast Python package installer and project manager.</p>"},{"location":"walkthrough/getting_started/#1-clone-the-repository","title":"1. Clone the Repository","text":"<p>If you're using the source code, clone the ZarrNii repository:</p> <pre><code>git clone https://github.com/khanlab/zarrnii.git\ncd zarrnii\n</code></pre>"},{"location":"walkthrough/getting_started/#2-install-with-uv","title":"2. Install with uv","text":"<p>Run the following command to install the library and its dependencies:</p> <pre><code>uv sync --dev\n</code></pre> <p>If you don't use uv, install ZarrNii and its dependencies using <code>pip</code>:</p> <pre><code>pip install zarrnii\n</code></pre>"},{"location":"walkthrough/getting_started/#3-optional-dependencies","title":"3. Optional Dependencies","text":"<p>For additional format support, install optional dependencies:</p> <pre><code># For Imaris (.ims) file support\npip install zarrnii[imaris]\n</code></pre>"},{"location":"walkthrough/getting_started/#prerequisites","title":"Prerequisites","text":"<p>Before using ZarrNii, ensure you have: - OME-Zarr datasets: Multidimensional images in Zarr format. - NIfTI datasets: Neuroimaging data in <code>.nii</code> or <code>.nii.gz</code> format. - Imaris datasets (optional): Microscopy data in <code>.ims</code> format (requires <code>zarrnii[imaris]</code>).</p>"},{"location":"walkthrough/getting_started/#basic-usage","title":"Basic Usage","text":""},{"location":"walkthrough/getting_started/#1-reading-data","title":"1. Reading Data","text":"<p>You can load an OME-Zarr or NIfTI dataset into a <code>ZarrNii</code> object.</p>"},{"location":"walkthrough/getting_started/#from-ome-zarr","title":"From OME-Zarr:","text":"<pre><code>from zarrnii import ZarrNii\n\n# Load OME-Zarr\nznimg = ZarrNii.from_ome_zarr(\"path/to/dataset.ome.zarr\")\n\nprint(\"Data shape:\", znimg.darr.shape)\nprint(\"Affine matrix:\\n\", znimg.affine.matrix)\n\n# For anisotropic data, automatically create more isotropic voxels\nznimg_isotropic = ZarrNii.from_ome_zarr(\n    \"path/to/dataset.ome.zarr\", \n    downsample_near_isotropic=True\n)\n# Load specific timepoints and channels from 5D data\nznimg_5d = ZarrNii.from_ome_zarr(\"timeseries.zarr\", timepoints=[0, 2], channels=[1])\nprint(\"5D subset shape:\", znimg_5d.darr.shape)\n</code></pre>"},{"location":"walkthrough/getting_started/#from-nifti","title":"From NIfTI:","text":"<pre><code># Load NIfTI (supports 3D, 4D, and 5D data)\nznimg = ZarrNii.from_nifti(\"path/to/dataset.nii\")\n\nprint(\"Data shape:\", znimg.darr.shape)\nprint(\"Affine matrix:\\n\", znimg.affine.matrix)\n</code></pre>"},{"location":"walkthrough/getting_started/#from-imaris-requires-zarrniiimaris","title":"From Imaris (requires <code>zarrnii[imaris]</code>):","text":"<pre><code># Load Imaris\nznimg = ZarrNii.from_imaris(\"path/to/dataset.ims\")\n\nprint(\"Data shape:\", znimg.darr.shape)\nprint(\"Affine matrix:\\n\", znimg.affine.matrix)\n</code></pre>"},{"location":"walkthrough/getting_started/#2-working-with-5d-data","title":"2. Working with 5D Data","text":"<p>ZarrNii supports 5D images with time and channel dimensions (T,C,Z,Y,X format). You can select specific timepoints and channels either during loading or after loading.</p>"},{"location":"walkthrough/getting_started/#loading-with-selection","title":"Loading with Selection:","text":"<pre><code># Load specific timepoints\nznimg_time = ZarrNii.from_ome_zarr(\"timeseries.zarr\", timepoints=[0, 2, 4])\n\n# Load specific channels  \nznimg_channels = ZarrNii.from_ome_zarr(\"multichannel.zarr\", channels=[0, 2])\n\n# Load specific channels by label\nznimg_labels = ZarrNii.from_ome_zarr(\"labeled.zarr\", channel_labels=[\"DAPI\", \"GFP\"])\n\n# Combine timepoint and channel selection\nznimg_subset = ZarrNii.from_ome_zarr(\"data.zarr\", timepoints=[1, 3], channels=[0])\n</code></pre>"},{"location":"walkthrough/getting_started/#post-loading-selection","title":"Post-loading Selection:","text":"<pre><code># Load full dataset first\nznimg = ZarrNii.from_ome_zarr(\"timeseries.zarr\")\n\n# Select timepoints after loading\nselected_time = znimg.select_timepoints([0, 2])\n\n# Select channels after loading\nselected_channels = znimg.select_channels([1, 2])\n\n# Chain selections\nsubset = znimg.select_timepoints([0, 1]).select_channels([0])\n</code></pre>"},{"location":"walkthrough/getting_started/#3-performing-transformations","title":"3. Performing Transformations","text":"<p>ZarrNii supports various transformations, such as cropping, downsampling, and upsampling. When working with 5D data, spatial transformations preserve the time and channel dimensions.</p>"},{"location":"walkthrough/getting_started/#cropping","title":"Cropping:","text":"<p>Crop a region from the dataset using voxel coordinates:</p> <pre><code>cropped = znimg.crop((10, 10, 10), (50, 50, 50))\nprint(\"Cropped shape:\", cropped.darr.shape)\n</code></pre>"},{"location":"walkthrough/getting_started/#downsampling","title":"Downsampling:","text":"<p>Reduce the resolution of your dataset:</p> <pre><code>downsampled = znimg.downsample(level=2)\nprint(\"Downsampled shape:\", downsampled.darr.shape)\n\n# For 5D data: (3, 2, 16, 32, 32) -&gt; (3, 2, 8, 16, 16)\n# Time and channel dimensions are preserved\n</code></pre>"},{"location":"walkthrough/getting_started/#upsampling","title":"Upsampling:","text":"<p>Increase the resolution of your dataset:</p> <pre><code>upsampled = znimg.upsample(along_x=2, along_y=2, along_z=2)\nprint(\"Upsampled shape:\", upsampled.darr.shape)\n</code></pre>"},{"location":"walkthrough/getting_started/#3-image-segmentation","title":"3. Image Segmentation","text":"<p>ZarrNii includes a plugin architecture for image segmentation algorithms. You can apply segmentation to identify regions of interest in your data.</p>"},{"location":"walkthrough/getting_started/#otsu-thresholding","title":"Otsu Thresholding:","text":"<p>Apply automatic Otsu thresholding for binary segmentation:</p> <pre><code>segmented = znimg.segment_otsu(nbins=256)\nprint(\"Segmented shape:\", segmented.darr.shape)\nprint(\"Unique values:\", segmented.darr.compute().unique())  # Should show [0, 1]\n</code></pre>"},{"location":"walkthrough/getting_started/#using-plugin-interface","title":"Using Plugin Interface:","text":"<p>You can also use the generic plugin interface:</p> <pre><code>from zarrnii import OtsuSegmentation\n\nplugin = OtsuSegmentation(nbins=128)\nsegmented = znimg.segment(plugin)\n</code></pre>"},{"location":"walkthrough/getting_started/#custom-chunk-processing","title":"Custom Chunk Processing:","text":"<p>For large datasets, you can control memory usage with custom chunk sizes:</p> <pre><code>segmented = znimg.segment_otsu(chunk_size=(1, 10, 50, 50))\n</code></pre>"},{"location":"walkthrough/getting_started/#4-saving-data","title":"4. Saving Data","text":"<p>ZarrNii makes it easy to save your datasets in both OME-Zarr and NIfTI formats.</p>"},{"location":"walkthrough/getting_started/#to-nifti","title":"To NIfTI:","text":"<p>Save the dataset as a <code>.nii</code> file:</p> <pre><code>znimg.to_nifti(\"output_dataset.nii\")\n</code></pre>"},{"location":"walkthrough/getting_started/#to-ome-zarr","title":"To OME-Zarr:","text":"<p>Save the dataset back to OME-Zarr format:</p> <pre><code>znimg.to_ome_zarr(\"output_dataset.ome.zarr\")\n</code></pre>"},{"location":"walkthrough/getting_started/#example-workflow","title":"Example Workflow","text":"<p>Here\u2019s a full workflow from loading an OME-Zarr dataset to saving a downsampled version as NIfTI:</p> <pre><code>from zarrnii import ZarrNii\n\n# Load a 5D OME-Zarr dataset with specific timepoints and channels\nznimg = ZarrNii.from_ome_zarr(\"path/to/timeseries.zarr\", \n                              timepoints=[0, 2, 4], \n                              channels=[0, 1])\n\n# Perform transformations\ncropped = znimg.crop((10, 10, 10), (100, 100, 100))\ndownsampled = cropped.downsample(level=2)\n\n# Save the result as a NIfTI file\n\ndownsampled.to_nifti(\"processed_timeseries.nii\")\n\n# Or save as OME-Zarr with metadata preservation\ndownsampled.to_ome_zarr(\"processed_timeseries.ome.zarr\")\n\n# Save as TIFF stack for compatibility with napari plugins like cellseg3d\n# (Crop first for better performance with large datasets)\ncropped_small = znimg.crop((10, 10, 10), (50, 50, 50))\ncropped_small.to_tiff_stack(\"tiff_stack/slice_{z:03d}.tif\")\n\n\n# Apply segmentation to the original image\nsegmented = znimg.segment_otsu(nbins=256)\nsegmented.to_nifti(\"segmented_output.nii\")\n\n</code></pre>"},{"location":"walkthrough/getting_started/#whats-next","title":"What\u2019s Next?","text":"<ul> <li>Walkthrough: Basic Tasks: Learn more about common workflows like cropping, interpolation, and combining transformations.</li> <li>Segmentation Plugin Examples: Learn how to use and create segmentation plugins.</li> <li>API Reference: Explore the detailed API for ZarrNii.</li> </ul>"},{"location":"walkthrough/overview/","title":"Walkthrough: Overview","text":"<p>This page provides an overview of the core concepts behind ZarrNii. It\u2019s the starting point for understanding how to work with OME-Zarr, NIfTI, and ZarrNii\u2019s transformation tools.</p>"},{"location":"walkthrough/overview/#core-concepts","title":"Core Concepts","text":""},{"location":"walkthrough/overview/#1-zarr-and-ome-zarr","title":"1. Zarr and OME-Zarr","text":"<ul> <li>Zarr is a format for chunked, compressed N-dimensional arrays.</li> <li>OME-Zarr extends Zarr with metadata for multidimensional microscopy images, supporting axes definitions and multiscale pyramids.</li> </ul>"},{"location":"walkthrough/overview/#key-features-of-ome-zarr","title":"Key Features of OME-Zarr:","text":"<ul> <li>Axes Metadata: Defines spatial dimensions (e.g., <code>x</code>, <code>y</code>, <code>z</code>).</li> <li>Multiscale Pyramids: Stores image resolutions at multiple scales.</li> <li>Annotations: Includes OME metadata for visualization and analysis.</li> </ul>"},{"location":"walkthrough/overview/#2-nifti","title":"2. NIfTI","text":"<ul> <li>NIfTI is a neuroimaging file format, commonly used for MRI and fMRI data.</li> <li>It supports spatial metadata, such as voxel sizes and affine transformations, for anatomical alignment.</li> </ul>"},{"location":"walkthrough/overview/#3-zarrnii","title":"3. ZarrNii","text":"<ul> <li>ZarrNii provides tools to bridge these formats while preserving spatial metadata and enabling transformations.</li> </ul>"},{"location":"walkthrough/overview/#main-features","title":"Main Features:","text":"<ul> <li>Read and write OME-Zarr and NIfTI formats.</li> <li>Apply transformations like cropping, downsampling, and interpolation.</li> <li>Apply segmentation algorithms through an extensible plugin system.</li> <li>Convert between ZYX (OME-Zarr) and XYZ (NIfTI) axes orders.</li> </ul>"},{"location":"walkthrough/overview/#data-model","title":"Data Model","text":"<p>ZarrNii wraps datasets using the <code>ZarrNii</code> class, which has the following attributes:</p> <ul> <li><code>darr</code>: The dask array containing image data.</li> <li><code>affine</code>: An affine transformation matrix for spatial alignment.</li> <li><code>axes_order</code>: Specifies the data layout (<code>ZYX</code> or <code>XYZ</code>).</li> <li>OME-Zarr Metadata:</li> <li><code>axes</code>: Defines the dimensions and units.</li> <li><code>coordinate_transformations</code>: Lists scaling and translation transformations.</li> <li><code>omero</code>: Contains channel and visualization metadata.</li> </ul>"},{"location":"walkthrough/overview/#example-workflow","title":"Example Workflow","text":"<p>Here\u2019s a high-level example workflow using ZarrNii:</p> <ol> <li> <p>Read Data:    <code>python    from zarrnii import ZarrNii    znimg = ZarrNii.from_ome_zarr(\"path/to/dataset.ome.zarr\")</code></p> </li> <li> <p>Apply Transformations:    <code>python    znimg_downsampled = znimg.downsample(level=2)    znimg_cropped = znimg_downsampled.crop((0, 0, 0), (100, 100, 100))</code></p> </li> <li> <p>Apply Segmentation:    <code>python    znimg_segmented = znimg.segment_otsu(nbins=256)</code></p> </li> <li> <p>Convert Formats:    <code>python    znimg_cropped.to_nifti(\"output.nii\")    znimg_segmented.to_nifti(\"segmented.nii\")</code></p> </li> </ol>"},{"location":"walkthrough/overview/#whats-next","title":"What\u2019s Next?","text":"<ul> <li>Getting Started: Step-by-step guide to installing and using ZarrNii.</li> <li>Basic Tasks: Learn how to read, write, and transform data.</li> </ul>"}]}